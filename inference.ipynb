{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/trans/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-04 17:33:28,645 - modelscope - INFO - PyTorch version 2.2.1 Found.\n",
      "2024-04-04 17:33:28,649 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
      "2024-04-04 17:33:28,685 - modelscope - INFO - Loading done! Current index file version is 1.13.1, with md5 b5a2c5fe01f7460b3e700a8ce7e6fc94 and a total number of 972 components indexed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import bitsandbytes\n",
    "import accelerate\n",
    "from datasets import Dataset\n",
    "\n",
    "from modelscope import snapshot_download\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from peft import PeftModel\n",
    "\n",
    "from custom_tokenizers import YueTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=r'/root/autodl-tmp/01ai/Yi-6B-Chat'\n",
    "model_dir = snapshot_download('01ai/Yi-6B-Chat', cache_dir='/root/autodl-tmp', revision='master')\n",
    "\n",
    "# base_tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True, padding_side='left', max_length=512, return_tensors='pt')\n",
    "\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(\n",
    "# \t model_path,\n",
    "# \t device_map='auto',\n",
    "# \t torch_dtype=torch.bfloat16,\n",
    "# \t trust_remote_code=True \n",
    "# ).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Translate the following words into English:\\n你係邊個？\"},\n",
    "]\n",
    "\n",
    "messages_plain = [\n",
    "    \"\"\"\n",
    "    <|im_start|> user\n",
    "    hi<|im_end|> \n",
    "    <|im_start|> assistant\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# input_ids = base_tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\n",
    "# output_ids = base_model.generate(input_ids.to('cuda'))\n",
    "# # response = base_tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True, max_length=100)\n",
    "# response = base_tokenizer.decode(output_ids[0], skip_special_tokens=False, max_length=100)\n",
    "\n",
    "# # Model response: \"Hello! How can I assist you today?\"\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LlamaTokenizer'. \n",
      "The class this function is called from is 'YueTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77969\n"
     ]
    }
   ],
   "source": [
    "tokenizer = YueTokenizer.from_pretrained('/root/autodl-tmp/01ai/Yi-6B-Chat', use_fast=True, padding_side='left', max_length=512, return_tensors='pt')\n",
    "base_tokenizer = AutoTokenizer.from_pretrained('/root/autodl-tmp/01ai/Yi-6B-Chat', use_fast=True, padding_side='left', max_length=512, return_tensors='pt')\n",
    "print(len(tokenizer.vocab))\n",
    "# tokenizer = AutoTokenizer.from_pretrained('/root/AIST4010-Cantonese-Translator/tokenizer', use_fast=True, padding_side='left', max_length=512, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PeftModel.from_pretrained(\n",
    "#     model,\n",
    "#    '/root/peft_model',\n",
    "#     is_trainable=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.36s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.47s/it]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "\t '/root/autodl-tmp/01ai/Yi-6B-Chat',\n",
    "\t device_map=device,\n",
    "\t torch_dtype=torch.bfloat16,\n",
    "     quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n",
    "\t trust_remote_code=True \n",
    ").eval()\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "\t model_path,\n",
    "\t device_map=device,\n",
    "\t torch_dtype=torch.bfloat16,\n",
    "\t quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n",
    "\t trust_remote_code=True \n",
    ").eval()\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.load_adapter('/root/autodl-tmp/peft_model_pretrained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_DIRECTORY = r'/root/'\n",
    "ABC_DICT_PATH = r'autodl-tmp/AIST4010-Cantonese-Translator-Data/ABC-Dict/abc_dict.csv'\n",
    "\n",
    "def load_abc_dataset():\n",
    "    abc_dict = pd.read_csv(REPO_DIRECTORY + ABC_DICT_PATH)\n",
    "    abc_dataset = Dataset.from_pandas(abc_dict)\n",
    "    return abc_dataset\n",
    "\n",
    "abc_set = load_abc_dataset()\n",
    "abc_shuffled_set = abc_set.train_test_split(seed=42, test_size=0.1)\n",
    "abc_train_set = abc_shuffled_set['train']\n",
    "abc_test_set = abc_shuffled_set['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'role': 'user',\n",
       "   'content': 'Translate the following words from Cantonese to English:\\n今日我同學唔小心，踢中我左面粒蛋蛋，好痛啊！'}],\n",
       " [{'role': 'user',\n",
       "   'content': \"Translate the following words from English to Cantonese:\\nToday my classmate wasn't careful, and kicked my left nut. It was very painful!\"}]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_translate_prompt(source_language, target_language, source_text):\n",
    "    return f\"Translate the following words from {source_language} to {target_language}:\\n{source_text}\"\n",
    "\n",
    "def get_messages(sample):\n",
    "    def get_translate_prompt(source_language, target_language, source_text):\n",
    "        return f\"Translate the following words from {source_language} to {target_language}:\\n{source_text}\"\n",
    "    return [\n",
    "        [{\"role\": \"user\", \"content\": get_translate_prompt('Cantonese', 'English', sample['yue'])}],\n",
    "        [{\"role\": \"user\", \"content\": get_translate_prompt('English', 'Cantonese', sample['en'])}]\n",
    "    ]\n",
    "\n",
    "train_samples = abc_train_set.shuffle(seed=10).select(range(20))\n",
    "test_samples = abc_test_set.shuffle(seed=10).select(range(20))\n",
    "\n",
    "get_messages(train_samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/trans/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/root/miniconda3/envs/trans/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742, 26212,\n",
      "          2823, 59569,   592,  4750, 59601,   144, 10721, 59646, 54274,   534,\n",
      "           453,   453, 13272,   101, 62328, 59642, 59646, 60488, 59724, 61329,\n",
      "         60751, 60751,   101, 59706, 60544, 60530,   103,     7, 59568,   144,\n",
      "             6, 14135,   144, 25585,   826,  1225, 13580, 24870, 19591,   795,\n",
      "           594,   567,  1999,  1682,  1935,    97,   648, 26288,   562,  1743,\n",
      "            99,     7]], device='cuda:0')\n",
      "Base model:\n",
      "Today my classmate accidentally kicked me in the left testicle, it hurts a lot!\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742,  4750,\n",
      "           592, 26212,  2823, 59569, 59601,   144, 25585,   826,  1225, 13580,\n",
      "          3613, 59610, 59570, 11369,    97,   597, 19591,   826,  1999,  6230,\n",
      "            98,   983,   717,  1196, 16465,    99,     7, 59568,   144,     6,\n",
      "         14135,   144,  3569,  2861, 54274, 12988, 13272,   101, 62328,  2131,\n",
      "          2861, 60488, 62295, 59599, 60751,   102,  2418, 60544,   103,     7]],\n",
      "       device='cuda:0')\n",
      "Base model:\n",
      "今天我的同學不太小心，踢到了我的左邊的蛋。非常痛！\n",
      "\n",
      "[None]\n"
     ]
    }
   ],
   "source": [
    "def model_output(model, tokenizer, messages, name=None):\n",
    "        for prompt in messages:\n",
    "                input_ids = tokenizer.apply_chat_template(conversation=prompt, tokenize=True, add_generation_prompt=True, return_tensors='pt')\n",
    "                with torch.cuda.amp.autocast():\n",
    "                        output_ids = model.generate(input_ids.to('cuda'), max_new_tokens=100)\n",
    "                response = base_tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True, max_length=100)\n",
    "                # response = tokenizer.decode(output_ids[0], skip_special_tokens=False, max_length=100)\n",
    "                print(output_ids)\n",
    "                print(f\"{name}:\\n{response}\\n\")\n",
    "\n",
    "\n",
    "print([model_output(base_model, base_tokenizer, get_messages(train_samples[0]), 'Base model'),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_outputs(samples):\n",
    "    for sample in samples:\n",
    "        print(sample)\n",
    "        prompts = get_messages(sample)\n",
    "        for prompt in prompts:\n",
    "            print(f\"Prompt:\\n{prompt[0]['content']}\")\n",
    "            print()\n",
    "            model_output(base_model, base_tokenizer, [prompt], 'Base model')\n",
    "            model_output(model, tokenizer, [prompt], 'Fine-tuned model')\n",
    "\n",
    "# compare_outputs(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926, 73580,  1994,   742,\n",
      "         26212,  2823, 59569,   592,  4750, 59601,   144, 10721, 59646, 54274,\n",
      "           534,   453,   453, 13272,   101, 62328, 59642, 59646, 60488, 59724,\n",
      "         61329, 60751, 60751,   101, 59706, 60544, 60530,   103,     7, 59568,\n",
      "           144,     6, 14135,   144,  6056,  1225, 13580, 24870, 19591,   795,\n",
      "           594,   567,  1999,  1682,  1935,  2272,   101,   878,   620,  1403,\n",
      "         16465,    99, 59640,  7025, 59631,  2085, 59640,  5684, 59631,   889,\n",
      "          7025, 59631,  2085, 59640,  5684, 59631,  7759,  4541,   742, 26212,\n",
      "          2823, 59569,   592,  4750, 59601, 59640,  7025, 59631,  2085, 59640,\n",
      "          5684, 59631,  2439,  1225, 13580, 24870, 19591,   795,   594,   567,\n",
      "          1999,  1682,  1935,  2272,   101,   878,   620,  1403, 16465,    99,\n",
      "         59640,  7025, 59631,  2085, 59640,  5684, 59631,   889,  7025, 59631,\n",
      "          2085, 59640,  5684, 59631,  7759,  4541,   742, 26212,  2823, 59569,\n",
      "           592,  4750, 59601, 59640,  7025, 59631,  2085, 59640,  5684, 59631,\n",
      "          2439,  1225, 13580, 24870]], device='cuda:0')\n",
      "Fine-tuned model:\n",
      "My classmate accidentally kicked me in the left testicle today， which is really painful!&lt;br&gt; &lt;br&gt;Translation from Cantonese to English:&lt;br&gt; My classmate accidentally kicked me in the left testicle today， which is really painful!&lt;br&gt; &lt;br&gt;Translation from Cantonese to English:&lt;br&gt; My classmate accidentally\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926, 73580,  1994,   742,\n",
      "          4750,   592, 26212,  2823, 59569, 59601,   144, 25585,   826,  1225,\n",
      "         13580,  3613, 59610, 59570, 11369,    97,   597, 19591,   826,  1999,\n",
      "          6230,    98,   983,   717,  1196, 16465,    99,     7, 59568,   144,\n",
      "             6, 14135,   144, 10721, 59646,   534, 70520, 59568, 54274,   534,\n",
      "         67704,   534, 75701, 59712, 73263, 59568,   101, 62328,   534, 75701,\n",
      "         59568, 63195, 59568,   101, 59706, 60544, 61459,   103, 59568, 10721,\n",
      "         59646, 59568, 54274, 59568, 59706, 59568, 68426, 59568,   101, 62328,\n",
      "         59568, 60488, 59568, 63195, 59568,   101, 59706, 60544, 61459,   103,\n",
      "         59568, 10721, 59646, 59568, 54274, 59568, 59706, 59568,   534, 64368,\n",
      "         59568,   101, 62328, 59568, 60488, 59568, 63195, 59568,   101, 59706,\n",
      "         60544, 61459,   103, 59568, 10721, 59646, 59568, 54274, 59568, 59706,\n",
      "         59568,   535, 75701, 59568,   101, 62328, 59568, 60488, 59568, 63195,\n",
      "         59568,   101, 59706, 60544, 61459,   103, 59568, 10721, 59646, 59568,\n",
      "         54274, 59568, 59706]], device='cuda:0')\n",
      "Fine-tuned model:\n",
      "今日我� 同學��心 ，踢� 腳 ，好痛呀！ 今日我 同學 好  ，踢 左 腳 ，好痛呀！ 今日我 同學 好 � ，踢 左 腳 ，好痛呀！ 今日我 同學 好 � ，踢 左 腳 ，好痛呀！ 今日我 同學 好\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_output(model, tokenizer, get_messages(train_samples[0]), 'Fine-tuned model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': \"Today my classmate wasn't careful, and kicked my left nut. It was very painful!\", 'yue': '今日我同學唔小心，踢中我左面粒蛋蛋，好痛啊！'}\n",
      "Prompt:\n",
      "Translate the following words from Cantonese to English:\n",
      "今日我同學唔小心，踢中我左面粒蛋蛋，好痛啊！\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742, 26212,\n",
      "          2823, 59569,   592,  4750, 59601,   144, 10721, 59646, 54274,   534,\n",
      "           453,   453, 13272,   101, 62328, 59642, 59646, 60488, 59724, 61329,\n",
      "         60751, 60751,   101, 59706, 60544, 60530,   103,     7, 59568,   144,\n",
      "             6, 14135,   144, 25585,   826,  1225, 13580, 24870, 19591,   795,\n",
      "           594,   567,  1999,  1682,  1935,    97,   648, 26288,   810,  1272,\n",
      "            99,     7]], device='cuda:0')\n",
      "Base model:\n",
      "Today my classmate accidentally kicked me in the left testicle, it hurts so much!\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926, 73580,  1994,   742,\n",
      "         26212,  2823, 59569,   592,  4750, 59601,   144, 10721, 59646, 54274,\n",
      "           534,   453,   453, 13272,   101, 62328, 59642, 59646, 60488, 59724,\n",
      "         61329, 60751, 60751,   101, 59706, 60544, 60530,   103,     7, 59568,\n",
      "           144,     6, 14135,   144, 25585,   826,  1225, 13580, 24870, 19591,\n",
      "           795,   594,   567,  1999,  1682,  1935,    97,   878,   620,  1196,\n",
      "         16465,    99, 59640,  7025, 59631,  2085, 59640,  5684, 59631,   889,\n",
      "          7025, 59631,  2085, 59640,  5684, 59631,   889,  7025, 59631,  2085,\n",
      "         59640,  5684, 59631,   889,  7025, 59631,  2085, 59640,  5684, 59631,\n",
      "           889,  7025, 59631,  2085, 59640,  5684, 59631,   889,  7025, 59631,\n",
      "          2085, 59640,  5684, 59631,   889,  7025, 59631,  2085, 59640,  5684,\n",
      "         59631,   889,  7025, 59631,  2085, 59640,  5684, 59631,   889,  7025,\n",
      "         59631,  2085, 59640,  5684, 59631,   889,  7025, 59631,  2085, 59640,\n",
      "          5684, 59631,   889,  7025, 59631,  2085, 59640,  5684, 59631,   889,\n",
      "          7025, 59631,  2085, 59640]], device='cuda:0')\n",
      "Fine-tuned model:\n",
      "Today my classmate accidentally kicked me in the left testicle, which is very painful!&lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&\n",
      "\n",
      "Prompt:\n",
      "Translate the following words from English to Cantonese:\n",
      "Today my classmate wasn't careful, and kicked my left nut. It was very painful!\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742,  4750,\n",
      "           592, 26212,  2823, 59569, 59601,   144, 25585,   826,  1225, 13580,\n",
      "          3613, 59610, 59570, 11369,    97,   597, 19591,   826,  1999,  6230,\n",
      "            98,   983,   717,  1196, 16465,    99,     7, 59568,   144,     6,\n",
      "         14135,   144, 17957, 54274, 12988, 13272,   101, 62328,  2131,  2861,\n",
      "         60488, 62295, 59599, 60751,   102, 22129, 60544,   103,     7]],\n",
      "       device='cuda:0')\n",
      "Base model:\n",
      "今天的同學不太小心，踢到了我的左邊的蛋。真的很痛！\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926, 73580,  1994,   742,\n",
      "          4750,   592, 26212,  2823, 59569, 59601,   144, 25585,   826,  1225,\n",
      "         13580,  3613, 59610, 59570, 11369,    97,   597, 19591,   826,  1999,\n",
      "          6230,    98,   983,   717,  1196, 16465,    99,     7, 59568,   144,\n",
      "             6, 14135,   144, 10721, 59646,   534, 65474, 54274,   534, 75701,\n",
      "         59712, 68426,   534, 65686,   101, 62328, 60488, 59646,   534, 67704,\n",
      "         60700,   102, 59706, 60544, 61459,   103,     7]], device='cuda:0')\n",
      "Fine-tuned model:\n",
      "今日我�同學�心�，踢左我�脚。好痛呀！\n",
      "\n",
      "{'en': 'My stomach hurt so I was rolling this way and that on the bed.', 'yue': '我肚痛到捵牀捵蓆。'}\n",
      "Prompt:\n",
      "Translate the following words from Cantonese to English:\n",
      "我肚痛到捵牀捵蓆。\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742, 26212,\n",
      "          2823, 59569,   592,  4750, 59601,   144, 59646, 61815, 60544, 59665,\n",
      "           535,   446,   486,   536,   442,   433,   535,   446,   486,   537,\n",
      "           452,   439,   102,     7, 59568,   144,     6, 14135,   144, 59646,\n",
      "         61815, 60544, 59665,   535,   446,   486,   536,   442,   433,   535,\n",
      "           446,   486,   537,   452,   439,   102,     7]], device='cuda:0')\n",
      "Base model:\n",
      "我肚痛到捵牀捵蓆。\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926, 73580,  1994,   742,\n",
      "         26212,  2823, 59569,   592,  4750, 59601,   144, 59646, 61815, 60544,\n",
      "         59665,   535,   446,   486,   536,   442,   433,   535,   446,   486,\n",
      "           537,   452,   439,   102,     7, 59568,   144,     6, 14135,   144,\n",
      "          6056, 15112,   620,   810, 16465,   639,   616,   748, 59610, 59570,\n",
      "          1212,  8894,  1310,   632,   567,  3807,    98, 59568,   536, 65474,\n",
      "         59568,   102, 59568,   102, 59568,   102, 59568,   102, 59568,   102,\n",
      "         59568,   102, 59568,   102, 59568,   102, 59568,   102, 59568,   102,\n",
      "         59568,   102, 59568,   102, 59568,   102, 59568,   102, 59568,   102,\n",
      "         59568,   102, 59568,   102, 59568,   102, 59568,   102, 59568,   102,\n",
      "         59568,   102, 59568,   102, 59568,   102, 59568,   102, 59568,   102,\n",
      "         59568,   102, 59568,   102, 59568,   102, 59568,   102, 59568,   102,\n",
      "         59568,   102, 59568,   102, 59568,   102, 59568,   102, 59568,   102,\n",
      "         59568,   102, 59568,   102, 59568,   102, 59568,   102, 59568,   102]],\n",
      "       device='cuda:0')\n",
      "Fine-tuned model:\n",
      "My stomach is so painful that I can't even lie down on the bed. � 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。\n",
      "\n",
      "Prompt:\n",
      "Translate the following words from English to Cantonese:\n",
      "My stomach hurt so I was rolling this way and that on the bed.\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742,  4750,\n",
      "           592, 26212,  2823, 59569, 59601,   144,  6056, 15112,  8192,   810,\n",
      "           616,   717, 14967,   719,  1217,   597,   639,   632,   567,  3807,\n",
      "            98,     7, 59568,   144,     6, 14135,   144, 59632, 22085,   535,\n",
      "           492,   495, 60599,   535,   492,   495, 59793,   101, 12354,  2861,\n",
      "         61186, 60544, 16729,   534,   447,   483, 60323,   102,     7]],\n",
      "       device='cuda:0')\n",
      "Base model:\n",
      "在床上滾來滾去，因為我的胃痛得很厲害。\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926, 73580,  1994,   742,\n",
      "          4750,   592, 26212,  2823, 59569, 59601,   144,  6056, 15112,  8192,\n",
      "           810,   616,   717, 14967,   719,  1217,   597,   639,   632,   567,\n",
      "          3807,    98,     7, 59568,   144,     6, 14135,   144, 59646, 61815,\n",
      "           537, 65474, 60544,   101,  1953,  8943, 22085,   535, 75701, 59568,\n",
      "         74167, 59568,   102,   141, 59640,  7025, 59631,  2085, 59640,  5684,\n",
      "         59631, 59568,  2861, 61186, 60544, 59721,   534, 75701, 59568,   101,\n",
      "         25907,   534, 70520, 59568, 60760, 60599, 61112, 59793, 59632, 22085,\n",
      "           102, 59640,  7025, 59631,  2085, 59640,  5684, 59631, 59568,  2861,\n",
      "         61815, 77683, 60544,   101,  1953,  8943, 22085, 60760, 60599, 61112,\n",
      "         59793,   102, 59640,  7025, 59631,  2085, 59640,  5684, 59631, 59568,\n",
      "          2861, 61815, 59568, 60544,   101,  1953,  8943, 22085, 60760, 60599,\n",
      "         61112, 59793,   102, 59640,  7025, 59631,  2085, 59640,  5684, 59631,\n",
      "         59568,  2861, 61815, 59568, 60544,   101, 25907, 60760]],\n",
      "       device='cuda:0')\n",
      "Fine-tuned model:\n",
      "我肚�痛，所以我在床上�  。  &lt;br&gt; 我的胃痛得� ，所以我� 翻來覆去在床上。&lt;br&gt; 我的肚痛，所以我在床上翻來覆去。&lt;br&gt; 我的肚 痛，所以我在床上翻來覆去。&lt;br&gt; 我的肚 痛，所以我翻\n",
      "\n",
      "{'en': \"I'm sorry, I can't recall right off the bat where those things of yours had got placed.\", 'yue': '唔好意思，我一時醒唔起你嗰啲嘢放咗喺邊度嘅。'}\n",
      "Prompt:\n",
      "Translate the following words from Cantonese to English:\n",
      "唔好意思，我一時醒唔起你嗰啲嘢放咗喺邊度嘅。\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742, 26212,\n",
      "          2823, 59569,   592,  4750, 59601,   144,   534,   453,   453, 59706,\n",
      "          7112,   101, 33442, 60531, 60808,   534,   453,   453, 59786, 59725,\n",
      "           534,   456,   481,   534,   454,   483,   534,   457,   467, 59932,\n",
      "           534,   451,   456,   534,   455,   491, 62295, 59781,   534,   457,\n",
      "           438,   102,     7, 59568,   144,     6, 14135,   144, 31368,   101,\n",
      "         59646, 22037,  1327, 16291, 59725, 59937,  4898,  4298,  7752,  9383,\n",
      "         59633,   102,     7]], device='cuda:0')\n",
      "Base model:\n",
      "对不起，我一时没有想起你把那些东西放在哪里了。\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926, 73580,  1994,   742,\n",
      "         26212,  2823, 59569,   592,  4750, 59601,   144,   534,   453,   453,\n",
      "         59706,  7112,   101, 33442, 60531, 60808,   534,   453,   453, 59786,\n",
      "         59725,   534,   456,   481,   534,   454,   483,   534,   457,   467,\n",
      "         59932,   534,   451,   456,   534,   455,   491, 62295, 59781,   534,\n",
      "           457,   438,   102,     7, 59568,   144,     6, 14135,   144, 59597,\n",
      "         59610, 59583,  9509,    97,   616,  2040, 59610, 59570, 11006,   828,\n",
      "           592,  1151,   641,  1710,  1367,  1613,    98, 59568, 59727, 61790,\n",
      "           101, 59646,   534,   456,   481,   534,   453,   453, 59706,  7112,\n",
      "           101, 59646,   534,   453,   453, 62028, 62364,   101,  1953,   534,\n",
      "           453,   453, 59706,  7112,   102, 59568, 59727, 61790,   101, 59646,\n",
      "           534,   453,   453, 62028, 62364,   101,  1953,   534,   441,   475,\n",
      "           534,   457,   438,   102, 59568, 59727, 61790,   101, 59646,   534,\n",
      "           453,   453, 62028, 62364,   101,  1953,   534,   453,   453, 59706,\n",
      "          7112,   102, 59568, 59727, 61790,   101, 59646,   534,   459,   464,\n",
      "           534, 72878, 59568,   101,  1953,   534, 65474, 59568,   102]],\n",
      "       device='cuda:0')\n",
      "Fine-tuned model:\n",
      "I'm sorry, I didn't wake up to where you put those things. 同埋，我嗰唔好意思，我唔識講，所以唔好意思。 同埋，我唔識講，所以刪嘅。 同埋，我唔識講，所以唔好意思。 同埋，我���� ，所以� 。\n",
      "\n",
      "Prompt:\n",
      "Translate the following words from English to Cantonese:\n",
      "I'm sorry, I can't recall right off the bat where those things of yours had got placed.\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742,  4750,\n",
      "           592, 26212,  2823, 59569, 59601,   144, 59597, 59610, 59583,  9509,\n",
      "            97,   616,   748, 59610, 59570,  8249,  1288,   965,   567, 10880,\n",
      "          1151,  1367,  1613,   593, 12796,   986,  1656,  6594,    98,     7,\n",
      "         59568,   144,     6, 14135,   144, 59632, 51974,   101, 59646, 60713,\n",
      "         11265,   101, 59646, 26912, 17581, 59890, 16291,  3326, 40401, 59777,\n",
      "          7752, 60403, 61898,   102,     7]], device='cuda:0')\n",
      "Base model:\n",
      "在這裡，我對不起，我無法立刻回想起你的東西被放在哪裡。\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926, 73580,  1994,   742,\n",
      "          4750,   592, 26212,  2823, 59569, 59601,   144, 59597, 59610, 59583,\n",
      "          9509,    97,   616,   748, 59610, 59570,  8249,  1288,   965,   567,\n",
      "         10880,  1151,  1367,  1613,   593, 12796,   986,  1656, 67380,  1203,\n",
      "            98,     7, 59568,   144,     6, 14135,   144, 59646, 60713, 11265,\n",
      "           101, 59646,   534, 75701, 63469, 59786, 59725,   534, 70375, 59568,\n",
      "         40401, 59793, 76548, 59568, 59633,   102, 59568,   536, 72202, 59568,\n",
      "         71909, 59568,   102, 59568,    98, 59568,    98, 59568,    98, 59568,\n",
      "            98, 59568,    98, 59568,    98, 59568,    98, 59568,    98, 59568,\n",
      "            98, 59568,    98, 59568,    98, 59568,    98, 59568,    98, 59568,\n",
      "            98, 59568,    98, 59568,    98, 59568,    98, 59568,    98, 59568,\n",
      "            98, 59568,    98, 59568,    98, 59568,    98, 59568,    98, 59568,\n",
      "            98, 59568,    98, 59568,    98, 59568,    98, 59568,    98, 59568,\n",
      "            98, 59568,    98, 59568,    98, 59568,    98, 59568,    98, 59568,\n",
      "            98, 59568,    98, 59568,    98, 59568,    98]], device='cuda:0')\n",
      "Fine-tuned model:\n",
      "我對不起，我�憶起你� 東西去 了。 �  。 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "{'en': 'According to what the old folks say, the winter solstice is more important than the Chinese New Year.', 'yue': '啲老人家話事偈，冬大過年。'}\n",
      "Prompt:\n",
      "Translate the following words from Cantonese to English:\n",
      "啲老人家話事偈，冬大過年。\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742, 26212,\n",
      "          2823, 59569,   592,  4750, 59601,   144,   534,   454,   483, 50873,\n",
      "         61845, 59736,   534,   434,   441,   101, 60820, 59647, 60799, 59663,\n",
      "           102,     7, 59568,   144,     6, 14135,   144,  1263, 26212,  2823,\n",
      "         59569, 14425,  1529,   534,   454,   483, 50873, 61845, 59736,   534,\n",
      "           434,   441,   101, 60820, 59647, 60799, 59663, 59592,   748,   629,\n",
      "         21096,  1029,  4750,   659,  1529,  2301, 59610, 59575,   567, 49066,\n",
      "           896,  1150,   567,  7247,    97,   597,   567,  7814,   620,  1212,\n",
      "          7577,   989,   567,  1876,  8567,    98, 59592,     7]],\n",
      "       device='cuda:0')\n",
      "Base model:\n",
      "The Cantonese phrase \"啲老人家話事偈，冬大過年\" can be translated into English as \"It's the elders who make the decisions, and the winter is even bigger than the New Year.\"\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926, 73580,  1994,   742,\n",
      "         26212,  2823, 59569,   592,  4750, 59601,   144,   534,   454,   483,\n",
      "         50873, 61845, 59736,   534,   434,   441,   101, 60820, 59647, 60799,\n",
      "         59663,   102,     7, 59568,   144,     6, 14135,   144,  1263, 21381,\n",
      "          1065,  1441,   639, 74112, 59647, 60799, 59663,   102, 59568, 65586,\n",
      "         59568, 73083, 59568,   536, 65474, 59568,   534, 69559, 59568,   101,\n",
      "           534,   453,   453, 59897, 70785, 59568,   102, 59568, 69012, 59568,\n",
      "           101,   534, 70520, 59568,   102, 59568,   531,   433,   436,   531,\n",
      "           433,   436,   531,   433,   436,   531,   433,   436,   531,   433,\n",
      "           436,   531,   433,   436,   531,   433,   436,   531,   433,   436,\n",
      "           531,   433,   436,   531,   433,   436,   531,   433,   436,   531,\n",
      "           433,   436,   531,   433,   436,   531,   433,   436,   531,   433,\n",
      "           436,   531,   433,   436,   531,   433,   436,   531,   433,   436,\n",
      "           531,   433,   436,   531,   433,   436,   531,   433]],\n",
      "       device='cuda:0')\n",
      "Fine-tuned model:\n",
      "The elderly people say that大過年。   � � ，唔使 。  ，� 。 ��������������������������������������������������������������\n",
      "\n",
      "Prompt:\n",
      "Translate the following words from English to Cantonese:\n",
      "According to what the old folks say, the winter solstice is more important than the Chinese New Year.\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742,  4750,\n",
      "           592, 26212,  2823, 59569, 59601,   144, 39559,   592,   981,   567,\n",
      "          1851,  8413,  1441,    97,   567,  7814,  1496,   608,   830,   620,\n",
      "           863,  1941,   989,   567,  6837,  1876,  8567,    98,     7, 59568,\n",
      "           144,     6, 14135,   144, 29630, 60913,   537,   493,   474, 59713,\n",
      "         60917,   101, 52776, 59806, 60333, 61905, 44386,   102,     7]],\n",
      "       device='cuda:0')\n",
      "Base model:\n",
      "根據長輩所說，冬至比春節更重要。\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926, 73580,  1994,   742,\n",
      "          4750,   592, 26212,  2823, 59569, 59601,   144, 39559,   592,   981,\n",
      "           567,  1851,  8413,  1441,    97,   567,  7814,  1496,   608,   830,\n",
      "           620,   863,  1941,   989,   567,  6837,  1876,  8567,    98,     7,\n",
      "         59568,   144,     6, 14135,   144, 29630, 50873, 62364,   101, 52776,\n",
      "         59806, 27480,  2377,   102, 59568,   536, 70520, 59568, 29630, 50873,\n",
      "         62364,   101, 52776, 59806, 27480,  2377,   102, 59568,   532,   434,\n",
      "           462, 63658, 62349,   532,   435,   452,   532,   434,   457,   532,\n",
      "           435,   438, 63887,   532,   434,   457,   532,   435,   438,   532,\n",
      "           435,   452,   532,   434,   474,   532,   434,   457,   532,   435,\n",
      "           438, 63658,   532,   434,   441, 62497,   532,   434,   465, 62497,\n",
      "           532,   434,   465, 62497,   532,   434,   465, 62497,   532,   434,\n",
      "           465, 62497,   532,   434,   465, 62497,   532,   434,   465, 62497,\n",
      "           532,   434,   465, 62497,   532,   434,   465, 62497,   532,   434,\n",
      "           465, 62497,   532,   434,   465]], device='cuda:0')\n",
      "Fine-tuned model:\n",
      "根據老人家講，冬至比新年重要。 � 根據老人家講，冬至比新年重要。 そうしんじゅくじゅんどじゅうえいだいだいだいだいだいだいだいだいだいだ\n",
      "\n",
      "{'en': \"Tonight we've been invited to a wedding banquet.\", 'yue': '今晚有人請飲。'}\n",
      "Prompt:\n",
      "Translate the following words from Cantonese to English:\n",
      "今晚有人請飲。\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742, 26212,\n",
      "          2823, 59569,   592,  4750, 59601,   144, 33490,  5189, 61919, 63105,\n",
      "           102,     7, 59568,   144,     6, 14135,   144, 59592, 33490,  5189,\n",
      "         61919, 63105,   102, 59592, 59568,   144,  7759,  4541, 59601,  1529,\n",
      "         59600, 51152,  2491,   620, 28516,   631, 14519,    98, 59592,     7]],\n",
      "       device='cuda:0')\n",
      "Base model:\n",
      "\"今晚有人請飲。\" \n",
      "Translation: \"Tonight someone is inviting for drinks.\"\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926, 73580,  1994,   742,\n",
      "         26212,  2823, 59569,   592,  4750, 59601,   144, 33490,  5189, 61919,\n",
      "         63105,   102,     7, 59568,   144,     6, 14135,   144,  7759,  4541,\n",
      "         59601, 39530,  2491, 32527,   795,   592,  4561,    98, 59640,  7025,\n",
      "         59631,  2085, 59640,  5684, 59631, 33490,  5189, 61919, 63105,   102,\n",
      "         59640,  7025, 59631,  2085, 59640,  5684, 59631, 33490,  5189, 61919,\n",
      "         59646, 63105, 60357,   102, 59640,  7025, 59631,  2085, 59640,  5684,\n",
      "         59631, 59600, 51152,  2491, 12456,   795,   592,  4561,    98, 59640,\n",
      "          7025, 59631,  2085, 59640,  5684, 59631, 33490,  5189, 61919, 59646,\n",
      "         63105,   102, 59640,  7025, 59631,  2085, 59640,  5684, 59631, 33490,\n",
      "          5189, 61919, 59646, 63105, 60357,   102, 59640,  7025, 59631,  2085,\n",
      "         59640,  5684, 59631, 59600, 51152,  2491, 12456,   795,   592,  4561,\n",
      "            98, 59640,  7025, 59631,  2085, 59640,  5684, 59631]],\n",
      "       device='cuda:0')\n",
      "Fine-tuned model:\n",
      "Translation: Tonight someone invites me to drink.&lt;br&gt;今晚有人請飲。&lt;br&gt;今晚有人請我飲酒。&lt;br&gt;Tonight someone invited me to drink.&lt;br&gt;今晚有人請我飲。&lt;br&gt;今晚有人請我飲酒。&lt;br&gt;Tonight someone invited me to drink.&lt;br&gt;\n",
      "\n",
      "Prompt:\n",
      "Translate the following words from English to Cantonese:\n",
      "Tonight we've been invited to a wedding banquet.\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742,  4750,\n",
      "           592, 26212,  2823, 59569, 59601,   144, 59600, 51152,   666, 59610,\n",
      "           613,   961, 12456,   592,   562,  8760,  7255,  3134,    98,     7,\n",
      "         59568,   144,     6, 14135,   144, 33490,  8506, 59777, 54280, 31985,\n",
      "         60424, 62101,   102,     7]], device='cuda:0')\n",
      "Base model:\n",
      "今晚我們被邀請參加婚宴。\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926, 73580,  1994,   742,\n",
      "          4750,   592, 26212,  2823, 59569, 59601,   144, 59600, 51152,   666,\n",
      "         59610,   613,   961, 12456,   592,   562,  8760,  7255,  3134,    98,\n",
      "             7, 59568,   144,     6, 14135,   144, 33490,  8506, 59777, 54280,\n",
      "         31985, 60424, 62101,   102,     7]], device='cuda:0')\n",
      "Fine-tuned model:\n",
      "今晚我們被邀請參加婚宴。\n",
      "\n",
      "{'en': 'He has not yet been to school, or He has not studied for it yet.', 'yue': '佢未讀書。'}\n",
      "Prompt:\n",
      "Translate the following words from Cantonese to English:\n",
      "佢未讀書。\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742, 26212,\n",
      "          2823, 59569,   592,  4750, 59601,   144,   533,   494,   467, 60049,\n",
      "         62283, 61473,   102,     7, 59568,   144,     6, 14135,   144,  4431,\n",
      "           815,   728,  1326,  3997,    98,     7]], device='cuda:0')\n",
      "Base model:\n",
      "He has not read books.\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926, 73580,  1994,   742,\n",
      "         26212,  2823, 59569,   592,  4750, 59601,   144,   533,   494,   467,\n",
      "         60049, 62283, 61473,   102,     7, 59568,   144,     6, 14135,   144,\n",
      "          4431,   815,   728,  1326,  3997,    98, 59568, 59686, 60049, 62283,\n",
      "         61473,   102, 59568, 59686, 60049, 62283, 61473,   102, 59568, 59686,\n",
      "         60049, 62283, 61473,   102, 59568, 59686, 60049, 62283, 61473,   102,\n",
      "         59568, 59686, 60049, 62283, 61473,   102, 59568, 59686, 60049, 62283,\n",
      "         61473,   102, 59568, 59686, 60049, 62283, 61473,   102, 59568, 59686,\n",
      "         60049, 62283, 61473,   102, 59568, 59686, 60049, 62283, 61473,   102,\n",
      "         59568, 59686, 60049, 62283, 61473,   102, 59568, 59686, 60049, 62283,\n",
      "         61473,   102, 59568, 59686, 60049, 62283, 61473,   102, 59568, 59686,\n",
      "         60049, 62283, 61473,   102, 59568, 59686, 60049, 62283, 61473,   102,\n",
      "         59568, 59686, 60049, 62283, 61473,   102, 59568, 59686, 60049, 62283]],\n",
      "       device='cuda:0')\n",
      "Fine-tuned model:\n",
      "He has not read books. 他未讀書。 他未讀書。 他未讀書。 他未讀書。 他未讀書。 他未讀書。 他未讀書。 他未讀書。 他未讀書。 他未讀書。 他未讀書。 他未讀書。 他未讀書。 他未讀書。 他未讀書。 他未讀\n",
      "\n",
      "Prompt:\n",
      "Translate the following words from English to Cantonese:\n",
      "He has not yet been to school, or He has not studied for it yet.\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742,  4750,\n",
      "           592, 26212,  2823, 59569, 59601,   144,  4431,   815,   728,  2583,\n",
      "           961,   592,  1923,    97,   705,  1214,   815,   728,  7945,   631,\n",
      "           648,  2583,    98,     7, 59568,   144,     6, 14135,   144, 59686,\n",
      "         61146, 60049, 59654, 60579,   101, 59876, 59686, 61146, 60049, 60379,\n",
      "         60565,  9605, 39976, 59706,   102,     7]], device='cuda:0')\n",
      "Base model:\n",
      "他還未上學，或他還未為這件事準備好。\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926, 73580,  1994,   742,\n",
      "          4750,   592, 26212,  2823, 59569, 59601,   144,  4431,   815,   728,\n",
      "          2583,   961,   592,  1923,    97,   705,  1214,   815,   728,  7945,\n",
      "           631,   648,  2583,    98,     7, 59568,   144,     6, 14135,   144,\n",
      "         59686, 60049, 68138, 24583,   101,  2850, 60049,   534, 72202, 62283,\n",
      "         60799,   102,   141, 59640,  7025, 59631,  2085, 59640,  5684, 59631,\n",
      "         59568,   533, 75701, 59568, 59686, 60049, 59793, 60799, 24583,   101,\n",
      "          2850, 60049, 62283, 60799,   102,   141, 59640,  7025, 59631,  2085,\n",
      "         59640,  5684, 59631, 59568, 59686, 60049, 59793, 60799, 24583,   101,\n",
      "          2850, 60049, 62283, 60799,   102,   141, 59640,  7025, 59631,  2085,\n",
      "         59640,  5684, 59631, 59568, 59686, 60049, 59793, 60799, 24583,   101,\n",
      "          2850, 60049, 62283, 60799,   102,   141, 59640,  7025, 59631,  2085,\n",
      "         59640,  5684, 59631, 59568, 59686, 60049, 59793, 60799, 24583,   101,\n",
      "          2850, 60049, 62283, 60799,   102,   141, 59640,  7025, 59631,  2085]],\n",
      "       device='cuda:0')\n",
      "Fine-tuned model:\n",
      "他未學校，或者未�讀過。  &lt;br&gt; � 他未去過學校，或者未讀過。  &lt;br&gt; 他未去過學校，或者未讀過。  &lt;br&gt; 他未去過學校，或者未讀過。  &lt;br&gt; 他未去過學校，或者未讀過。  &lt;br\n",
      "\n",
      "{'en': 'Back at that time I worked as a one-star probationary police inspector, I earned twenty to thirty thousand bucks a month.', 'yue': '嗰陣時我做過一粒花嘅幫辦仔，搵兩三皮一個月。'}\n",
      "Prompt:\n",
      "Translate the following words from Cantonese to English:\n",
      "嗰陣時我做過一粒花嘅幫辦仔，搵兩三皮一個月。\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742, 26212,\n",
      "          2823, 59569,   592,  4750, 59601,   144,   534,   456,   481, 63598,\n",
      "         60531, 58409, 60799, 59625, 61329, 60048,   534,   457,   438, 62592,\n",
      "         61778, 61365,   101,   535,   449,   486, 61348, 59799, 60445, 12666,\n",
      "         59751,   102,     7, 59568,   144,     6, 14135,   144, 41910,   639,\n",
      "           922,    97,   616,  3983,   659,   562, 18063,  7492,  5576,    97,\n",
      "          2160,   883,  1074,   705,  1604,  7883,   562,  1780,    98,     7]],\n",
      "       device='cuda:0')\n",
      "Base model:\n",
      "During that time, I worked as a flower delivery boy, making about two or three dollars a month.\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926, 73580,  1994,   742,\n",
      "         26212,  2823, 59569,   592,  4750, 59601,   144,   534,   456,   481,\n",
      "         63598, 60531, 58409, 60799, 59625, 61329, 60048,   534,   457,   438,\n",
      "         62592, 61778, 61365,   101,   535,   449,   486, 61348, 59799, 60445,\n",
      "         12666, 59751,   102,     7, 59568,   144,     6, 14135,   144,  7759,\n",
      "          4541, 59601,  2364,   616,  3983,   659,   562, 39319,   631,   562,\n",
      "         18063, 34865,    97,   616, 10989,  1501,  1074,   705,  1604,  7883,\n",
      "           562,  1780,    98, 59568, 70982, 59568, 62376, 60352, 62592, 61778,\n",
      "         61365, 60353,   101, 66265, 59568, 62376, 60352, 62592, 59845, 60353,\n",
      "           101, 60352, 62592, 61778, 60353, 62376, 60352, 62592, 59845, 60353,\n",
      "           534, 73857, 59568, 62376, 60352, 12666, 59751, 60353,   101, 59568,\n",
      "         62376, 60352, 59751, 60353,   102, 59568,  3215,   101, 59568,  1229,\n",
      "          5181, 60379, 60352, 59842, 59625, 61329, 60048,   534,   457,   438,\n",
      "         62592, 59845,   101,   535,   451,   441, 61348, 59799, 35542, 60353,\n",
      "           102, 59568,  1229, 60995, 59568,  5181, 60379, 60352, 62592]],\n",
      "       device='cuda:0')\n",
      "Fine-tuned model:\n",
      "Translation: When I worked as a helper for a flower stall, I earned around two or three dollars a month.  係「幫辦仔」， 係「幫手」，「幫辦」係「幫手」� 係「一個月」， 係「月」。 因此， 可以理解為「做一粒花嘅幫手，撈兩三個月」。 可以將 理解為「幫\n",
      "\n",
      "Prompt:\n",
      "Translate the following words from English to Cantonese:\n",
      "Back at that time I worked as a one-star probationary police inspector, I earned twenty to thirty thousand bucks a month.\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742,  4750,\n",
      "           592, 26212,  2823, 59569, 59601,   144, 10298,   702,   639,   922,\n",
      "           616,  3983,   659,   562,   853, 59594,  6481, 40897,   898,  4031,\n",
      "         50801,    97,   616, 10989, 12445,   592, 19566, 12340, 25883,   562,\n",
      "          1780,    98,     7, 59568,   144,     6, 14135,   144, 59632, 61720,\n",
      "         14695, 61282,   101, 59646, 26747,  8399, 59625, 60273, 62007, 59599,\n",
      "         61044, 61748, 12261, 42994,   101, 17243,   537,   484,   491, 60021,\n",
      "         59853, 61515, 59905, 59799, 61515, 39712,   102,     7]],\n",
      "       device='cuda:0')\n",
      "Base model:\n",
      "在該時間點，我作為一名一星級的實習警察督察，每月賺取二萬至三萬港元。\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926, 73580,  1994,   742,\n",
      "          4750,   592, 26212,  2823, 59569, 59601,   144, 10298,   702,   639,\n",
      "           922,   616, 73580, 17029,   659,   562,   853, 59594,  6481, 40897,\n",
      "           898,  4031, 50801,    97,   616, 10989, 12445,   592, 19566, 12340,\n",
      "         25883,   562,  1780,    98,     7, 59568,   144,     6, 14135,   144,\n",
      "          7759,  4541,   742,  4750,   592, 26212,  2823, 59569, 59601,   144,\n",
      "         61007, 59663, 59646, 62376, 59625, 60273, 62007, 61584, 61748, 60298,\n",
      "         60913,   101, 59751, 61476, 59635,   534, 77128, 59568, 59905, 59568,\n",
      "            80,    77,    97,    77,    77,    77, 59568, 62806, 64868, 59568,\n",
      "           102, 59568, 60353, 59568, 60353, 59568, 60353, 59568, 60353, 59568,\n",
      "         60353, 59568, 60353, 59568, 60353, 59568, 60353, 59568, 60353, 59568,\n",
      "         60353, 59568, 60353, 59568, 60353, 59568, 60353, 59568, 60353, 59568,\n",
      "         60353, 59568, 60353, 59568, 60353, 59568, 60353, 59568, 60353, 59568,\n",
      "         60353, 59568, 60353, 59568, 60353, 59568, 60353, 59568, 60353, 59568,\n",
      "         60353, 59568, 60353, 59568, 60353, 59568, 60353, 59568, 60353, 59568]],\n",
      "       device='cuda:0')\n",
      "Fine-tuned model:\n",
      "Translation from English to Cantonese:\n",
      "當年我係一星級見習警長，月薪有� 至 30,000 蚊 。 」 」 」 」 」 」 」 」 」 」 」 」 」 」 」 」 」 」 」 」 」 」 」 」 」 」 」 」 」 \n",
      "\n",
      "{'en': \"Why does everyone have it, but I'm the only who doesn't?\", 'yue': '點解個個都有，得我一個冇？'}\n",
      "Prompt:\n",
      "Translate the following words from Cantonese to English:\n",
      "點解個個都有，得我一個冇？\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742, 26212,\n",
      "          2823, 59569,   592,  4750, 59601,   144, 61282, 59862, 60588, 60588,\n",
      "          5074,   101, 59721, 59646, 12666,   534,   439,   440,   104,     7,\n",
      "         59568,   144,     6, 14135,   144, 59592, 61282, 59862, 60588, 60588,\n",
      "          5074,   101, 59721, 59646, 12666,   534,   439,   440,   104, 59592,\n",
      "         59568,  1229, 15480, 59648,  1529, 47888, 51284, 12572,  4299,   101,\n",
      "          3481, 59646, 10870,   104, 59592,     7]], device='cuda:0')\n",
      "Base model:\n",
      "\"點解個個都有，得我一個冇？\" 可以翻译为 \"為什麼每個人都有的，只有我沒有？\"\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926, 73580,  1994,   742,\n",
      "         26212,  2823, 59569,   592,  4750, 59601,   144, 61282, 59862, 60588,\n",
      "         60588,  5074,   101, 59721, 59646, 12666,   534,   439,   440,   104,\n",
      "             7, 59568,   144,     6, 14135,   144, 59592, 61282, 59862, 60588,\n",
      "         60588,  5074,   101, 59721, 59646, 12666,   534, 69559,   104, 59592,\n",
      "         59568,  1229, 15480, 59687,  4750,  1529, 12665,   620,   648,   639,\n",
      "          2983,   815,   853,    97,   796,   616,  1182, 59610, 59570,   100,\n",
      "         59592, 59568,  2850,  1529, 12665,   620,   648,   639,  2983,   815,\n",
      "           853,    97,   796,   616,  1182, 59610, 59570,   100, 59592, 59568,\n",
      "          2850,  1529, 12665,   620,   648,   639,  2983,   815,   853,    97,\n",
      "           796,   616,  1182, 59610, 59570,   100, 59592, 59568,  2850,  1529,\n",
      "         12665,   620,   648,   639,  2983,   815,   853,    97,   796,   616,\n",
      "          1182, 59610, 59570,   100, 59592, 59568,  2850,  1529, 12665,   620,\n",
      "           648,   639,  2983,   815,   853,    97]], device='cuda:0')\n",
      "Fine-tuned model:\n",
      "\"點解個個都有，得我一個�？\" 可以翻译成 English \"Why is it that everyone has one, but I don't?\" 或者 \"Why is it that everyone has one, but I don't?\" 或者 \"Why is it that everyone has one, but I don't?\" 或者 \"Why is it that everyone has one, but I don't?\" 或者 \"Why is it that everyone has one,\n",
      "\n",
      "Prompt:\n",
      "Translate the following words from English to Cantonese:\n",
      "Why does everyone have it, but I'm the only who doesn't?\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742,  4750,\n",
      "           592, 26212,  2823, 59569, 59601,   144, 12665,  1269,  2983,   726,\n",
      "           648,    97,   796,   616, 59610, 59583,   567,  1035,   896,  2153,\n",
      "         59610, 59570,   100,     7, 59568,   144,     6, 14135,   144, 59632,\n",
      "         21931,  5074,   101, 31179, 62072, 59626,  8711, 10870,  1702,   104,\n",
      "             7]], device='cuda:0')\n",
      "Base model:\n",
      "在所有人都有，而我卻是唯一沒有的人？\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926, 73580,  1994,   742,\n",
      "          4750,   592, 26212,  2823, 59569, 59601,   144, 12665,  1269,  2983,\n",
      "           726,   648,    97,   796,   616, 59610, 59583,   567,  1035,   896,\n",
      "          2153, 59610, 59570,   100,     7, 59568,   144,     6, 14135,   144,\n",
      "         59592, 12665,  1269,  2983,   726,   648,    97,   796,   616, 59610,\n",
      "         59583,   567,  1035,   853,   896,  2153, 59610, 59570,   100, 59592,\n",
      "         59568,  1229, 15480, 59842,   106, 60352, 60379, 60032, 18919,  5074,\n",
      "           101, 59784,  8081,   534, 64368,  8711, 12666, 61133,   104, 60353,\n",
      "           102, 59608,   727,  2823, 59569, 59568, 23072,  1529, 64409, 59592,\n",
      "         59568,  2850,  1529, 59592, 59568, 60599, 56983, 60352,  8711, 60353,\n",
      "         20978,   102,  1746, 59822, 56983, 60352, 60379, 60032, 60353,   101,\n",
      "          1229, 15480, 59842,   106, 60352, 60379, 60032, 18919,  5074,   101,\n",
      "         16097, 74063, 59568, 73083,  8711, 12666, 61133,   104, 60353,   102,\n",
      "         59608,   727,  2823, 59569, 59568, 23072,  1529, 59592, 59568,  2850]],\n",
      "       device='cuda:0')\n",
      "Fine-tuned model:\n",
      "\"Why does everyone have it, but I'm the only one who doesn't?\" 可以翻译做：「為何人人都有，但我就�唯一一個無？」。Cantonese 可以用 \"\" 或者 \"\" 來表達「唯一」的意思。如果想表達「為何」，可以翻译做：「為何人人都有，但我 唯一一個無？」。Cantonese 可以用 \"\" 或者\n",
      "\n",
      "{'en': \"You don't need to be concerned about the fall in the value of the US dollar, as I still know how to deal in foreign currency and make money.\", 'yue': '你唔使擔心美元嘅貶值，我仲識套匯賺錢。'}\n",
      "Prompt:\n",
      "Translate the following words from Cantonese to English:\n",
      "你唔使擔心美元嘅貶值，我仲識套匯賺錢。\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742, 26212,\n",
      "          2823, 59569,   592,  4750, 59601,   144, 59725,   534,   453,   453,\n",
      "         59897, 62471, 59712,  5061,   534,   457,   438,   537,   483,   487,\n",
      "         60110,   101, 59646, 61669, 62028, 60494, 63664,   537,   484,   491,\n",
      "         62617,   102,     7, 59568,   144,     6, 14135,   144,  3961,  1182,\n",
      "         59610, 59570,   726,   592,  6761,   883,   567,  1825, 52704,   593,\n",
      "           567, 11790,    97,   616,  1451,  1082,  1040,   592,  4902,   823,\n",
      "         12959,  6778,   597,  1150,  2166,    98,     7]], device='cuda:0')\n",
      "Base model:\n",
      "You don't have to worry about the depreciation of the dollar, I still know how to carry out currency exchange and make money.\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926, 73580,  1994,   742,\n",
      "         26212,  2823, 59569,   592,  4750, 59601,   144, 59725,   534,   453,\n",
      "           453, 59897, 62471, 59712,  5061,   534,   457,   438,   537,   483,\n",
      "           487, 60110,   101, 59646, 61669, 62028, 60494, 63664,   537,   484,\n",
      "           491, 62617,   102,     7, 59568,   144,     6, 14135,   144,  3961,\n",
      "          1182, 59610, 59570,  1049,   592,  6761,   883,   567,  1825, 52704,\n",
      "           593,   567,  2134, 11790, 59631,   616,   748,  1212,  1150,  2166,\n",
      "           737, 53876, 36072,    98, 59568,   536, 65474, 59568, 59727, 59568,\n",
      "           102, 59568,   102, 59568,   102, 59568,   102, 59568,   102, 59568,\n",
      "           102, 59568,   102, 59568,   102, 59568,   102, 59568,   102, 59568,\n",
      "           102, 59568,   102, 59568,   102, 59568,   102, 59568,   102, 59568,\n",
      "           102, 59568,   102, 59568,   102, 59568,   102, 59568,   102, 59568,\n",
      "           102, 59568,   102, 59568,   102, 59568,   102, 59568,   102, 59568,\n",
      "           102, 59568,   102, 59568,   102, 59568,   102, 59568,   102, 59568,\n",
      "           102, 59568,   102, 59568,   102, 59568,   102, 59568,   102]],\n",
      "       device='cuda:0')\n",
      "Fine-tuned model:\n",
      "You don't need to worry about the depreciation of the US dollar; I can even make money by exchanging currencies. � 同 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。\n",
      "\n",
      "Prompt:\n",
      "Translate the following words from English to Cantonese:\n",
      "You don't need to be concerned about the fall in the value of the US dollar, as I still know how to deal in foreign currency and make money.\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742,  4750,\n",
      "           592, 26212,  2823, 59569, 59601,   144,  3961,  1182, 59610, 59570,\n",
      "          1049,   592,   629,  7703,   883,   567,  3038,   594,   567,  1830,\n",
      "           593,   567,  2134, 11790,    97,   659,   616,  1451,  1082,  1040,\n",
      "           592,  2527,   594,  6485, 12959,   597,  1150,  2166,    98,     7,\n",
      "         59568,   144,     6, 14135,   144, 59725,   534,   453,   453,  1867,\n",
      "         62471, 59712, 13075,  5061,   534,   457,   438, 34455, 11818,   101,\n",
      "         12354, 59646,  8173, 62028, 31063, 59820, 62861, 59727,   537,   484,\n",
      "           491, 62617,   102,     7]], device='cuda:0')\n",
      "Base model:\n",
      "你唔需要擔心美國美元嘅價值下跌，因為我仍然識處理外幣同賺錢。\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926, 73580,  1994,   742,\n",
      "          4750,   592, 26212,  2823, 59569, 59601,   144,  3961,  1182, 59610,\n",
      "         59570,  1049,   592,   629,  7703,   883,   567,  3038,   594,   567,\n",
      "          1830,   593,   567,  2134, 11790,    97,   659,   616,  1451,  1082,\n",
      "          1040,   592,  2527,   594,  6485, 12959,   597,  1150,  2166,    98,\n",
      "             7, 59568,   144,     6, 14135,   144, 59725,   534, 70520, 59568,\n",
      "           534, 70520, 59568, 13075,   538, 75701, 59568, 60110, 60740,   534,\n",
      "         70375,   101, 12354, 59646,  8173,  2222,  3152, 31063, 59820, 62861,\n",
      "           101, 44089,   537, 72202, 62617,   102,     7]], device='cuda:0')\n",
      "Fine-tuned model:\n",
      "你� � 美國� 值跌�，因為我仍然知道如何處理外幣，並且�錢。\n",
      "\n",
      "{'en': 'The two teams of the company kicked up a ruckus and the big boss had to settle it personally.', 'yue': '公司兩組人炒大鑊，要由大老細出面擺平。'}\n",
      "Prompt:\n",
      "Translate the following words from Cantonese to English:\n",
      "公司兩組人炒大鑊，要由大老細出面擺平。\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742, 26212,\n",
      "          2823, 59569,   592,  4750, 59601,   144,  1424, 61348, 61728, 59634,\n",
      "         61241, 59647,   538,   450,   443,   101, 59662, 59903, 59647, 59857,\n",
      "         62500, 59676, 59724,   535,   452,   491, 59849,   102,     7, 59568,\n",
      "           144,     6, 14135,   144, 45904,  1074,  3953,   593,  1065,   678,\n",
      "           594,  2987,  2127,    97,   597,   648,  5293,   567,  7639,  4330,\n",
      "           592,  3201,   594,   592, 13414,   567,  4007,    98,     7]],\n",
      "       device='cuda:0')\n",
      "Base model:\n",
      "Company two groups of people are in hot water, and it requires the senior management to step in to resolve the situation.\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926, 73580,  1994,   742,\n",
      "         26212,  2823, 59569,   592,  4750, 59601,   144,  1424, 61348, 61728,\n",
      "         59634, 61241, 59647,   538,   450,   443,   101, 59662, 59903, 59647,\n",
      "         59857, 62500, 59676, 59724,   535,   452,   491, 59849,   102,     7,\n",
      "         59568,   144,     6, 14135,   144,  2034,  4750,    97,   567,  9554,\n",
      "         12433, 59601,  1529, 19890,  3953,   593,  1065,   702,   567,  2165,\n",
      "           678,   594,   562,  4323,    97,   597,   567,  2566,  2599,   592,\n",
      "           629, 15986,   737,   567,  1806,  6442,    98, 59592,     7]],\n",
      "       device='cuda:0')\n",
      "Fine-tuned model:\n",
      "In English, the sentence reads: \"Two groups of people at the company are in a mess, and the matter needs to be resolved by the head manager.\"\n",
      "\n",
      "Prompt:\n",
      "Translate the following words from English to Cantonese:\n",
      "The two teams of the company kicked up a ruckus and the big boss had to settle it personally.\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742,  4750,\n",
      "           592, 26212,  2823, 59569, 59601,   144,  1263,  1074,  5155,   593,\n",
      "           567,  2165, 19591,   828,   562,   731,  1836,   633,   597,   567,\n",
      "          1741, 10920,   986,   592, 14738,   648,  9363,    98,     7, 59568,\n",
      "           144,     6, 14135,   144, 59632,  8385, 61348, 61791,   538,   477,\n",
      "           472, 16729,   534,   447,   483, 60323,   101, 59647, 59857,   538,\n",
      "           456,   439, 12011, 62061, 59689, 35754,   102,     7]],\n",
      "       device='cuda:0')\n",
      "Base model:\n",
      "在公司的兩隊鬧得很厲害，大老闆不得不親自解決。\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926, 73580,  1994,   742,\n",
      "          4750,   592, 26212,  2823, 59569, 59601,   144,  1263,  1074,  5155,\n",
      "           593,   567,  2165, 19591,   828,   562,   731,  1836,   633,   597,\n",
      "           567,  1741, 10920,   986,   592, 14738,   648,  9363,    98,     7,\n",
      "         59568,   144,     6, 14135,   144, 68871, 61791, 27095, 61148, 59676,\n",
      "           534, 70520, 59568, 74167, 59568, 59727, 59647, 59857,   538, 75701,\n",
      "         59568, 59662, 62061, 59689, 74167, 59568,   102,   141, 59640,  7025,\n",
      "         59631,  2085, 59640,  5684, 59631,   889,  7025, 59631,  2085, 59640,\n",
      "          5684, 59631,   889,  7025, 59631,  2085, 59640,  5684, 59631,   889,\n",
      "          7025, 59631,  2085, 59640,  5684, 59631,   889,  7025, 59631,  2085,\n",
      "         59640,  5684, 59631,   889,  7025, 59631,  2085, 59640,  5684, 59631,\n",
      "           889,  7025, 59631,  2085, 59640,  5684, 59631,   889,  7025, 59631,\n",
      "          2085, 59640,  5684, 59631,   889,  7025, 59631,  2085, 59640,  5684,\n",
      "         59631,   889,  7025, 59631,  2085, 59640,  5684, 59631,   889,  7025,\n",
      "         59631,  2085, 59640,  5684, 59631]], device='cuda:0')\n",
      "Fine-tuned model:\n",
      "隊的公司搞出�  同大老� 要親自 。  &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt;\n",
      "\n",
      "{'en': \"Don't you worry, I wouldn't give out false information to everybody.\", 'yue': '你哋放心啦，我唔會放山貼士畀大家嘅。'}\n",
      "Prompt:\n",
      "Translate the following words from Cantonese to English:\n",
      "你哋放心啦，我唔會放山貼士畀大家嘅。\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742, 26212,\n",
      "          2823, 59569,   592,  4750, 59601,   144, 59725,   534,   452,   444,\n",
      "         17294, 61332,   101, 59646,   534,   453,   453, 60418, 59932, 59960,\n",
      "         63353, 60222,   536,   454,   433,  2243,   534,   457,   438,   102,\n",
      "             7, 59568,   144,     6, 14135,   144,  3961,  4227,   748,  1797,\n",
      "         19504,    97,   616,  2552, 59610, 59570, 17335,   567,  7610,   592,\n",
      "          2983,    98,     7]], device='cuda:0')\n",
      "Base model:\n",
      "You guys can rest assured, I won't leak the tips to everyone.\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926, 73580,  1994,   742,\n",
      "         26212,  2823, 59569,   592,  4750, 59601,   144, 59725,   534,   452,\n",
      "           444, 17294, 61332,   101, 59646,   534,   453,   453, 60418, 59932,\n",
      "         59960, 63353, 60222,   536,   454,   433,  2243,   534,   457,   438,\n",
      "           102,     7, 59568,   144,     6, 14135,   144,  3961,   748,  1797,\n",
      "         19504,    97,   616,  2552, 59610, 59570,  3617,   641, 74112, 59575,\n",
      "          3328,    98, 59568, 60629, 17294,   101, 59646,   534,   458,   442,\n",
      "           534, 72878, 63353, 60222,   536,   458,   479,   534,   455,   491,\n",
      "          2243,   534,   458,   442,   102, 59568, 60629,   534,   457,   438,\n",
      "          2046, 59646,   534, 69559, 62860, 61897,   102, 59568, 60629,   534,\n",
      "           457,   438,  2046, 59646,   534,   458,   442, 62860, 61897,   102,\n",
      "         59568, 60629,   534,   457,   438,  2046, 59646,   536, 65474, 62860,\n",
      "         61897,   102, 59568, 60629,   534,   457,   438,  2046, 59646, 59568,\n",
      "         62860, 61897, 62860, 61897, 62860, 61897, 62860, 61897, 62860, 61897,\n",
      "         62860, 61897, 62860, 61897, 62860, 61897, 62860]], device='cuda:0')\n",
      "Fine-tuned model:\n",
      "You can rest assured, I won't leave yous behind. 您放心，我����貼士癮喺大家噉。 您嘅安全我�緊張。 您嘅安全我噉緊張。 您嘅安全我�緊張。 您嘅安全我 緊張緊張緊張緊張緊張緊張緊張緊張緊\n",
      "\n",
      "Prompt:\n",
      "Translate the following words from English to Cantonese:\n",
      "Don't you worry, I wouldn't give out false information to everybody.\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742,  4750,\n",
      "           592, 26212,  2823, 59569, 59601,   144,  9550, 59610, 59570,   641,\n",
      "          6761,    97,   616,  4781, 59610, 59570,  1863,   823,  3876,  1585,\n",
      "           592, 11091,    98,     7, 59568,   144,     6, 14135,   144, 59632,\n",
      "           534,   453,   453, 62471, 59712,   101, 59646,   534,   453,   453,\n",
      "         60418, 60995,   534,   454,   483, 60471,   534,   457,   467,   536,\n",
      "           454,   433, 59634, 59861,   102,     7]], device='cuda:0')\n",
      "Base model:\n",
      "在唔擔心，我唔會將啲假嘢畀人知。\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926, 73580,  1994,   742,\n",
      "          4750,   592, 26212,  2823, 59569, 59601,   144,  9550, 59610, 59570,\n",
      "           641, 73580,   851,    97,   616,  4781, 59610, 59570,  1863,   823,\n",
      "          3876,  1585,   592, 11091,    98,     7, 59568,   144,     6, 14135,\n",
      "           144,   536, 74948, 59725,   534, 70520, 59568,   534, 70520, 59568,\n",
      "           101, 59646,   534, 75701, 59568,   533, 75701, 59568, 59676,   534,\n",
      "         70375, 59568,   534, 75701, 59568, 35377,   536, 65686, 59568, 68426,\n",
      "         59568,   102,   141, 59640,  7025, 59631,  2085, 59640,  5684, 59631,\n",
      "         59568, 15480,   106, 59725,  3269, 62471, 59712,   101, 59646, 62635,\n",
      "         60713, 27509, 62231, 60150, 60624, 60556, 63480, 60471, 35377,   102,\n",
      "         59640,  7025, 59631,  2085, 59640,  5684, 59631, 59568, 15480,   106,\n",
      "         59725,  3269, 62471, 59712,   101, 59646, 62635, 60713, 27509, 62231,\n",
      "         60150, 60624, 60556, 63480, 60471, 35377,   102, 59640,  7025, 59631,\n",
      "          2085, 59640,  5684, 59631, 59568, 15480,   106, 59725,  3269, 62471,\n",
      "         59712]], device='cuda:0')\n",
      "Fine-tuned model:\n",
      "�你� � ，我� � 出� � 資訊�  。  &lt;br&gt; 翻译：你不要擔心，我絕對不會隨便散播虛假資訊。&lt;br&gt; 翻译：你不要擔心，我絕對不會隨便散播虛假資訊。&lt;br&gt; 翻译：你不要擔心\n",
      "\n",
      "{'en': 'Somebody told me a few days ago his wife came to this hotel and created a scene at a tryst he had with his girlfriend here.', 'yue': '我聽人哋話，前幾日佢太太嚟呢間酒店踢竇。'}\n",
      "Prompt:\n",
      "Translate the following words from Cantonese to English:\n",
      "我聽人哋話，前幾日佢太太嚟呢間酒店踢竇。\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742, 26212,\n",
      "          2823, 59569,   592,  4750, 59601,   144, 59646, 62322, 59634,   534,\n",
      "           452,   444, 61845,   101, 59742, 62037, 59720,   533,   494,   467,\n",
      "         23733,   534,   459,   464, 60329, 61085,  9499, 62328,   536,   476,\n",
      "           440,   102,     7, 59568,   144,     6, 14135,   144, 59646, 62322,\n",
      "         59634,   534,   452,   444, 61845,   101, 59742, 62037, 59720,   533,\n",
      "           494,   467, 23733,   534,   459,   464, 60329, 61085,  9499, 62328,\n",
      "           536,   476,   440,   102,     7]], device='cuda:0')\n",
      "Base model:\n",
      "我聽人哋話，前幾日佢太太嚟呢間酒店踢竇。\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926, 73580,  1994,   742,\n",
      "         26212,  2823, 59569,   592,  4750, 59601,   144, 59646, 62322, 59634,\n",
      "           534,   452,   444, 61845,   101, 59742, 62037, 59720,   533,   494,\n",
      "           467, 23733,   534,   459,   464, 60329, 61085,  9499, 62328,   536,\n",
      "           476,   440,   102,     7, 59568,   144,     6, 14135,   144,  6056,\n",
      "          2891,  2622,   795,   639,  1019,  5018,  2514,   592,   719,  6622,\n",
      "           567,   924,  1448,   597,   986,  7807,    98, 59568,   100,   100,\n",
      "           100,   100,   100,   100,   100,   100,   100,   100,   100,   100,\n",
      "           100,   100,   100,   100,   100,   100,   100,   100,   100,   100,\n",
      "           100,   100,   100,   100,   100,   100,   100,   100,   100,   100,\n",
      "           100,   100,   100,   100,   100,   100,   100,   100,   100,   100,\n",
      "           100,   100,   100,   100,   100,   100,   100,   100,   100,   100,\n",
      "           100,   100,   100,   100,   100,   100,   100,   100,   100,   100,\n",
      "           100,   100,   100,   100,   100,   100,   100,   100,   100,   100,\n",
      "           100,   100,   100,   100,   100,   100,   100,   100,   100]],\n",
      "       device='cuda:0')\n",
      "Fine-tuned model:\n",
      "My friends told me that her wife came to this hotel the other day and had trouble. ?????????????????????????????????????????????????????????????????????????????????\n",
      "\n",
      "Prompt:\n",
      "Translate the following words from English to Cantonese:\n",
      "Somebody told me a few days ago his wife came to this hotel and created a scene at a tryst he had with his girlfriend here.\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742,  4750,\n",
      "           592, 26212,  2823, 59569, 59601,   144, 14984,  3040,  2622,   795,\n",
      "           562,  1573,  2043,  2781,   893,  5018,  2514,   592,   719,  6622,\n",
      "           597,  3527,   562,  5753,   702,   562,  1628,   608,   660,   986,\n",
      "           651,   893, 13139,  1364,    98,     7, 59568,   144,     6, 14135,\n",
      "           144,  5189, 47884, 59646, 62037, 59739, 59742,  3092, 10622, 60599,\n",
      "         60565, 59690,  9499,   101, 60702,  3092, 27767, 10212, 61759, 60418,\n",
      "         60531,   538,   477,   472, 61082,   102,     7]], device='cuda:0')\n",
      "Base model:\n",
      "有人告訴我幾天前他的妻子來這家酒店，與他的女朋友在此幽會時鬧場。\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926, 73580,  1994,   742,\n",
      "          4750,   592, 26212,  2823, 59569, 59601,   144, 14984,  3040,  2622,\n",
      "           795,   562,  1573,  2043,  2781,   893,  5018,  2514,   592,   719,\n",
      "          6622,   597,  3527,   562,  5753,   702,   562,  1628,   608,   660,\n",
      "           986,   651,   893, 13139,  1364,    98,     7, 59568,   144,     6,\n",
      "         14135,   144,  5189, 61845,   533, 75701, 59720,  4000,   533, 75701,\n",
      "         16387, 74167, 70523, 60329, 61085,  9499,   101, 59727, 74167, 59568,\n",
      "         74167, 59568, 61148, 68426,   102,   141,     7]], device='cuda:0')\n",
      "Fine-tuned model:\n",
      "有人話�日之前�老婆呢間酒店，同  搞。  \n",
      "\n",
      "{'en': \"You've been so stingy for your whole life.\", 'yue': '你成世都咁孤寒。'}\n",
      "Prompt:\n",
      "Translate the following words from Cantonese to English:\n",
      "你成世都咁孤寒。\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompare_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_samples\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m, in \u001b[0;36mcompare_outputs\u001b[0;34m(samples)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mprompt[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m----> 8\u001b[0m \u001b[43mmodel_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBase model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m model_output(model, tokenizer, [prompt], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFine-tuned model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m, in \u001b[0;36mmodel_output\u001b[0;34m(model, tokenizer, messages, name)\u001b[0m\n\u001b[1;32m      3\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(conversation\u001b[38;5;241m=\u001b[39mprompt, tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m----> 5\u001b[0m         output_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m response \u001b[38;5;241m=\u001b[39m base_tokenizer\u001b[38;5;241m.\u001b[39mdecode(output_ids[\u001b[38;5;241m0\u001b[39m][input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# response = tokenizer.decode(output_ids[0], skip_special_tokens=False, max_length=100)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/generation/utils.py:1592\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1584\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1585\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1586\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1587\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1588\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1589\u001b[0m     )\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1608\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1609\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1610\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1611\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1616\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1617\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/generation/utils.py:2696\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2693\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2695\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2696\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2697\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2699\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2700\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2701\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2704\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1176\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1173\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1176\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1189\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1019\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1009\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1010\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1016\u001b[0m         cache_position,\n\u001b[1;32m   1017\u001b[0m     )\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1019\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:740\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    737\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    739\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    750\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    752\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:641\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    639\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states)\n\u001b[1;32m    640\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\n\u001b[0;32m--> 641\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m query_states \u001b[38;5;241m=\u001b[39m query_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    644\u001b[0m key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:687\u001b[0m, in \u001b[0;36mLinear8bitLt.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m x\u001b[38;5;241m.\u001b[39mdtype:\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 687\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mhas_fp16_weights:\n\u001b[1;32m    690\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mCB \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mCxB \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m         \u001b[38;5;66;03m# we converted 8-bit row major to turing/ampere format in the first inference pass\u001b[39;00m\n\u001b[1;32m    692\u001b[0m         \u001b[38;5;66;03m# we no longer need the row-major weight\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:562\u001b[0m, in \u001b[0;36mmatmul\u001b[0;34m(A, B, out, state, threshold, bias)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m threshold \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    561\u001b[0m     state\u001b[38;5;241m.\u001b[39mthreshold \u001b[38;5;241m=\u001b[39m threshold\n\u001b[0;32m--> 562\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul8bitLt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/torch/autograd/function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:327\u001b[0m, in \u001b[0;36mMatMul8bitLt.forward\u001b[0;34m(ctx, A, B, out, bias, state)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(A\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    326\u001b[0m     A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 327\u001b[0m CA, CAt, SCA, SCAt, coo_tensorA \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble_quant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state\u001b[38;5;241m.\u001b[39mthreshold \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m coo_tensorA \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m state\u001b[38;5;241m.\u001b[39mhas_fp16_weights:\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/bitsandbytes/functional.py:2249\u001b[0m, in \u001b[0;36mdouble_quant\u001b[0;34m(A, col_stats, row_stats, out_col, out_row, threshold)\u001b[0m\n\u001b[1;32m   2234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2235\u001b[0m     lib\u001b[38;5;241m.\u001b[39mcdouble_rowcol_quant(\n\u001b[1;32m   2236\u001b[0m         ptrA,\n\u001b[1;32m   2237\u001b[0m         ptrRowStats,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2247\u001b[0m         ct\u001b[38;5;241m.\u001b[39mc_int32(cols),\n\u001b[1;32m   2248\u001b[0m     )\n\u001b[0;32m-> 2249\u001b[0m \u001b[43mpost_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_row, out_col, row_stats, col_stats, coo_tensor\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/bitsandbytes/functional.py:441\u001b[0m, in \u001b[0;36mpost_call\u001b[0;34m(prev_device)\u001b[0m\n\u001b[1;32m    437\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mset_device(device)\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prev_device\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost_call\u001b[39m(prev_device):\n\u001b[1;32m    442\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mset_device(prev_device)\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_transform_func\u001b[39m(dtype, orderA, orderOut, transpose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "compare_outputs(train_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': 'Miss Linda.', 'yue': 'Linda姐。'}\n",
      "Prompt:\n",
      "Translate the following words from Cantonese to English:\n",
      "Linda姐。\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model:\n",
      "Linda姐.\n",
      "\n",
      "SFT model:\n",
      "Linda, or Miss Linda. \n",
      "\n",
      "Prompt:\n",
      "Translate the following words from English to Cantonese:\n",
      "Miss Linda.\n",
      "\n",
      "Base model:\n",
      "Miss Linda.\n",
      "\n",
      "SFT model:\n",
      "李生。 \n",
      "\n",
      "{'en': 'What is he doing in the kitchen to make such banging and clanging noise?', 'yue': '佢喺廚房度打得咁𠽤叻𡃈嘞做乜嘢啊？'}\n",
      "Prompt:\n",
      "Translate the following words from Cantonese to English:\n",
      "佢喺廚房度打得咁𠽤叻𡃈嘞做乜嘢啊？\n",
      "\n",
      "Base model:\n",
      "\"佢喺廚房度打得咁𠽤叻𡃈嘞做乜嘢啊？\" 可以翻译为 \"他在厨房里打得这么好啊？\" 或者 \"他在厨房里打得这么好吗？\" 其中，\"打得咁𠽤叻𡃈嘞\" 表示打得很厉害，\"做乜嘢啊？\" 表示在做些什么。\n",
      "\n",
      "SFT model:\n",
      "What is he doing in the kitchen looking so engrossed in his work? \n",
      "\n",
      "Prompt:\n",
      "Translate the following words from English to Cantonese:\n",
      "What is he doing in the kitchen to make such banging and clanging noise?\n",
      "\n",
      "Base model:\n",
      "他在廚房裡做什麼會發出這麼大的敲擊和碰撞噪音？\n",
      "\n",
      "SFT model:\n",
      "佢喺廚房咁多嘈? \n",
      "\n",
      "{'en': 'How high do you think the possibility of success for this matter will be?', 'yue': '計你話呢件事嘅成數會有幾高呢？'}\n",
      "Prompt:\n",
      "Translate the following words from Cantonese to English:\n",
      "計你話呢件事嘅成數會有幾高呢？\n",
      "\n",
      "Base model:\n",
      "Translation from Cantonese to English:\n",
      "\"How high do you think the number for this matter will be?\"\n",
      "\n",
      "SFT model:\n",
      "How high do you think the chances of this matter turning out well are? \n",
      "\n",
      "Prompt:\n",
      "Translate the following words from English to Cantonese:\n",
      "How high do you think the possibility of success for this matter will be?\n",
      "\n",
      "Base model:\n",
      "在這件事上你認為成功的可能性有多大？\n",
      "\n",
      "SFT model:\n",
      "你估呢單嘢成功機會有幾高啊？ \n",
      "\n",
      "{'en': 'Nowadays very few people wear Chinese-style jackets and trousers.', 'yue': '今時今日好少人着唐裝衫嘞。'}\n",
      "Prompt:\n",
      "Translate the following words from Cantonese to English:\n",
      "今時今日好少人着唐裝衫嘞。\n",
      "\n",
      "Base model:\n",
      "\"今時今日好少人着唐裝衫嘞。\" \n",
      "\n",
      "Translation: \"Nowadays, it is rare to see people wearing traditional Chinese clothing (唐裝).\"\n",
      "\n",
      "SFT model:\n",
      "Nowadays very few people wear traditional Chinese dress. \n",
      "\n",
      "Prompt:\n",
      "Translate the following words from English to Cantonese:\n",
      "Nowadays very few people wear Chinese-style jackets and trousers.\n",
      "\n",
      "Base model:\n",
      "現今很少人穿中式外套和褲子。\n",
      "\n",
      "SFT model:\n",
      "而家好少人着唐裝西裝。 \n",
      "\n",
      "{'en': 'The kid is so naughty he deserves a spanking!', 'yue': '個細蚊仔咁曳，真抵打！'}\n",
      "Prompt:\n",
      "Translate the following words from Cantonese to English:\n",
      "個細蚊仔咁曳，真抵打！\n",
      "\n",
      "Base model:\n",
      "\"個細蚊仔咁曳，真抵打！\" 可以翻译为 \"That little mosquito is so annoying, it really deserves a beating!\" 其中，\"個細蚊仔\" 指的是 \"那個小蚊子\"，\"咁曳\" 指的是 \"so annoying\"，\"真抵打\" 指的是 \"it really deserves a beating\"。\n",
      "\n",
      "SFT model:\n",
      "The young boy is so naughty, he really deserves to be beaten! \n",
      "\n",
      "Prompt:\n",
      "Translate the following words from English to Cantonese:\n",
      "The kid is so naughty he deserves a spanking!\n",
      "\n",
      "Base model:\n",
      "The kid is so naughty he deserves a spanking!\n",
      "\n",
      "SFT model:\n",
      "個細蚊仔咁叻，！ \n",
      "\n",
      "{'en': 'After he retired he then lived a hard life supported by his meager savings.', 'yue': '佢退休之後就要捱穀種。'}\n",
      "Prompt:\n",
      "Translate the following words from Cantonese to English:\n",
      "佢退休之後就要捱穀種。\n",
      "\n",
      "Base model:\n",
      "After he retires, he has to endure the hardships of farming.\n",
      "\n",
      "SFT model:\n",
      "After he retires he will have to live on his savings. \n",
      "\n",
      "Prompt:\n",
      "Translate the following words from English to Cantonese:\n",
      "After he retired he then lived a hard life supported by his meager savings.\n",
      "\n",
      "Base model:\n",
      "在他退休後，他過上了拮据的生活，依靠微薄的儲蓄來維持。\n",
      "\n",
      "SFT model:\n",
      "佢咗之後就捱。 \n",
      "\n",
      "{'en': 'I have only one mouth, so how can I eat so much?', 'yue': '我一個人得一把口，唔食得咁多？'}\n",
      "Prompt:\n",
      "Translate the following words from Cantonese to English:\n",
      "我一個人得一把口，唔食得咁多？\n",
      "\n",
      "Base model:\n",
      "\"我一個人得一把口，唔食得咁多？\" 可以翻译为 \"我一个人吃得不多，是不是？\" 或者 \"我一个人吃得不多，对吗？\" 这里的 \"一把口\" 是指 \"一个人\"，\"唔食得咁多\" 是指 \"吃不太多\"。\n",
      "\n",
      "SFT model:\n",
      "I'm alone with only one mouth, so how can I eat so much? \n",
      "\n",
      "Prompt:\n",
      "Translate the following words from English to Cantonese:\n",
      "I have only one mouth, so how can I eat so much?\n",
      "\n",
      "Base model:\n",
      "我只有一個嘴，所以怎麼可能吃得那麼多？\n",
      "\n",
      "SFT model:\n",
      "我一個嘴，點食得咁多嘢啊？ \n",
      "\n",
      "{'en': \"Lots of times when taking Light Rail I see people get off the train and not swipe their Octopus cards on the machine, so it's quite obvious that before they had boarded the train they hadn't used their Octopus cards.\", 'yue': '我搭輕鐵好多時見到有人落車唔嘟機，好明顯佢哋上車前都冇嘟機。'}\n",
      "Prompt:\n",
      "Translate the following words from Cantonese to English:\n",
      "我搭輕鐵好多時見到有人落車唔嘟機，好明顯佢哋上車前都冇嘟機。\n",
      "\n",
      "Base model:\n",
      "我搭輕鐵好多時見到有人落車唔嘟機，好明顯佢哋上車前都冇嘟機。\n",
      "\n",
      "SFT model:\n",
      "When I take the light rail quite often I see people get on the train without tapping their Octopus cards. It's obvious they didn't tap their cards when they boarded the train. \n",
      "\n",
      "Prompt:\n",
      "Translate the following words from English to Cantonese:\n",
      "Lots of times when taking Light Rail I see people get off the train and not swipe their Octopus cards on the machine, so it's quite obvious that before they had boarded the train they hadn't used their Octopus cards.\n",
      "\n",
      "Base model:\n",
      "在乘搭輕鐵的很多次經驗中，我見到有人下車時沒有在機器上刷八達通卡，所以很明顯地，在他們上車前並沒有使用八達通卡。\n",
      "\n",
      "SFT model:\n",
      "我搭輕鐵時好多次見到有人落車冇咭機，好明顯佢哋車前唔用咭。 \n",
      "\n",
      "{'en': 'What I have sold to you are all genuine items.', 'yue': '我賣畀你嘅全部係堅嘢。'}\n",
      "Prompt:\n",
      "Translate the following words from Cantonese to English:\n",
      "我賣畀你嘅全部係堅嘢。\n",
      "\n",
      "Base model:\n",
      "\"我賣畀你嘅全部係堅嘢。\" \n",
      "\n",
      "Translation from Cantonese to English:\n",
      "\"What I sell to you is all solid.\"\n",
      "\n",
      "SFT model:\n",
      "All the stuff I've sold you are all genuine goods. \n",
      "\n",
      "Prompt:\n",
      "Translate the following words from English to Cantonese:\n",
      "What I have sold to you are all genuine items.\n",
      "\n",
      "Base model:\n",
      "在賣給你的所有物品都是真品。\n",
      "\n",
      "SFT model:\n",
      "我賣畀你嘅都係正貨。 \n",
      "\n",
      "{'en': 'Where shall we go today to hang out?', 'yue': '我哋今日去邊度蒲啊？'}\n",
      "Prompt:\n",
      "Translate the following words from Cantonese to English:\n",
      "我哋今日去邊度蒲啊？\n",
      "\n",
      "Base model:\n",
      "\"我哋今日去邊度蒲啊？\" 可以翻译为 \"我们今天要去哪里玩呢？\" 或者 \"我们今天要去哪里聚会呢？\" 这里的 \"蒲\" 是一个粤语词汇，意思是 \"聚会、玩乐\"。\n",
      "\n",
      "SFT model:\n",
      "Where are we going to hang out today? \n",
      "\n",
      "Prompt:\n",
      "Translate the following words from English to Cantonese:\n",
      "Where shall we go today to hang out?\n",
      "\n",
      "Base model:\n",
      "今天我们去哪里闲逛？\n",
      "\n",
      "SFT model:\n",
      "今日去邊度蒲？ \n",
      "\n",
      "{'en': 'Many teenaged girls try to act like adults and start smoking when they are young.', 'yue': '好多𡃁妹仔細細個就學人食煙。'}\n",
      "Prompt:\n",
      "Translate the following words from Cantonese to English:\n",
      "好多𡃁妹仔細細個就學人食煙。\n",
      "\n",
      "Base model:\n",
      "Many young girls started smoking very young.\n",
      "\n",
      "SFT model:\n",
      "Many young girls very young have learned from others to smoke cigarettes. \n",
      "\n",
      "Prompt:\n",
      "Translate the following words from English to Cantonese:\n",
      "Many teenaged girls try to act like adults and start smoking when they are young.\n",
      "\n",
      "Base model:\n",
      "在年輕時，許多青少年女孩試圖表現得像成年人，並且開始吸煙。\n",
      "\n",
      "SFT model:\n",
      "好多女仔都好想扮大，細個嗰陣時就開始食煙。 \n",
      "\n",
      "{'en': 'After getting off work and returning home, I then change into casual clothes and go buy food to prepare.', 'yue': '落咗班返屋企就着住街坊裝去買餸。'}\n",
      "Prompt:\n",
      "Translate the following words from Cantonese to English:\n",
      "落咗班返屋企就着住街坊裝去買餸。\n",
      "\n",
      "Base model:\n",
      "\"落咗班\" means \"finished work\" or \"left work.\"\n",
      "\n",
      "\"返屋企\" means \"go back home.\"\n",
      "\n",
      "\"就着住\" means \"stay by.\"\n",
      "\n",
      "\"街坊\" means \"neighbors.\"\n",
      "\n",
      "\"裝\" is a verb in Cantonese that means \"to go.\"\n",
      "\n",
      "\"去買餸\" means \"go to buy groceries.\"\n",
      "\n",
      "SFT model:\n",
      "After I got off work I went home and changed into my street clothes before going out to buy food. \n",
      "\n",
      "Prompt:\n",
      "Translate the following words from English to Cantonese:\n",
      "After getting off work and returning home, I then change into casual clothes and go buy food to prepare.\n",
      "\n",
      "Base model:\n",
      "在下班後回到家，我會換上便服，然後去買食物準備。\n",
      "\n",
      "SFT model:\n",
      "放工返屋企，我就換咗件嘅衫出街，買咗啲嘅嘢做咗嘢先至返屋企。 \n",
      "\n",
      "{'en': \"The hill has no trees or grass, so it's devoid of any covering.\", 'yue': '個山冇樹又冇草，光脫脫噉。'}\n",
      "Prompt:\n",
      "Translate the following words from Cantonese to English:\n",
      "個山冇樹又冇草，光脫脫噉。\n",
      "\n",
      "Base model:\n",
      "The following words from Cantonese are being translated to English:\n",
      "\n",
      "個 - 一个\n",
      "山 - 山\n",
      "冇 - 没有\n",
      "樹 - 树\n",
      "又 - 又\n",
      "冇 - 没有\n",
      "草 - 草\n",
      "光 - 光\n",
      "脫 - 脱\n",
      "脫脫 - 光光\n",
      "噉 - 这样\n",
      "\n",
      "Translation: There is no mountain, no trees, and no grass. It is just a plain.\n",
      "\n",
      "SFT model:\n",
      "There's no tree or grass on the hill, so it's bare and dry. \n",
      "\n",
      "Prompt:\n",
      "Translate the following words from English to Cantonese:\n",
      "The hill has no trees or grass, so it's devoid of any covering.\n",
      "\n",
      "Base model:\n",
      "The hill is 無樹木亦無草，所以它是 無遮蓋的。\n",
      "\n",
      "SFT model:\n",
      "山樹樹，冇乜遮蔽。 \n",
      "\n",
      "{'en': 'The young boy sits on the tree stump.', 'yue': '個細路仔坐响樹頭上便。'}\n",
      "Prompt:\n",
      "Translate the following words from Cantonese to English:\n",
      "個細路仔坐响樹頭上便。\n",
      "\n",
      "Base model:\n",
      "The following words from Cantonese are being translated to English:\n",
      "\n",
      "個 - \"ko\" (literally means \"one\" or \"a\")\n",
      "細路仔 - \"siu2 lam4 zi2\" (literally means \"small road child\", but in this context, it refers to a young child)\n",
      "坐 - \"zo2\" (means to sit)\n",
      "響 - \"heon2\" (means to sound or to be heard)\n",
      "樹頭上 - \"sui4 tou2 saang1\" (literally means \"tree top\", but in this context, it refers to the branch or the top of the tree)\n",
      "便 - \"bin2\" (means to go or to do something immediately after)\n",
      "\n",
      "SFT model:\n",
      "The young boy is sitting on the tree. \n",
      "\n",
      "Prompt:\n",
      "Translate the following words from English to Cantonese:\n",
      "The young boy sits on the tree stump.\n",
      "\n",
      "Base model:\n",
      "在樹墩上，年輕男孩坐著。\n",
      "\n",
      "SFT model:\n",
      "個細路仔坐喺個樹樘上。 \n",
      "\n",
      "{'en': \"If some English words seep into your Cantonese, it's quite normal that you then pronounce them incorrectly. It's because you're not really speaking English, you've only mixed some English words into your Cantonese\", 'yue': '如果你嘅粵語入邊滲咗啲英文詞彙，就讀歪音係好正常嘅，因為你唔係真係講緊英文，靜係將啲英文字融入粵語喎！'}\n",
      "Prompt:\n",
      "Translate the following words from Cantonese to English:\n",
      "如果你嘅粵語入邊滲咗啲英文詞彙，就讀歪音係好正常嘅，因為你唔係真係講緊英文，靜係將啲英文字融入粵語喎！\n",
      "\n",
      "Base model:\n",
      "If you've incorporated some English words into your Cantonese, it's perfectly normal, as you're not really speaking English; you're just infusing English words into Cantonese!\n",
      "\n",
      "SFT model:\n",
      "If you have some English words in your Cantonese speech, it's quite normal, as you're not really speaking English. You're just mixing English words into Cantonese. \n",
      "\n",
      "Prompt:\n",
      "Translate the following words from English to Cantonese:\n",
      "If some English words seep into your Cantonese, it's quite normal that you then pronounce them incorrectly. It's because you're not really speaking English, you've only mixed some English words into your Cantonese\n",
      "\n",
      "Base model:\n",
      "如果一些英语词汇渗入你的广东话，那么你偶尔发音错误是很正常的，因为你并不是在说英语，你只是在你的广东话中混合了一些英语词汇。\n",
      "\n",
      "SFT model:\n",
      "如果你啲英文入嚟，好正常，因為你唔係講，係夾嚟講。 \n",
      "\n",
      "{'en': 'We now want to take our sons and daughters out to eat half-priced buffet.', 'yue': '依家我哋要帶啲囝囝囡囡出街食半價布菲。'}\n",
      "Prompt:\n",
      "Translate the following words from Cantonese to English:\n",
      "依家我哋要帶啲囝囝囡囡出街食半價布菲。\n",
      "\n",
      "Base model:\n",
      "Currently, we need to take our children out to enjoy half-priced brunch.\n",
      "\n",
      "SFT model:\n",
      "Now we need to take our children out to eat a half-price buffet. \n",
      "\n",
      "Prompt:\n",
      "Translate the following words from English to Cantonese:\n",
      "We now want to take our sons and daughters out to eat half-priced buffet.\n",
      "\n",
      "Base model:\n",
      "在我們現在想帶我們的兒女去吃半價自助餐。\n",
      "\n",
      "SFT model:\n",
      "我哋而家想帶埋細路仔女去食半。 \n",
      "\n",
      "{'en': \"You're making that kind of decision, so people may request that you give an explanation for it.\", 'yue': '你做噉樣嘅決定，人哋會要求你解畫嘅噃。'}\n",
      "Prompt:\n",
      "Translate the following words from Cantonese to English:\n",
      "你做噉樣嘅決定，人哋會要求你解畫嘅噃。\n",
      "\n",
      "Base model:\n",
      "You make such a decision, people will ask you to explain it, right?\n",
      "\n",
      "SFT model:\n",
      "If you make such a decision, people will demand that you explain yourself. \n",
      "\n",
      "Prompt:\n",
      "Translate the following words from English to Cantonese:\n",
      "You're making that kind of decision, so people may request that you give an explanation for it.\n",
      "\n",
      "Base model:\n",
      "在做出那種決定時，別人可能會要求你為此提供解釋。\n",
      "\n",
      "SFT model:\n",
      "你做嗰啲嘢，好可能會有人要求你解釋嘅。 \n",
      "\n",
      "{'en': \"The size of a blue whale's heart is almost as big as a car.\", 'yue': '藍鯨嘅心臟大細同車差唔多咁大。'}\n",
      "Prompt:\n",
      "Translate the following words from Cantonese to English:\n",
      "藍鯨嘅心臟大細同車差唔多咁大。\n",
      "\n",
      "Base model:\n",
      "The heart of the blue whale is as large as a car.\n",
      "\n",
      "SFT model:\n",
      "The blue whale's heart is about the same size as the car. \n",
      "\n",
      "Prompt:\n",
      "Translate the following words from English to Cantonese:\n",
      "The size of a blue whale's heart is almost as big as a car.\n",
      "\n",
      "Base model:\n",
      "在藍鯨的心臟大得幾乎就像一輛車一樣。\n",
      "\n",
      "SFT model:\n",
      "藍嘅心臟嘅大小近似車咁大。 \n",
      "\n",
      "{'en': 'In former times some people would become very angry as soon as they heard someone say he was selling insurance, as they felt like they were being cursed to die.', 'yue': '以前啲人一聽話賣燕梳就會好嬲，覺得好似咒緊佢哋去死噉。'}\n",
      "Prompt:\n",
      "Translate the following words from Cantonese to English:\n",
      "以前啲人一聽話賣燕梳就會好嬲，覺得好似咒緊佢哋去死噉。\n",
      "\n",
      "Base model:\n",
      "以前有些人一聽說賣燕窩就會很生氣，覺得好像在詛咒他們去死一樣。\n",
      "\n",
      "SFT model:\n",
      "In the past when people heard someone say sell the they would be quite angry. They seemed to be cursing them to die. \n",
      "\n",
      "Prompt:\n",
      "Translate the following words from English to Cantonese:\n",
      "In former times some people would become very angry as soon as they heard someone say he was selling insurance, as they felt like they were being cursed to die.\n",
      "\n",
      "Base model:\n",
      "在昔日的時候，有些人會對聽到有人說他在賣保險就變得很生氣，因為他們覺得自己好像被詛咒要死了一樣。\n",
      "\n",
      "SFT model:\n",
      "以前有啲人一聽到有人賣保險就發爛渣，好似俾人詛咒死噉。 \n",
      "\n",
      "{'en': 'Last night thirty people came to attend my class.', 'yue': '琴晚有卅個人嚟上我嘅堂。'}\n",
      "Prompt:\n",
      "Translate the following words from Cantonese to English:\n",
      "琴晚有卅個人嚟上我嘅堂。\n",
      "\n",
      "Base model:\n",
      "Translation: There were thirty people came to my class in the evening.\n",
      "\n",
      "SFT model:\n",
      "Last night thirty people came to take my class. \n",
      "\n",
      "Prompt:\n",
      "Translate the following words from English to Cantonese:\n",
      "Last night thirty people came to attend my class.\n",
      "\n",
      "Base model:\n",
      "昨晚三十個人來參加我的班級。\n",
      "\n",
      "SFT model:\n",
      "琴晚有三十三個人嚟上我堂。 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_outputs(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_translate_prompt() missing 1 required positional argument: 'source_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m train_sample \u001b[38;5;241m=\u001b[39m abc_train_set\u001b[38;5;241m.\u001b[39mshuffle(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m      7\u001b[0m test_sample \u001b[38;5;241m=\u001b[39m abc_test_set\u001b[38;5;241m.\u001b[39mshuffle(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m----> 9\u001b[0m en_train_messages \u001b[38;5;241m=\u001b[39m {get_translate_prompt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCantonese\u001b[39m\u001b[38;5;124m'\u001b[39m, sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m train_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m     10\u001b[0m en_test_messages \u001b[38;5;241m=\u001b[39m {get_translate_prompt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCantonese\u001b[39m\u001b[38;5;124m'\u001b[39m, sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m test_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m     11\u001b[0m yue_train_messages \u001b[38;5;241m=\u001b[39m {get_translate_prompt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnglish\u001b[39m\u001b[38;5;124m'\u001b[39m, sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m train_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myue\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n",
      "Cell \u001b[0;32mIn[19], line 9\u001b[0m, in \u001b[0;36m<setcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m train_sample \u001b[38;5;241m=\u001b[39m abc_train_set\u001b[38;5;241m.\u001b[39mshuffle(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m      7\u001b[0m test_sample \u001b[38;5;241m=\u001b[39m abc_test_set\u001b[38;5;241m.\u001b[39mshuffle(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m----> 9\u001b[0m en_train_messages \u001b[38;5;241m=\u001b[39m {\u001b[43mget_translate_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCantonese\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m train_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m     10\u001b[0m en_test_messages \u001b[38;5;241m=\u001b[39m {get_translate_prompt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCantonese\u001b[39m\u001b[38;5;124m'\u001b[39m, sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m test_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m     11\u001b[0m yue_train_messages \u001b[38;5;241m=\u001b[39m {get_translate_prompt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnglish\u001b[39m\u001b[38;5;124m'\u001b[39m, sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m train_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myue\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n",
      "\u001b[0;31mTypeError\u001b[0m: get_translate_prompt() missing 1 required positional argument: 'source_text'"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Translate the following words into English:\\n乜嘢都係波士決定嘅，打工仔啲人淨係得個知字。\\n\"},\n",
    "]\n",
    "\n",
    "# get 5 random samples from train and test dataset\n",
    "train_sample = abc_train_set.shuffle(seed=42).select(range(5))\n",
    "test_sample = abc_test_set.shuffle(seed=42).select(range(5))\n",
    "\n",
    "en_train_messages = {get_translate_prompt('Cantonese', sentence) for sentence in train_sample['en']}\n",
    "en_test_messages = {get_translate_prompt('Cantonese', sentence) for sentence in test_sample['en']}\n",
    "yue_train_messages = {get_translate_prompt('English', sentence) for sentence in train_sample['yue']}\n",
    "yue_test_messages = {get_translate_prompt('English', sentence) for sentence in test_sample['yue']}\n",
    "\n",
    "for messages in [en_train_messages, en_test_messages, yue_train_messages, yue_test_messages]:\n",
    "    for message in messages:\n",
    "        print(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefinedError",
     "evalue": "'str object' has no attribute 'role'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUndefinedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(input_ids\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# response = base_tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True, max_length=100)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1745\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[0;34m(self, conversation, chat_template, add_generation_prompt, tokenize, padding, truncation, max_length, return_tensors, return_dict, **tokenizer_kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# Compilation function uses a cache to avoid recompiling the same template\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m compiled_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compile_jinja_template(chat_template)\n\u001b[0;32m-> 1745\u001b[0m rendered \u001b[38;5;241m=\u001b[39m \u001b[43mcompiled_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconversation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspecial_tokens_map\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m padding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1750\u001b[0m     padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# There's only one sequence here, so \"longest\" makes no sense\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/jinja2/environment.py:1301\u001b[0m, in \u001b[0;36mTemplate.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment\u001b[38;5;241m.\u001b[39mconcat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_render_func(ctx))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m-> 1301\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/jinja2/environment.py:936\u001b[0m, in \u001b[0;36mEnvironment.handle_exception\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Exception handling helper.  This is used internally to either raise\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;124;03mrewritten exceptions or return a rendered traceback for the template.\u001b[39;00m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rewrite_traceback_stack\n\u001b[0;32m--> 936\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m rewrite_traceback_stack(source\u001b[38;5;241m=\u001b[39msource)\n",
      "File \u001b[0;32m<template>:2\u001b[0m, in \u001b[0;36mtop-level template code\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUndefinedError\u001b[0m: 'str object' has no attribute 'role'"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\n",
    "output_ids = model.generate(input_ids.to('cuda'))\n",
    "# response = base_tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True, max_length=100)\n",
    "response = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True, max_length=100)\n",
    "\n",
    "# Model response: \"Hello! How can I assist you today?\"\n",
    "print(\"Tuned model:\", response)\n",
    "\n",
    "input_ids = base_tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\n",
    "output_ids = base_model.generate(input_ids.to('cuda'))\n",
    "# response = base_tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True, max_length=100)\n",
    "response = base_tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True, max_length=100)\n",
    "\n",
    "print(\"Base model:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
