{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import bitsandbytes\n",
    "import accelerate\n",
    "from datasets import Dataset\n",
    "\n",
    "from modelscope import snapshot_download\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from peft import PeftModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=r'/root/autodl-tmp/01ai/Yi-6B-Chat'\n",
    "model_dir = snapshot_download('01ai/Yi-6B-Chat', cache_dir='/root/autodl-tmp', revision='master')\n",
    "\n",
    "# base_tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True, padding_side='left', max_length=512, return_tensors='pt')\n",
    "\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(\n",
    "# \t model_path,\n",
    "# \t device_map='auto',\n",
    "# \t torch_dtype=torch.bfloat16,\n",
    "# \t trust_remote_code=True \n",
    "# ).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Translate the following words into English:\\n你係邊個？\"},\n",
    "]\n",
    "\n",
    "messages_plain = [\n",
    "    \"\"\"\n",
    "    <|im_start|> user\n",
    "    hi<|im_end|> \n",
    "    <|im_start|> assistant\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# input_ids = base_tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\n",
    "# output_ids = base_model.generate(input_ids.to('cuda'))\n",
    "# # response = base_tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True, max_length=100)\n",
    "# response = base_tokenizer.decode(output_ids[0], skip_special_tokens=False, max_length=100)\n",
    "\n",
    "# # Model response: \"Hello! How can I assist you today?\"\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65167\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('/root/tokenizer', use_fast=True, padding_side='left', max_length=512, return_tensors='pt')\n",
    "base_tokenizer = AutoTokenizer.from_pretrained('/root/autodl-tmp/01ai/Yi-6B-Chat', use_fast=True, padding_side='left', max_length=512, return_tensors='pt')\n",
    "print(len(tokenizer.vocab))\n",
    "# tokenizer = AutoTokenizer.from_pretrained('/root/AIST4010-Cantonese-Translator/tokenizer', use_fast=True, padding_side='left', max_length=512, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PeftModel.from_pretrained(\n",
    "#     model,\n",
    "#    '/root/peft_model',\n",
    "#     is_trainable=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.68s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.43s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.48s/it]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "\t '/root/autodl-tmp/01ai/Yi-6B-Chat',\n",
    "\t device_map=device,\n",
    "\t torch_dtype=torch.bfloat16,\n",
    "     quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n",
    "\t trust_remote_code=True \n",
    ").eval()\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "\t model_path,\n",
    "\t device_map=device,\n",
    "\t torch_dtype=torch.bfloat16,\n",
    "\t quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n",
    "\t trust_remote_code=True \n",
    ").eval()\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.load_adapter('/root/peft_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_DIRECTORY = r'/root/'\n",
    "ABC_DICT_PATH = r'autodl-tmp/AIST4010-Cantonese-Translator-Data/ABC-Dict/abc_dict.csv'\n",
    "\n",
    "def load_abc_dataset():\n",
    "    abc_dict = pd.read_csv(REPO_DIRECTORY + ABC_DICT_PATH)\n",
    "    abc_dataset = Dataset.from_pandas(abc_dict)\n",
    "    return abc_dataset\n",
    "\n",
    "abc_set = load_abc_dataset()\n",
    "abc_shuffled_set = abc_set.shuffle(seed=42).train_test_split(test_size=0.1)\n",
    "abc_train_set = abc_shuffled_set['train']\n",
    "abc_test_set = abc_shuffled_set['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'role': 'user',\n",
       "   'content': 'Translate the following words into English:\\n佢雖然係經理，但個個都當佢冇到。'}],\n",
       " [{'role': 'user',\n",
       "   'content': 'Translate the following words into Cantonese:\\nEven though he is the manager, he is looked down upon and ignored by everyone.'}]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_translate_prompt(target_language, source_text):\n",
    "    return f\"Translate the following words into {target_language}:\\n{source_text}\"\n",
    "\n",
    "def get_messages(sample):\n",
    "    def get_translate_prompt(target_language, source_text):\n",
    "        return f\"Translate the following words into {target_language}:\\n{source_text}\"\n",
    "    return [\n",
    "        [{\"role\": \"user\", \"content\": get_translate_prompt('English', sample['yue'])}],\n",
    "        [{\"role\": \"user\", \"content\": get_translate_prompt('Cantonese', sample['en'])}]\n",
    "    ]\n",
    "\n",
    "train_samples = abc_train_set.shuffle(seed=10).select(range(5))\n",
    "test_samples = abc_test_set.shuffle(seed=10).select(range(5))\n",
    "\n",
    "get_messages(train_samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/trans/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model: Although he is the manager, everyone treats him as if he were not there.\n",
      "Base model: Even though he is the manager, he is looked down upon and ignored by everyone.\n",
      "[None]\n"
     ]
    }
   ],
   "source": [
    "def model_output(model, tokenizer, messages, name=None):\n",
    "        for prompt in messages:\n",
    "                input_ids = tokenizer.apply_chat_template(conversation=prompt, tokenize=True, add_generation_prompt=True, return_tensors='pt')\n",
    "                output_ids = model.generate(input_ids.to('cuda'))\n",
    "                response = base_tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True, max_length=100)\n",
    "                # response = tokenizer.decode(output_ids[0], skip_special_tokens=False, max_length=100)\n",
    "                print(f\"{name}: {response}\")\n",
    "\n",
    "\n",
    "print([model_output(base_model, base_tokenizer, get_messages(train_samples[0]), 'Base model'),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_outputs(samples):\n",
    "    for sample in samples:\n",
    "        print(sample)\n",
    "        prompts = get_messages(sample)\n",
    "        for prompt in prompts:\n",
    "            print(prompt)\n",
    "            model_output(base_model, base_tokenizer, [prompt], 'Base model')\n",
    "            model_output(model, tokenizer, [prompt], 'SFT model')\n",
    "\n",
    "# compare_outputs(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': 'Even though he is the manager, he is looked down upon and ignored by everyone.', 'yue': '佢雖然係經理，但個個都當佢冇到。'}\n",
      "[{'role': 'user', 'content': 'Translate the following words into English:\\n佢雖然係經理，但個個都當佢冇到。'}]\n",
      "Base model: He is the manager, but everyone treats him as if he isn't there.\n",
      "SFT model: assistant\n",
      "Although he is a manager, no one respects him. \n",
      "[{'role': 'user', 'content': 'Translate the following words into Cantonese:\\nEven though he is the manager, he is looked down upon and ignored by everyone.'}]\n",
      "Base model: 即使他是经理，他还是被所有人看不起并被忽视。\n",
      "SFT model: assistant\n",
      "Even though he is the manager, he is looked down upon and ignored by everyone. \n",
      "{'en': \"I should give you all my money? I'll give you nothing!\", 'yue': '畀晒啲錢你？畀你個頭！'}\n",
      "[{'role': 'user', 'content': 'Translate the following words into English:\\n畀晒啲錢你？畀你個頭！'}]\n",
      "Base model: \"畀晒啲錢你？畀你個頭！\" 可以翻译为 \"Give all the money to you? Give you the head!\" 或者 \"Give all the money to you? Give you the money!\" 这两种翻译都表示\"给你所有的钱\"，但是第一种翻译更直接，而第二种翻译则更礼貌。根据上下文，你需要选择最合适的翻译。\n",
      "SFT model: Translate the following words into English:\n",
      "Give all the money to you? Give it to your head! \n",
      "[{'role': 'user', 'content': \"Translate the following words into Cantonese:\\nI should give you all my money? I'll give you nothing!\"}]\n",
      "Base model: \"我應該將所有錢都給你？我會給你nothing！\"\n",
      "SFT model: assistant\n",
      "我畀你啲錢？唔畀你！\n",
      "{'en': \"I've got a male colleague, he often craftily says things to me that make me feel quite uncomfortable. What can I do about it?\", 'yue': '我有一個男同事，經常有意冇意向我講一啲令我唔好舒服嘅說話，點算啊？'}\n",
      "[{'role': 'user', 'content': 'Translate the following words into English:\\n我有一個男同事，經常有意冇意向我講一啲令我唔好舒服嘅說話，點算啊？'}]\n",
      "Base model: 我有一個男同事，經常有意冇意向我講一啲令我唔好舒服嘅說話，點算啊？\n",
      "\n",
      "Translation: I have a male colleague who often seems to say things to me that make me feel uncomfortable. What should I do?\n",
      "SFT model: assistant\n",
      "My male colleague often says things to me which make me feel uncomfortable on purpose, so what should I do about it? \n",
      "[{'role': 'user', 'content': \"Translate the following words into Cantonese:\\nI've got a male colleague, he often craftily says things to me that make me feel quite uncomfortable. What can I do about it?\"}]\n",
      "Base model: 在職場上，我有一位男同事，他經常巧妙地說一些話給我聽，讓我感到很不舒服。我該怎麼辦？\n",
      "SFT model: assistant\n",
      "assistant\n",
      "In my office I have a male colleague who often says things to me that make me feel quite uncomfortable. What can I do about it? \n",
      "{'en': \"It's freezing, and there's no liquor, so I really can't bear it.\", 'yue': '咁凍，又冇酒飲，真係吊癮。'}\n",
      "[{'role': 'user', 'content': 'Translate the following words into English:\\n咁凍，又冇酒飲，真係吊癮。'}]\n",
      "Base model: That's so cold, and I haven't had any alcohol to drink, it really gets to me.\n",
      "SFT model: assistant\n",
      "assistant\n",
      "That's so cold, and I can't drink any wine, so it's really a waste. \n",
      "[{'role': 'user', 'content': \"Translate the following words into Cantonese:\\nIt's freezing, and there's no liquor, so I really can't bear it.\"}]\n",
      "Base model: 在凍死啦，又冇酒飲，所以真係頂唔順。\n",
      "SFT model: assistant\n",
      "assistant\n",
      "It's freezing, and there's no liquor, so I really can't bear it. \n",
      "{'en': 'You even refuse to lend me twenty dollars for a bus journey, so how stingy you are!', 'yue': '借廿蚊搭車都唔畀，乜你咁孤寒㗎！'}\n",
      "[{'role': 'user', 'content': 'Translate the following words into English:\\n借廿蚊搭車都唔畀，乜你咁孤寒㗎！'}]\n",
      "Base model: Translation of the following Chinese words into English:\n",
      "\n",
      "借廿蚊搭車都唔畀，乜你咁孤寒㗎！\n",
      "\n",
      "English Translation:\n",
      "\"I won't lend you twenty yuan to take a taxi, are you that stingy!\"\n",
      "SFT model: assistant\n",
      "assistant\n",
      "You don't give twenty dollars to borrow to take a taxi, so you're so stingy! \n",
      "[{'role': 'user', 'content': 'Translate the following words into Cantonese:\\nYou even refuse to lend me twenty dollars for a bus journey, so how stingy you are!'}]\n",
      "Base model: 在你不願借我二十美元乘巴士的旅途中，你真是太小氣了！\n",
      "SFT model: assistant\n",
      "你借我二十蚊都唔借，真悭 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_outputs(train_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': 'As soon as he sees a pretty girl, he feels totally infatuated with and attracted to her.', 'yue': '佢一見到靚女就暈晒大浪。'}\n",
      "[{'role': 'user', 'content': 'Translate the following words into English:\\n佢一見到靚女就暈晒大浪。'}]\n",
      "Base model: He faints at the sight of a beautiful woman.\n",
      "SFT model: assistant\n",
      "He getshead 靓女就到。 \n",
      "[{'role': 'user', 'content': 'Translate the following words into Cantonese:\\nAs soon as he sees a pretty girl, he feels totally infatuated with and attracted to her.'}]\n",
      "Base model: 當他一看到一個漂亮的女孩時，他就對她完全著迷，被她吸引。\n",
      "SFT model: assistant\n",
      "assistant\n",
      "As soon as he sees a pretty girl, he's infatuated with and attracted to her. \n",
      "{'en': \"Nowadays parents really spend lots of money for the sake of celebrating their children's birthdays.\", 'yue': '而家父母真係好抌本去為仔女慶祝生日。'}\n",
      "[{'role': 'user', 'content': 'Translate the following words into English:\\n而家父母真係好抌本去為仔女慶祝生日。'}]\n",
      "Base model: Currently, my parents are really happy to go all out to celebrate their children's birthdays.\n",
      "SFT model: assistant\n",
      "assistant\n",
      "Nowadays parents are really going all out to celebrate their children's birthday. \n",
      "[{'role': 'user', 'content': \"Translate the following words into Cantonese:\\nNowadays parents really spend lots of money for the sake of celebrating their children's birthdays.\"}]\n",
      "Base model: 現在，父母真的花了很多錢來慶祝他們孩子的生日。\n",
      "SFT model: assistant\n",
      "assistant\n",
      "日頭咗，家長真係花好多錢。 \n",
      "{'en': \"Your young child must have the vaccination certificate. If he hasn't had his vaccinations, the kindergarten won't accept him!\", 'yue': '你小朋友都要有張針紙，如果佢哋冇打過針，幼稚園都唔收佢哋喎！'}\n",
      "[{'role': 'user', 'content': 'Translate the following words into English:\\n你小朋友都要有張針紙，如果佢哋冇打過針，幼稚園都唔收佢哋喎！'}]\n",
      "Base model: You kids all need a needle, if they haven't been vaccinated, the kindergarten won't accept them, right?\n",
      "SFT model: assistant\n",
      "Translate the following words into English:\n",
      "You kids need針紙，如果他們沒打過針，幼稚園都不會收他們啊！ \n",
      "[{'role': 'user', 'content': \"Translate the following words into Cantonese:\\nYour young child must have the vaccination certificate. If he hasn't had his vaccinations, the kindergarten won't accept him!\"}]\n",
      "Base model: 你的年輕子女必須有疫苗接種證明。如果他還沒有接受疫苗接種，幼稚園不會接受他！\n",
      "SFT model: assistant\n",
      "assistant\n",
      "仔啲唔夠，幼稚園唔掂！ \n",
      "{'en': 'Wallah-wallahs had been a means of water transport in the 1970s.', 'yue': '哇啦哇嚹係七十年代嘅水上交通工具。'}\n",
      "[{'role': 'user', 'content': 'Translate the following words into English:\\n哇啦哇嚹係七十年代嘅水上交通工具。'}]\n",
      "Base model: Wowla wawa is a water transportation tool from the 1970s.\n",
      "SFT model: assistant\n",
      "Translate the following words into English:\n",
      "Water taxis were a means of transportation in the seventies. \n",
      "[{'role': 'user', 'content': 'Translate the following words into Cantonese:\\nWallah-wallahs had been a means of water transport in the 1970s.'}]\n",
      "Base model: 在1970年代，挽拉挽拉（Wallah-wallahs）曾經是水路運輸的方式。\n",
      "SFT model: assistant\n",
      "\n",
      "{'en': 'He is so unhurried that it sometimes gets people really angry.', 'yue': '佢咁滋油淡定，有時真係激死人。'}\n",
      "[{'role': 'user', 'content': 'Translate the following words into English:\\n佢咁滋油淡定，有時真係激死人。'}]\n",
      "Base model: He's so oily and calm, sometimes it really drives me crazy.\n",
      "SFT model: assistant\n",
      "He's so quiet and calm, sometimes it really makes me mad. \n",
      "[{'role': 'user', 'content': 'Translate the following words into Cantonese:\\nHe is so unhurried that it sometimes gets people really angry.'}]\n",
      "Base model: 他如此不急不躁，有时会让人们真的很生气。\n",
      "SFT model: assistant\n",
      "assistant\n",
      "He's so slow that sometimes it really gets people angry. \n"
     ]
    }
   ],
   "source": [
    "compare_outputs(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the following words into Cantonese:\n",
      "The hooker has just finished, and her client has gone to pay the money.\n",
      "Translate the following words into Cantonese:\n",
      "He's become so fat that he's split the seams of his shirt.\n",
      "Translate the following words into Cantonese:\n",
      "Some Hongkongers have gone abroad and experienced living there.\n",
      "Translate the following words into Cantonese:\n",
      "This guy keeps wandering around at the entrance of the bank and it doesn't seem quite right.\n",
      "Translate the following words into Cantonese:\n",
      "Truly I'm not cheating you.\n",
      "Translate the following words into Cantonese:\n",
      "His wife got sick, and now his friends feel very sympathetic toward him.\n",
      "Translate the following words into Cantonese:\n",
      "What did you say just now?\n",
      "Translate the following words into Cantonese:\n",
      "Hey punk kid! Are you thinking you can kill me?\n",
      "Translate the following words into Cantonese:\n",
      "Your way of doing things is really thoroughly ridiculous!\n",
      "Translate the following words into Cantonese:\n",
      "Of course Mom's home-cooked dishes are better than what you get in restaurants.\n",
      "Translate the following words into English:\n",
      "佢肥到掙爆件衫。\n",
      "Translate the following words into English:\n",
      "條鷄啱啱落鐘喇，佢個顧客去咗畀錢嘍。\n",
      "Translate the following words into English:\n",
      "真係冇呃你個喎。\n",
      "Translate the following words into English:\n",
      "有啲港人都出過國生活過。\n",
      "Translate the following words into English:\n",
      "呢條友仔喺銀行門口兜嚟兜去，好似有啲唔係路。\n",
      "Translate the following words into English:\n",
      "死𡃁仔！諗住收我皮！\n",
      "Translate the following words into English:\n",
      "你噉樣做法真係離晒譜！\n",
      "Translate the following words into English:\n",
      "你大早話乜嘢話？\n",
      "Translate the following words into English:\n",
      "佢太太病咗，佢啲朋友噔佢好肉赤。\n",
      "Translate the following words into English:\n",
      "梗係阿媽煮嘅屋企餸好過餐廳嘅。\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Translate the following words into English:\\n乜嘢都係波士決定嘅，打工仔啲人淨係得個知字。\\n\"},\n",
    "]\n",
    "\n",
    "# get 5 random samples from train and test dataset\n",
    "train_sample = abc_train_set.shuffle(seed=42).select(range(5))\n",
    "test_sample = abc_test_set.shuffle(seed=42).select(range(5))\n",
    "\n",
    "en_train_messages = {get_translate_prompt('Cantonese', sentence) for sentence in train_sample['en']}\n",
    "en_test_messages = {get_translate_prompt('Cantonese', sentence) for sentence in test_sample['en']}\n",
    "yue_train_messages = {get_translate_prompt('English', sentence) for sentence in train_sample['yue']}\n",
    "yue_test_messages = {get_translate_prompt('English', sentence) for sentence in test_sample['yue']}\n",
    "\n",
    "for messages in [en_train_messages, en_test_messages, yue_train_messages, yue_test_messages]:\n",
    "    for message in messages:\n",
    "        print(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefinedError",
     "evalue": "'str object' has no attribute 'role'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUndefinedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(input_ids\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# response = base_tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True, max_length=100)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1745\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[0;34m(self, conversation, chat_template, add_generation_prompt, tokenize, padding, truncation, max_length, return_tensors, return_dict, **tokenizer_kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# Compilation function uses a cache to avoid recompiling the same template\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m compiled_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compile_jinja_template(chat_template)\n\u001b[0;32m-> 1745\u001b[0m rendered \u001b[38;5;241m=\u001b[39m \u001b[43mcompiled_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconversation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspecial_tokens_map\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m padding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1750\u001b[0m     padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# There's only one sequence here, so \"longest\" makes no sense\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/jinja2/environment.py:1301\u001b[0m, in \u001b[0;36mTemplate.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment\u001b[38;5;241m.\u001b[39mconcat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_render_func(ctx))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m-> 1301\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/jinja2/environment.py:936\u001b[0m, in \u001b[0;36mEnvironment.handle_exception\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Exception handling helper.  This is used internally to either raise\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;124;03mrewritten exceptions or return a rendered traceback for the template.\u001b[39;00m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rewrite_traceback_stack\n\u001b[0;32m--> 936\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m rewrite_traceback_stack(source\u001b[38;5;241m=\u001b[39msource)\n",
      "File \u001b[0;32m<template>:2\u001b[0m, in \u001b[0;36mtop-level template code\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUndefinedError\u001b[0m: 'str object' has no attribute 'role'"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\n",
    "output_ids = model.generate(input_ids.to('cuda'))\n",
    "# response = base_tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True, max_length=100)\n",
    "response = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True, max_length=100)\n",
    "\n",
    "# Model response: \"Hello! How can I assist you today?\"\n",
    "print(\"Tuned model:\", response)\n",
    "\n",
    "input_ids = base_tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\n",
    "output_ids = base_model.generate(input_ids.to('cuda'))\n",
    "# response = base_tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True, max_length=100)\n",
    "response = base_tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True, max_length=100)\n",
    "\n",
    "print(\"Base model:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
