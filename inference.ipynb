{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/trans/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-05 20:43:08,141 - modelscope - INFO - PyTorch version 2.2.1 Found.\n",
      "2024-04-05 20:43:08,144 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
      "2024-04-05 20:43:08,184 - modelscope - INFO - Loading done! Current index file version is 1.13.1, with md5 b5a2c5fe01f7460b3e700a8ce7e6fc94 and a total number of 972 components indexed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import bitsandbytes\n",
    "import accelerate\n",
    "from datasets import Dataset\n",
    "\n",
    "from modelscope import snapshot_download\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from peft import PeftModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=r'/root/autodl-tmp/01ai/Yi-6B-Chat'\n",
    "model_dir = snapshot_download('01ai/Yi-6B-Chat', cache_dir='/root/autodl-tmp', revision='master')\n",
    "\n",
    "# base_tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True, padding_side='left', max_length=512, return_tensors='pt')\n",
    "\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(\n",
    "# \t model_path,\n",
    "# \t device_map='auto',\n",
    "# \t torch_dtype=torch.bfloat16,\n",
    "# \t trust_remote_code=True \n",
    "# ).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Translate the following words into English:\\n你係邊個？\"},\n",
    "]\n",
    "\n",
    "messages_plain = [\n",
    "    \"\"\"\n",
    "    <|im_start|> user\n",
    "    hi<|im_end|> \n",
    "    <|im_start|> assistant\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# input_ids = base_tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\n",
    "# output_ids = base_model.generate(input_ids.to('cuda'))\n",
    "# # response = base_tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True, max_length=100)\n",
    "# response = base_tokenizer.decode(output_ids[0], skip_special_tokens=False, max_length=100)\n",
    "\n",
    "# # Model response: \"Hello! How can I assist you today?\"\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64002\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('/root/autodl-tmp/01ai/Yi-6B-Chat', use_fast=True, padding_side='left', max_length=512, return_tensors='pt')\n",
    "base_tokenizer = AutoTokenizer.from_pretrained('/root/autodl-tmp/01ai/Yi-6B-Chat', use_fast=True, padding_side='left', max_length=512, return_tensors='pt')\n",
    "print(len(tokenizer.vocab))\n",
    "# tokenizer = AutoTokenizer.from_pretrained('/root/AIST4010-Cantonese-Translator/tokenizer', use_fast=True, padding_side='left', max_length=512, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PeftModel.from_pretrained(\n",
    "#     model,\n",
    "#    '/root/peft_model',\n",
    "#     is_trainable=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.49s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.30s/it]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "\t '/root/autodl-tmp/01ai/Yi-6B-Chat',\n",
    "\t device_map=device,\n",
    "\t torch_dtype=torch.bfloat16,\n",
    "     quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n",
    "\t trust_remote_code=True \n",
    ").eval()\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "\t model_path,\n",
    "\t device_map=device,\n",
    "\t torch_dtype=torch.bfloat16,\n",
    "\t quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n",
    "\t trust_remote_code=True \n",
    ").eval()\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.load_adapter('/root/autodl-tmp/peft_model_sft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_DIRECTORY = r'/root/'\n",
    "ABC_DICT_PATH = r'autodl-tmp/AIST4010-Cantonese-Translator-Data/ABC-Dict/abc_dict.csv'\n",
    "\n",
    "def load_abc_dataset():\n",
    "    abc_dict = pd.read_csv(REPO_DIRECTORY + ABC_DICT_PATH)\n",
    "    abc_dataset = Dataset.from_pandas(abc_dict)\n",
    "    return abc_dataset\n",
    "\n",
    "abc_set = load_abc_dataset()\n",
    "abc_shuffled_set = abc_set.train_test_split(seed=42, test_size=0.1)\n",
    "abc_train_set = abc_shuffled_set['train']\n",
    "abc_test_set = abc_shuffled_set['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'role': 'system',\n",
       "   'content': 'Translate the given English words to Cantonese.'},\n",
       "  {'role': 'user',\n",
       "   'content': \"Today my classmate wasn't careful, and kicked my left nut. It was very painful!\"}],\n",
       " [{'role': 'system',\n",
       "   'content': 'Translate the given Cantonese words to English.'},\n",
       "  {'role': 'user', 'content': '今日我同學唔小心，踢中我左面粒蛋蛋，好痛啊！'}]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_messages(sample):\n",
    "    lang_map = {'English': 'en', 'Cantonese': 'yue'}\n",
    "    def get_prompt(src, tgt, sample):\n",
    "        system_prompt = f\"Translate the given {src} words to {tgt}.\"\n",
    "        user_prompt = sample[lang_map[src]]\n",
    "        return system_prompt, user_prompt\n",
    "    system1, user1 = get_prompt('English', 'Cantonese', sample)\n",
    "    system2, user2 = get_prompt('Cantonese', 'English', sample)\n",
    "    return [[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system1\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user1\n",
    "        }],\n",
    "        [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system2\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user2\n",
    "        }]\n",
    "    ]\n",
    "    \n",
    "\n",
    "train_samples = abc_train_set.shuffle(seed=10).select(range(20))\n",
    "test_samples = abc_test_set.shuffle(seed=10).select(range(20))\n",
    "\n",
    "get_messages(train_samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/trans/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/root/miniconda3/envs/trans/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858,  4750,  3151,   592,\n",
      "         26212,  2823, 59569,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "         25585,   826,  1225, 13580,  3613, 59610, 59570, 11369,    97,   597,\n",
      "         19591,   826,  1999,  6230,    98,   983,   717,  1196, 16465,    99,\n",
      "             7, 59568,   144,     6, 14135,   144,  3569, 59646, 60588, 54274,\n",
      "         59706,   534,   453,   453, 13272,   101, 62328, 40994, 60488, 62295,\n",
      "         59599, 61847, 60751,   101, 59706, 60544, 61459,   103,     7]],\n",
      "       device='cuda:0')\n",
      "Base model:\n",
      "今天我個同學好唔小心，踢到我左邊的卵蛋，好痛呀！\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858, 26212,  2823, 59569,\n",
      "          3151,   592,  4750,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "         10721, 59646, 54274,   534,   453,   453, 13272,   101, 62328, 59642,\n",
      "         59646, 60488, 59724, 61329, 60751, 60751,   101, 59706, 60544, 60530,\n",
      "           103,     7, 59568,   144,     6, 14135,   144, 25585,   826,  1225,\n",
      "         13580, 24870, 19591,   795,   594,   567,  1999,  1682,  1935,    97,\n",
      "           648, 26288,   810,  1272,    99,     7]], device='cuda:0')\n",
      "Base model:\n",
      "Today my classmate accidentally kicked me in the left testicle, it hurts so much!\n",
      "\n",
      "[None]\n"
     ]
    }
   ],
   "source": [
    "def model_output(model, tokenizer, messages, name=None):\n",
    "        for prompt in messages:\n",
    "                input_ids = tokenizer.apply_chat_template(conversation=prompt, tokenize=True, add_generation_prompt=True, return_tensors='pt')\n",
    "                with torch.cuda.amp.autocast():\n",
    "                        output_ids = model.generate(input_ids.to('cuda'), max_new_tokens=100)\n",
    "                response = base_tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True, max_length=100)\n",
    "                # response = tokenizer.decode(output_ids[0], skip_special_tokens=False, max_length=100)\n",
    "                # print(output_ids)\n",
    "                print(f\"{name}:\\n{response}\\n\")\n",
    "\n",
    "\n",
    "print([model_output(base_model, base_tokenizer, get_messages(train_samples[0]), 'Base model'),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_outputs(samples):\n",
    "    for sample in samples:\n",
    "        print(sample)\n",
    "        prompts = get_messages(sample)\n",
    "        for prompt in prompts:\n",
    "            print(f\"Prompt:\\n{prompt[1]['content']}\")\n",
    "            print()\n",
    "            model_output(base_model, base_tokenizer, [prompt], 'Base model')\n",
    "            model_output(model, tokenizer, [prompt], 'Fine-tuned model')\n",
    "\n",
    "# compare_outputs(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858,  4750,  3151,   592,\n",
      "         26212,  2823, 59569,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "         25585,   826,  1225, 13580,  3613, 59610, 59570, 11369,    97,   597,\n",
      "         19591,   826,  1999,  6230,    98,   983,   717,  1196, 16465,    99,\n",
      "             7, 59568,   144,     6, 14135,   144, 10721, 54274,   534,   453,\n",
      "           453, 13272,   101, 62328, 59642, 59646, 60488,   537,   438,   476,\n",
      "           102, 59706, 60544,   534,   449,   455,   103,     7]],\n",
      "       device='cuda:0')\n",
      "Fine-tuned model:\n",
      "今日同學唔小心，踢中我左腫。好痛吖！\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858, 26212,  2823, 59569,\n",
      "          3151,   592,  4750,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "         10721, 59646, 54274,   534,   453,   453, 13272,   101, 62328, 59642,\n",
      "         59646, 60488, 59724, 61329, 60751, 60751,   101, 59706, 60544, 60530,\n",
      "           103,     7, 59568,   144,     6, 14135,   144, 25585,   826,  1225,\n",
      "         13580, 24870, 19591,   795,   594,   567,  1999,  1682,  1935,    97,\n",
      "           648,  1403, 26288,    99,     7]], device='cuda:0')\n",
      "Fine-tuned model:\n",
      "Today my classmate accidentally kicked me in the left testicle, it really hurts!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_output(model, tokenizer, get_messages(train_samples[0]), 'Fine-tuned model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': \"Today my classmate wasn't careful, and kicked my left nut. It was very painful!\", 'yue': '今日我同學唔小心，踢中我左面粒蛋蛋，好痛啊！'}\n",
      "Prompt:\n",
      "Translate the given English words to Cantonese.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858,  4750,  3151,   592,\n",
      "         26212,  2823, 59569,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "         25585,   826,  1225, 13580,  3613, 59610, 59570, 11369,    97,   597,\n",
      "         19591,   826,  1999,  6230,    98,   983,   717,  1196, 16465,    99,\n",
      "             7, 59568,   144,     6, 14135,   144, 10721, 59646, 60588, 54274,\n",
      "         59706,   534,   453,   453, 13272,   101, 62328, 40994, 60588, 60488,\n",
      "         62295,   536,   462,   495, 62278,   101, 59706, 60544, 61459,   103,\n",
      "             7]], device='cuda:0')\n",
      "Base model:\n",
      "今日我個同學好唔小心，踢到我個左邊睾丸，好痛呀！\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858,  4750,  3151,   592,\n",
      "         26212,  2823, 59569,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "         25585,   826,  1225, 13580,  3613, 59610, 59570, 11369,    97,   597,\n",
      "         19591,   826,  1999,  6230,    98,   983,   717,  1196, 16465,    99,\n",
      "             7, 59568,   144,     6, 14135,   144, 10721, 54274, 59630, 62447,\n",
      "         13272,   101, 62328, 59642, 59646, 60488,   537,   437,   490,   102,\n",
      "         59706, 60544, 61459,   103,     7]], device='cuda:0')\n",
      "Fine-tuned model:\n",
      "今日同學不夠小心，踢中我左脹。好痛呀！\n",
      "\n",
      "Prompt:\n",
      "Translate the given Cantonese words to English.\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858, 26212,  2823, 59569,\n",
      "          3151,   592,  4750,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "         10721, 59646, 54274,   534,   453,   453, 13272,   101, 62328, 59642,\n",
      "         59646, 60488, 59724, 61329, 60751, 60751,   101, 59706, 60544, 60530,\n",
      "           103,     7, 59568,   144,     6, 14135,   144, 25585,   826,  1225,\n",
      "         13580, 24870, 19591,   795,   594,   567,  1999,  1682,  1935,    97,\n",
      "           648,  1403, 26288,    99,     7]], device='cuda:0')\n",
      "Base model:\n",
      "Today my classmate accidentally kicked me in the left testicle, it really hurts!\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858, 26212,  2823, 59569,\n",
      "          3151,   592,  4750,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "         10721, 59646, 54274,   534,   453,   453, 13272,   101, 62328, 59642,\n",
      "         59646, 60488, 59724, 61329, 60751, 60751,   101, 59706, 60544, 60530,\n",
      "           103,     7, 59568,   144,     6, 14135,   144, 25585,   826,  1225,\n",
      "         13580, 24870, 19591,   795,   594,   567,  1999,  1682,  1935,    97,\n",
      "           648,  1403, 26288,    99,     7]], device='cuda:0')\n",
      "Fine-tuned model:\n",
      "Today my classmate accidentally kicked me in the left testicle, it really hurts!\n",
      "\n",
      "{'en': 'My stomach hurt so I was rolling this way and that on the bed.', 'yue': '我肚痛到捵牀捵蓆。'}\n",
      "Prompt:\n",
      "Translate the given English words to Cantonese.\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858,  4750,  3151,   592,\n",
      "         26212,  2823, 59569,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "          6056, 15112,  8192,   810,   616,   717, 14967,   719,  1217,   597,\n",
      "           639,   632,   567,  3807,    98,     7, 59568,   144,     6, 14135,\n",
      "           144,  2861, 61186, 60544, 16729,   534,   447,   483, 60323,   101,\n",
      "         25907,   535,   492,   495, 59665, 60741, 12008,   102,     7]],\n",
      "       device='cuda:0')\n",
      "Base model:\n",
      "我的胃痛得很厲害，所以我滾到床上去。\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858,  4750,  3151,   592,\n",
      "         26212,  2823, 59569,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "          6056, 15112,  8192,   810,   616,   717, 14967,   719,  1217,   597,\n",
      "           639,   632,   567,  3807,    98,     7, 59568,   144,     6, 14135,\n",
      "           144, 59646, 61815, 60544, 59721,   534,   451,   434,   534,   444,\n",
      "           492,   101, 62446, 63195,   535,   446,   482,   534,   451,   456,\n",
      "         60799, 60599,   534,   457,   438,   102,     7]], device='cuda:0')\n",
      "Fine-tuned model:\n",
      "我肚痛得咁勻，雙腳捱咗過來嘅。\n",
      "\n",
      "Prompt:\n",
      "Translate the given Cantonese words to English.\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858, 26212,  2823, 59569,\n",
      "          3151,   592,  4750,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "         59646, 61815, 60544, 59665,   535,   446,   486,   536,   442,   433,\n",
      "           535,   446,   486,   537,   452,   439,   102,     7, 59568,   144,\n",
      "             6, 14135,   144, 59646, 61815, 60544, 59665,   535,   446,   486,\n",
      "           536,   442,   433,   535,   446,   486,   537,   452,   439,    98,\n",
      "             7]], device='cuda:0')\n",
      "Base model:\n",
      "我肚痛到捵牀捵蓆.\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858, 26212,  2823, 59569,\n",
      "          3151,   592,  4750,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "         59646, 61815, 60544, 59665,   535,   446,   486,   536,   442,   433,\n",
      "           535,   446,   486,   537,   452,   439,   102,     7, 59568,   144,\n",
      "             6, 14135,   144,  6056, 15112, 35168,   678,   810, 11376,   616,\n",
      "           748, 59610, 59570,  8894,  1310,   632,   567,  3807,    98,     7]],\n",
      "       device='cuda:0')\n",
      "Fine-tuned model:\n",
      "My stomach pains are so severe I can't lie down on the bed.\n",
      "\n",
      "{'en': \"I'm sorry, I can't recall right off the bat where those things of yours had got placed.\", 'yue': '唔好意思，我一時醒唔起你嗰啲嘢放咗喺邊度嘅。'}\n",
      "Prompt:\n",
      "Translate the given English words to Cantonese.\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858,  4750,  3151,   592,\n",
      "         26212,  2823, 59569,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "         59597, 59610, 59583,  9509,    97,   616,   748, 59610, 59570,  8249,\n",
      "          1288,   965,   567, 10880,  1151,  1367,  1613,   593, 12796,   986,\n",
      "          1656,  6594,    98,     7, 59568,   144,     6, 14135,   144, 59646,\n",
      "         61641, 11265, 59725, 40401, 59932, 60403, 61898, 59633,   102,     7]],\n",
      "       device='cuda:0')\n",
      "Base model:\n",
      "我記不起你東西放哪裡了。\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858,  4750,  3151,   592,\n",
      "         26212,  2823, 59569,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "         59597, 59610, 59583,  9509,    97,   616,   748, 59610, 59570,  8249,\n",
      "          1288,   965,   567, 10880,  1151,  1367,  1613,   593, 12796,   986,\n",
      "          1656,  6594,    98,     7, 59568,   144,     6, 14135,   144, 60713,\n",
      "           534,   453,   453, 60118,   101, 59646,   534,   453,   453, 61641,\n",
      "         59721, 59725,   534,   457,   438,   534,   457,   467, 59932,   534,\n",
      "           451,   456, 62295, 59781,   102,     7]], device='cuda:0')\n",
      "Fine-tuned model:\n",
      "對唔住，我唔記得你嘅嘢放咗邊度。\n",
      "\n",
      "Prompt:\n",
      "Translate the given Cantonese words to English.\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858, 26212,  2823, 59569,\n",
      "          3151,   592,  4750,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "           534,   453,   453, 59706,  7112,   101, 33442, 60531, 60808,   534,\n",
      "           453,   453, 59786, 59725,   534,   456,   481,   534,   454,   483,\n",
      "           534,   457,   467, 59932,   534,   451,   456,   534,   455,   491,\n",
      "         62295, 59781,   534,   457,   438,   102,     7, 59568,   144,     6,\n",
      "         14135,   144, 36778,    97,   616, 59610, 59583,   728,  4836,  1191,\n",
      "           702,   567,  2964,    97,   796,   616, 59610,   928,  1628,   826,\n",
      "          1485,   592,  1314,   641,    98, 15842,   641,  3310,  2120,   795,\n",
      "          1151,   641,  1710,   788,  1613,   100,     7]], device='cuda:0')\n",
      "Base model:\n",
      "Sorry, I'm not feeling well at the moment, but I'll try my best to help you. Could you please tell me where you put your things?\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858, 26212,  2823, 59569,\n",
      "          3151,   592,  4750,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "           534,   453,   453, 59706,  7112,   101, 33442, 60531, 60808,   534,\n",
      "           453,   453, 59786, 59725,   534,   456,   481,   534,   454,   483,\n",
      "           534,   457,   467, 59932,   534,   451,   456,   534,   455,   491,\n",
      "         62295, 59781,   534,   457,   438,   102,     7, 59568,   144,     6,\n",
      "         14135,   144, 59597, 59610, 59583,  1403,  9509,    97,   631,   562,\n",
      "          2964,   616, 17682,   592,  1710,   788,  3695,  1151,   648,  1206,\n",
      "           629,    98,     7]], device='cuda:0')\n",
      "Fine-tuned model:\n",
      "I'm really sorry, for a moment I forgot to put your stuff where it should be.\n",
      "\n",
      "{'en': 'According to what the old folks say, the winter solstice is more important than the Chinese New Year.', 'yue': '啲老人家話事偈，冬大過年。'}\n",
      "Prompt:\n",
      "Translate the given English words to Cantonese.\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858,  4750,  3151,   592,\n",
      "         26212,  2823, 59569,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "         39559,   592,   981,   567,  1851,  8413,  1441,    97,   567,  7814,\n",
      "          1496,   608,   830,   620,   863,  1941,   989,   567,  6837,  1876,\n",
      "          8567,    98,     7, 59568,   144,     6, 14135,   144, 39559,   592,\n",
      "           567,  1851,  8413,    97,   567,  7814,  1496,   608,   830,   620,\n",
      "           863,  1941,   989,   567,  6837,  1876,  8567,    98,     7]],\n",
      "       device='cuda:0')\n",
      "Base model:\n",
      "According to the old folks, the winter solstice is more important than the Chinese New Year.\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858,  4750,  3151,   592,\n",
      "         26212,  2823, 59569,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "         39559,   592,   981,   567,  1851,  8413,  1441,    97,   567,  7814,\n",
      "          1496,   608,   830,   620,   863,  1941,   989,   567,  6837,  1876,\n",
      "          8567,    98,     7, 59568,   144,     6, 14135,   144, 29630, 60913,\n",
      "           537,   493,   474, 60917, 59709,   101, 52776, 44386, 60799, 27480,\n",
      "           102,     7]], device='cuda:0')\n",
      "Fine-tuned model:\n",
      "根據長輩說法，冬至更重要過新年。\n",
      "\n",
      "Prompt:\n",
      "Translate the given Cantonese words to English.\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858, 26212,  2823, 59569,\n",
      "          3151,   592,  4750,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "           534,   454,   483, 50873, 61845, 59736,   534,   434,   441,   101,\n",
      "         60820, 59647, 60799, 59663,   102,     7, 59568,   144,     6, 14135,\n",
      "           144,  1263,  1851,  1065,  1441,   648,    97,  7814,   620,  7577,\n",
      "           989,   567,  1876,  8567,    98,     7]], device='cuda:0')\n",
      "Base model:\n",
      "The old people say it, winter is bigger than the New Year.\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858, 26212,  2823, 59569,\n",
      "          3151,   592,  4750,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "           534,   454,   483, 50873, 61845, 59736,   534,   434,   441,   101,\n",
      "         60820, 59647, 60799, 59663,   102,     7, 59568,   144,     6, 14135,\n",
      "           144,  1263, 49066,  1441,   648, 59610, 59575,   567,   922,   631,\n",
      "           567,  1741,  7814,  1496,   608,   830,    98,     7]],\n",
      "       device='cuda:0')\n",
      "Fine-tuned model:\n",
      "The elders say it's the time for the big winter solstice.\n",
      "\n",
      "{'en': \"Tonight we've been invited to a wedding banquet.\", 'yue': '今晚有人請飲。'}\n",
      "Prompt:\n",
      "Translate the given English words to Cantonese.\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858,  4750,  3151,   592,\n",
      "         26212,  2823, 59569,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "         59600, 51152,   666, 59610,   613,   961, 12456,   592,   562,  8760,\n",
      "          7255,  3134,    98,     7, 59568,   144,     6, 14135,   144, 33490,\n",
      "          8506, 59777, 54280, 59793, 60424, 62101,   102,     7]],\n",
      "       device='cuda:0')\n",
      "Base model:\n",
      "今晚我們被邀請去婚宴。\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858,  4750,  3151,   592,\n",
      "         26212,  2823, 59569,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "         59600, 51152,   666, 59610,   613,   961, 12456,   592,   562,  8760,\n",
      "          7255,  3134,    98,     7, 59568,   144,     6, 14135,   144, 33490,\n",
      "         59635, 60588, 60424, 62101, 61919,  8506, 59793, 60158,   102,     7]],\n",
      "       device='cuda:0')\n",
      "Fine-tuned model:\n",
      "今晚有個婚宴請我們去吃。\n",
      "\n",
      "Prompt:\n",
      "Translate the given Cantonese words to English.\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858, 26212,  2823, 59569,\n",
      "          3151,   592,  4750,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "         33490,  5189, 61919, 63105,   102,     7, 59568,   144,     6, 14135,\n",
      "           144, 59600, 51152,  2491,   620, 28516,   631,   562,  4561,    98,\n",
      "             7]], device='cuda:0')\n",
      "Base model:\n",
      "Tonight someone is inviting for a drink.\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858, 26212,  2823, 59569,\n",
      "          3151,   592,  4750,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "         33490,  5189, 61919, 63105,   102,     7, 59568,   144,     6, 14135,\n",
      "           144, 59600, 51152,  2491,   620, 28516,   745,   592,  4561,    98,\n",
      "             7]], device='cuda:0')\n",
      "Fine-tuned model:\n",
      "Tonight someone is inviting us to drink.\n",
      "\n",
      "{'en': 'He has not yet been to school, or He has not studied for it yet.', 'yue': '佢未讀書。'}\n",
      "Prompt:\n",
      "Translate the given English words to Cantonese.\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858,  4750,  3151,   592,\n",
      "         26212,  2823, 59569,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "          4431,   815,   728,  2583,   961,   592,  1923,    97,   705,  1214,\n",
      "           815,   728,  7945,   631,   648,  2583,    98,     7, 59568,   144,\n",
      "             6, 14135,   144, 59686, 16750, 59654, 59700, 59672,   101,  2850,\n",
      "         59686, 16750, 15375, 59732,  2188,   102,     7]], device='cuda:0')\n",
      "Base model:\n",
      "他尚未上过学，或者他尚未为此而学习。\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858,  4750,  3151,   592,\n",
      "         26212,  2823, 59569,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "          4431,   815,   728,  2583,   961,   592,  1923,    97,   705,  1214,\n",
      "           815,   728,  7945,   631,   648,  2583,    98,     7, 59568,   144,\n",
      "             6, 14135,   144,   533,   494,   467, 60049, 59654, 60799, 60579,\n",
      "           101, 59876,   533,   494,   467, 60049, 62283, 60799, 60329, 59726,\n",
      "         61473,   102,     7]], device='cuda:0')\n",
      "Fine-tuned model:\n",
      "佢未上過學，或佢未讀過呢本書。\n",
      "\n",
      "Prompt:\n",
      "Translate the given Cantonese words to English.\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858, 26212,  2823, 59569,\n",
      "          3151,   592,  4750,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "           533,   494,   467, 60049, 62283, 61473,   102,     7, 59568,   144,\n",
      "             6, 14135,   144,  4431,  8934, 59610, 59570,  1326,   567,  1687,\n",
      "            98,     7]], device='cuda:0')\n",
      "Base model:\n",
      "He hasn't read the book.\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858, 26212,  2823, 59569,\n",
      "          3151,   592,  4750,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "           533,   494,   467, 60049, 62283, 61473,   102,     7, 59568,   144,\n",
      "             6, 14135,   144,  4431,  8934, 59610, 59570,  6121,  1923,    98,\n",
      "             7]], device='cuda:0')\n",
      "Fine-tuned model:\n",
      "He hasn't finished school.\n",
      "\n",
      "{'en': 'Back at that time I worked as a one-star probationary police inspector, I earned twenty to thirty thousand bucks a month.', 'yue': '嗰陣時我做過一粒花嘅幫辦仔，搵兩三皮一個月。'}\n",
      "Prompt:\n",
      "Translate the given English words to Cantonese.\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858,  4750,  3151,   592,\n",
      "         26212,  2823, 59569,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "         10298,   702,   639,   922,   616,  3983,   659,   562,   853, 59594,\n",
      "          6481, 40897,   898,  4031, 50801,    97,   616, 10989, 12445,   592,\n",
      "         19566, 12340, 25883,   562,  1780,    98,     7, 59568,   144,     6,\n",
      "         14135,   144, 61007, 59663, 59646, 62376, 59625, 60273, 62007, 61584,\n",
      "         61748, 60298, 61343, 42994,   101, 59646,   534,   456,   481, 35542,\n",
      "          5198, 59635,  7126, 59905, 14398, 61515,   102,     7]],\n",
      "       device='cuda:0')\n",
      "Base model:\n",
      "當年我係一星級見習警務督察，我嗰個月收入有二十至三十萬。\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858,  4750,  3151,   592,\n",
      "         26212,  2823, 59569,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "         10298,   702,   639,   922,   616,  3983,   659,   562,   853, 59594,\n",
      "          6481, 40897,   898,  4031, 50801,    97,   616, 10989, 12445,   592,\n",
      "         19566, 12340, 25883,   562,  1780,    98,     7, 59568,   144,     6,\n",
      "         14135,   144, 61007, 59663, 59646,   534,   456,   481, 60531, 59842,\n",
      "         59625, 60273, 62007, 60298, 61137,   101, 59751, 61476,  7126, 59905,\n",
      "         14398, 61515,   102,     7]], device='cuda:0')\n",
      "Fine-tuned model:\n",
      "當年我嗰時做一星級警員，月薪二十至三十萬。\n",
      "\n",
      "Prompt:\n",
      "Translate the given Cantonese words to English.\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858, 26212,  2823, 59569,\n",
      "          3151,   592,  4750,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "           534,   456,   481, 63598, 60531, 58409, 60799, 59625, 61329, 60048,\n",
      "           534,   457,   438, 62592, 61778, 61365,   101,   535,   449,   486,\n",
      "         61348, 59799, 60445, 12666, 59751,   102,     7, 59568,   144,     6,\n",
      "         14135,   144, 41910,   639,   922,    97,   616,   717,   562, 18063,\n",
      "          7492,  5576,    97,   616,  1316,   592,  5253,  1074,   705,  1604,\n",
      "          7883,   562,  1780,    98,     7]], device='cuda:0')\n",
      "Base model:\n",
      "During that time, I was a flower delivery boy, I used to earn two or three dollars a month.\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858, 26212,  2823, 59569,\n",
      "          3151,   592,  4750,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "           534,   456,   481, 63598, 60531, 58409, 60799, 59625, 61329, 60048,\n",
      "           534,   457,   438, 62592, 61778, 61365,   101,   535,   449,   486,\n",
      "         61348, 59799, 60445, 12666, 59751,   102,     7, 59568,   144,     6,\n",
      "         14135,   144,  4786,   562, 18063, 59594,  5945,  4253, 59610, 59575,\n",
      "         14135,    97,   616,  1442,  1074,   705,  1604,  7883,   562,  1780,\n",
      "            98,     7]], device='cuda:0')\n",
      "Fine-tuned model:\n",
      "As a flower-arranger's assistant, I made two or three dollars a month.\n",
      "\n",
      "{'en': \"Why does everyone have it, but I'm the only who doesn't?\", 'yue': '點解個個都有，得我一個冇？'}\n",
      "Prompt:\n",
      "Translate the given English words to Cantonese.\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858,  4750,  3151,   592,\n",
      "         26212,  2823, 59569,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "         12665,  1269,  2983,   726,   648,    97,   796,   616, 59610, 59583,\n",
      "           567,  1035,   896,  2153, 59610, 59570,   100,     7, 59568,   144,\n",
      "             6, 14135,   144, 12665,  1269,  2983,   726,   648,    97,   796,\n",
      "           616, 59610, 59583,   567,  1035,   853,   896,  2153, 59610, 59570,\n",
      "           100,     7]], device='cuda:0')\n",
      "Base model:\n",
      "Why does everyone have it, but I'm the only one who doesn't?\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858,  4750,  3151,   592,\n",
      "         26212,  2823, 59569,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "         12665,  1269,  2983,   726,   648,    97,   796,   616, 59610, 59583,\n",
      "           567,  1035,   896,  2153, 59610, 59570,   100,     7, 59568,   144,\n",
      "             6, 14135,   144, 60379, 60450, 61512, 13403,  4299,   101, 59646,\n",
      "         62072, 10870,   104,     7]], device='cuda:0')\n",
      "Fine-tuned model:\n",
      "為甚麼大家都有的，我卻沒有？\n",
      "\n",
      "Prompt:\n",
      "Translate the given Cantonese words to English.\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858, 26212,  2823, 59569,\n",
      "          3151,   592,  4750,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "         61282, 59862, 60588, 60588,  5074,   101, 59721, 59646, 12666,   534,\n",
      "           439,   440,   104,     7, 59568,   144,     6, 14135,   144, 12665,\n",
      "           678,   786,   567,  2501,  2286, 13113,    97,   796,   616,  1064,\n",
      "           567,  1035,   853,  1691,   853,   100,     7]], device='cuda:0')\n",
      "Base model:\n",
      "Why are all the others getting gifts, but I am the only one without one?\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858, 26212,  2823, 59569,\n",
      "          3151,   592,  4750,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "         61282, 59862, 60588, 60588,  5074,   101, 59721, 59646, 12666,   534,\n",
      "           439,   440,   104,     7, 59568,   144,     6, 14135,   144, 12665,\n",
      "           620,  2983,   815,   853,  5244,   795,   100,     7]],\n",
      "       device='cuda:0')\n",
      "Fine-tuned model:\n",
      "Why is everyone has one except me?\n",
      "\n",
      "{'en': \"You don't need to be concerned about the fall in the value of the US dollar, as I still know how to deal in foreign currency and make money.\", 'yue': '你唔使擔心美元嘅貶值，我仲識套匯賺錢。'}\n",
      "Prompt:\n",
      "Translate the given English words to Cantonese.\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858,  4750,  3151,   592,\n",
      "         26212,  2823, 59569,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "          3961,  1182, 59610, 59570,  1049,   592,   629,  7703,   883,   567,\n",
      "          3038,   594,   567,  1830,   593,   567,  2134, 11790,    97,   659,\n",
      "           616,  1451,  1082,  1040,   592,  2527,   594,  6485, 12959,   597,\n",
      "          1150,  2166,    98,     7, 59568,   144,     6, 14135,   144, 59725,\n",
      "         63725, 59914,  9773, 41324, 28533,  6685,   101,  1728, 62160, 59634,\n",
      "          7518, 10386, 59820, 60605,  4730,   101, 60079, 59671, 23218, 39377,\n",
      "           102,     7]], device='cuda:0')\n",
      "Base model:\n",
      "你毋需担心美金价值的下降，因为吾人依然熟悉外币交易，且能从中获利。\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858,  4750,  3151,   592,\n",
      "         26212,  2823, 59569,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "          3961,  1182, 59610, 59570,  1049,   592,   629,  7703,   883,   567,\n",
      "          3038,   594,   567,  1830,   593,   567,  2134, 11790,    97,   659,\n",
      "           616,  1451,  1082,  1040,   592,  2527,   594,  6485, 12959,   597,\n",
      "          1150,  2166,    98,     7, 59568,   144,     6, 14135,   144, 59725,\n",
      "           534,   453,   453, 59897, 62471, 59712,  5061,   534,   457,   438,\n",
      "           537,   483,   487, 60110,   101, 59646,   534,   455,   491, 59820,\n",
      "         62861, 21384,  2284,  8173, 59635, 36074,   101,  1229,   537,   484,\n",
      "           491, 59665, 62617,   102,     7]], device='cuda:0')\n",
      "Fine-tuned model:\n",
      "你唔使擔心美元嘅貶值，我喺外幣投資方面仍然有經驗，可以賺到錢。\n",
      "\n",
      "Prompt:\n",
      "Translate the given Cantonese words to English.\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858, 26212,  2823, 59569,\n",
      "          3151,   592,  4750,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "         59725,   534,   453,   453, 59897, 62471, 59712,  5061,   534,   457,\n",
      "           438,   537,   483,   487, 60110,   101, 59646, 61669, 62028, 60494,\n",
      "         63664,   537,   484,   491, 62617,   102,     7, 59568,   144,     6,\n",
      "         14135,   144,  3961,  1182, 59610, 59570,  1049,   592,  6761,   883,\n",
      "           567,  1825, 52704,   593,   567, 11790,    97,   616,  1451,  1082,\n",
      "          1040,   592,   771, 12959,  6778,   597,  1150,  2166,    98,     7]],\n",
      "       device='cuda:0')\n",
      "Base model:\n",
      "You don't need to worry about the depreciation of the dollar, I still know how to do currency exchange and make money.\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858, 26212,  2823, 59569,\n",
      "          3151,   592,  4750,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "         59725,   534,   453,   453, 59897, 62471, 59712,  5061,   534,   457,\n",
      "           438,   537,   483,   487, 60110,   101, 59646, 61669, 62028, 60494,\n",
      "         63664,   537,   484,   491, 62617,   102,     7, 59568,   144,     6,\n",
      "         14135,   144,  3961,  1182, 59610, 59570,  1049,   592,  6761,   883,\n",
      "           567,  1825, 52704,   593,   567, 11790,    97,   616,   748,  1451,\n",
      "          1150,   562, 10083,   737, 25142,  3077,   567, 12959,    98,     7]],\n",
      "       device='cuda:0')\n",
      "Fine-tuned model:\n",
      "You don't need to worry about the depreciation of the dollar, I can still make a profit by hedging the currency.\n",
      "\n",
      "{'en': 'The two teams of the company kicked up a ruckus and the big boss had to settle it personally.', 'yue': '公司兩組人炒大鑊，要由大老細出面擺平。'}\n",
      "Prompt:\n",
      "Translate the given English words to Cantonese.\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858,  4750,  3151,   592,\n",
      "         26212,  2823, 59569,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "          1263,  1074,  5155,   593,   567,  2165, 19591,   828,   562,   731,\n",
      "          1836,   633,   597,   567,  1741, 10920,   986,   592, 14738,   648,\n",
      "          9363,    98,     7, 59568,   144,     6, 14135,   144,  1424, 61183,\n",
      "         59752, 61348, 61791, 35051, 26196, 59633, 62265, 62445,   101, 59647,\n",
      "         59857,   538,   456,   439, 12011, 62061, 59689, 59676, 59724, 35754,\n",
      "           102,     7]], device='cuda:0')\n",
      "Base model:\n",
      "公司內部兩隊之間發生了爭執，大老闆不得不親自出面解決。\n",
      "\n",
      "tensor([[    6,  1328,   144,  7759, 14429,   567,  1858,  4750,  3151,   592,\n",
      "         26212,  2823, 59569,    98,     7, 59568,   144,     6,  2942,   144,\n",
      "          1263,  1074,  5155,   593,   567,  2165, 19591,   828,   562,   731,\n",
      "          1836,   633,   597,   567,  1741, 10920,   986,   592, 14738,   648,\n",
      "          9363,    98,     7, 59568,   144,     6, 14135,   144,  1424, 61348,\n",
      "         61791, 59634, 61731,   538,   477,   472, 60760, 59633,   101, 59857,\n",
      "           538,   456,   439, 61108,  8969, 62061, 59689, 59676, 59724, 61630,\n",
      "         60411,   102,     7]], device='cuda:0')\n",
      "Fine-tuned model:\n",
      "公司兩隊人馬鬧翻了，老闆娘都要親自出面調停。\n",
      "\n",
      "Prompt:\n",
      "Translate the given Cantonese words to English.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompare_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_samples\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m, in \u001b[0;36mcompare_outputs\u001b[0;34m(samples)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mprompt[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m----> 8\u001b[0m \u001b[43mmodel_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBase model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m model_output(model, tokenizer, [prompt], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFine-tuned model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m, in \u001b[0;36mmodel_output\u001b[0;34m(model, tokenizer, messages, name)\u001b[0m\n\u001b[1;32m      3\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(conversation\u001b[38;5;241m=\u001b[39mprompt, tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m----> 5\u001b[0m         output_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m response \u001b[38;5;241m=\u001b[39m base_tokenizer\u001b[38;5;241m.\u001b[39mdecode(output_ids[\u001b[38;5;241m0\u001b[39m][input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# response = tokenizer.decode(output_ids[0], skip_special_tokens=False, max_length=100)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/generation/utils.py:1592\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1584\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1585\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1586\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1587\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1588\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1589\u001b[0m     )\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1608\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1609\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1610\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1611\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1616\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1617\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/generation/utils.py:2696\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2693\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2695\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2696\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2697\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2699\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2700\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2701\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2704\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1176\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1173\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1176\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1189\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1019\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1009\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1010\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1016\u001b[0m         cache_position,\n\u001b[1;32m   1017\u001b[0m     )\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1019\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:740\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    737\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    739\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    750\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    752\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:639\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mforward(\n\u001b[1;32m    628\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    629\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    634\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    635\u001b[0m     )\n\u001b[1;32m    637\u001b[0m bsz, q_len, _ \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m--> 639\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\n\u001b[1;32m    641\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/accelerate/hooks.py:161\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_forward\u001b[39m(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 161\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hf_hook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mno_grad:\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/accelerate/hooks.py:356\u001b[0m, in \u001b[0;36mAlignDevicesHook.pre_forward\u001b[0;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_pointers_to_remove\u001b[38;5;241m.\u001b[39madd((value\u001b[38;5;241m.\u001b[39mdata_ptr(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device))\n\u001b[1;32m    347\u001b[0m         set_module_tensor_to_device(\n\u001b[1;32m    348\u001b[0m             module,\n\u001b[1;32m    349\u001b[0m             name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    353\u001b[0m             tied_params_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map,\n\u001b[1;32m    354\u001b[0m         )\n\u001b[0;32m--> 356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m send_to_device(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device), \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mskip_keys\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/accelerate/utils/operations.py:148\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_to_device\u001b[39m(tensor, device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, skip_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    136\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m    Recursively sends the elements in a nested list/tuple/dictionary of tensors to a given device.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m        The same data structure as `tensor` with all tensors sent to the proper device.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_torch_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;66;03m# `torch.Tensor.to(\"npu\")` could not find context when called for the first time (see this [issue](https://gitee.com/ascend/pytorch/issues/I8KECW?from=project-issue)).\u001b[39;00m\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    151\u001b[0m             device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpu:0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "compare_outputs(train_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': 'Miss Linda.', 'yue': 'Linda姐。'}\n",
      "Prompt:\n",
      "Translate the following words from Cantonese to English:\n",
      "Linda姐。\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742, 26212,\n",
      "          2823, 59569,   592,  4750, 59601,   144, 59620, 27164, 60706,   102,\n",
      "             7, 59568,   144,     6, 14135,   144, 59620, 27164, 60706,    98,\n",
      "             7]], device='cuda:0')\n",
      "Base model:\n",
      "Linda姐.\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742, 26212,\n",
      "          2823, 59569,   592,  4750, 59601,   144, 59620, 27164, 60706,   102,\n",
      "             7, 59568,   144,     6, 14135,   144, 59620, 27164, 60706,   101,\n",
      "         62376, 59967, 12666, 60521, 30916, 59568,   534,   457,   438, 11940,\n",
      "           102,     7]], device='cuda:0')\n",
      "Fine-tuned model:\n",
      "Linda姐，係指一個叫 Linda 嘅女士。\n",
      "\n",
      "Prompt:\n",
      "Translate the following words from English to Cantonese:\n",
      "Miss Linda.\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742,  4750,\n",
      "           592, 26212,  2823, 59569, 59601,   144, 30874, 30916,    98,     7,\n",
      "         59568,   144,     6, 14135,   144, 30874, 30916,    98,     7]],\n",
      "       device='cuda:0')\n",
      "Base model:\n",
      "Miss Linda.\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742,  4750,\n",
      "           592, 26212,  2823, 59569, 59601,   144, 30874, 30916,    98,     7,\n",
      "         59568,   144,     6, 14135,   144, 30874, 30916, 59568, 62376, 14877,\n",
      "           101,  7112, 62376, 60352, 60181,   537,   490,   469, 60353,   102,\n",
      "         59568, 61280, 14877, 60760, 59687, 62098, 61518, 61845,   101, 59568,\n",
      "          1229, 62376, 60352, 60181,   537,   490,   469, 60353,   101, 59568,\n",
      "         59784, 62376, 60352, 60181,   537,   490,   469, 60353,   534,   457,\n",
      "           438, 62098, 61518, 61845, 60521, 60352, 60036, 62911, 60353,   102,\n",
      "         59568,  1953,   101, 59568, 60352, 30874, 30916, 60353, 59568,  1229,\n",
      "         60760, 59842, 60352, 60036, 62911,   534,   457,   438, 60251, 22799,\n",
      "         60353,   102,     7]], device='cuda:0')\n",
      "Fine-tuned model:\n",
      "Miss Linda 係英文，意思係「失蹤」。 從英文翻成廣東話， 可以係「失蹤」， 但係「失蹤」嘅廣東話叫「走佬」。 所以， 「Miss Linda」 可以翻做「走佬嘅李小姐」。\n",
      "\n",
      "{'en': 'What is he doing in the kitchen to make such banging and clanging noise?', 'yue': '佢喺廚房度打得咁𠽤叻𡃈嘞做乜嘢啊？'}\n",
      "Prompt:\n",
      "Translate the following words from Cantonese to English:\n",
      "佢喺廚房度打得咁𠽤叻𡃈嘞做乜嘢啊？\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742, 26212,\n",
      "          2823, 59569,   592,  4750, 59601,   144,   533,   494,   467,   534,\n",
      "           455,   491,   534,   492,   459, 60044, 59781, 45679,   534,   451,\n",
      "           434,   545,   465,   494,   469,   534,   448,   492,   545,   466,\n",
      "           436,   441,   534,   457,   463, 59842,   533,   490,   461,   534,\n",
      "           457,   467, 60530,   104,     7, 59568,   144,     6, 14135,   144,\n",
      "          4431,   620,   594,   567,  7421,  2249,  1253,   562,  1226,  2123,\n",
      "            99,  2371,   678,   641,  2249,   100,     7]], device='cuda:0')\n",
      "Base model:\n",
      "He is in the kitchen doing such a good job! What are you doing?\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742, 26212,\n",
      "          2823, 59569,   592,  4750, 59601,   144,   533,   494,   467,   534,\n",
      "           455,   491,   534,   492,   459, 60044, 59781, 45679,   534,   451,\n",
      "           434,   545,   465,   494,   469,   534,   448,   492,   545,   466,\n",
      "           436,   441,   534,   457,   463, 59842,   533,   490,   461,   534,\n",
      "           457,   467, 60530,   104,     7, 59568,   144,     6, 14135,   144,\n",
      "          4431, 59610, 59575,   594,   567,  7421,  2249,  1253,   562,  1226,\n",
      "          2123,   639,   616,  1182, 59610, 59570,  1082,   981,   660, 59610,\n",
      "         59575,  2249,    98,     7]], device='cuda:0')\n",
      "Fine-tuned model:\n",
      "He's in the kitchen doing such a good job that I don't know what he's doing.\n",
      "\n",
      "Prompt:\n",
      "Translate the following words from English to Cantonese:\n",
      "What is he doing in the kitchen to make such banging and clanging noise?\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742,  4750,\n",
      "           592, 26212,  2823, 59569, 59601,   144,  5697,   620,   660,  2249,\n",
      "           594,   567,  7421,   592,  1150,  1253,   582,  5940,   597,   845,\n",
      "          5940,  5730,   100,     7, 59568,   144,     6, 14135,   144, 10034,\n",
      "           534,   492,   459, 60044, 61898, 59842, 17103, 60418, 60732, 59676,\n",
      "         37494, 59647, 62203, 59599, 61971, 59911, 59652, 22584, 62203, 60275,\n",
      "           104,     7]], device='cuda:0')\n",
      "Base model:\n",
      "他在廚房裡做什麼會發出那麼大聲的敲打和碰撞聲音？\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742,  4750,\n",
      "           592, 26212,  2823, 59569, 59601,   144,  5697,   620,   660,  2249,\n",
      "           594,   567,  7421,   592,  1150,  1253,   582,  5940,   597,   845,\n",
      "          5940,  5730,   100,     7, 59568,   144,     6, 14135,   144,   533,\n",
      "           494,   467, 59842,   534,   451,   474,   534,   455,   491,   534,\n",
      "           492,   459, 60044,   534,   457,   438, 59781,   534,   451,   434,\n",
      "         59647, 62203,   534,   457,   438, 61385, 61385, 62203,   104,     7]],\n",
      "       device='cuda:0')\n",
      "Fine-tuned model:\n",
      "佢做咩喺廚房嘅度咁大聲嘅撞撞聲？\n",
      "\n",
      "{'en': 'How high do you think the possibility of success for this matter will be?', 'yue': '計你話呢件事嘅成數會有幾高呢？'}\n",
      "Prompt:\n",
      "Translate the following words from Cantonese to English:\n",
      "計你話呢件事嘅成數會有幾高呢？\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742, 26212,\n",
      "          2823, 59569,   592,  4750, 59601,   144, 61392, 59725, 61845, 60329,\n",
      "          9605,   534,   457,   438, 59687, 61344, 53053, 62037, 59719, 60329,\n",
      "           104,     7, 59568,   144,     6, 14135,   144,  7759,  4541,   742,\n",
      "         26212,  2823, 59569,   592,  4750, 59601,   144, 59592,  6546,  1374,\n",
      "           771,   641,  1200,   567,  2918,   631,   719,  2566,   787,   629,\n",
      "           100, 59592,     7]], device='cuda:0')\n",
      "Base model:\n",
      "Translation from Cantonese to English:\n",
      "\"How high do you think the total for this matter will be?\"\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742, 26212,\n",
      "          2823, 59569,   592,  4750, 59601,   144, 61392, 59725, 61845, 60329,\n",
      "          9605,   534,   457,   438, 59687, 61344, 53053, 62037, 59719, 60329,\n",
      "           104,     7, 59568,   144,     6, 14135,   144,  7759,  4541,   742,\n",
      "         26212,  2823, 59569,   592,  4750, 59601,   144,  9888,   641,  1200,\n",
      "           567, 16685,   631,   719,  2291,   678,  1374,   100,     7]],\n",
      "       device='cuda:0')\n",
      "Fine-tuned model:\n",
      "Translation from Cantonese to English:\n",
      "Do you think the odds for this event are high?\n",
      "\n",
      "Prompt:\n",
      "Translate the following words from English to Cantonese:\n",
      "How high do you think the possibility of success for this matter will be?\n",
      "\n",
      "tensor([[    6,  2942,   144,  7759, 14429,   567,  1926,  3151,   742,  4750,\n",
      "           592, 26212,  2823, 59569, 59601,   144,  6546,  1374,   771,   641,\n",
      "          1200,   567,  7975,   593,  2416,   631,   719,  2566,   787,   629,\n",
      "           100,     7, 59568,   144,     6, 14135,   144, 59632, 60565, 43770,\n",
      "         59654,   101, 59725, 18874, 18411, 16345, 11406, 59719,   104,     7]],\n",
      "       device='cuda:0')\n",
      "Base model:\n",
      "在這件事情上，你認為成功的可能性有多高？\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_outputs(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_translate_prompt() missing 1 required positional argument: 'source_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m train_sample \u001b[38;5;241m=\u001b[39m abc_train_set\u001b[38;5;241m.\u001b[39mshuffle(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m      7\u001b[0m test_sample \u001b[38;5;241m=\u001b[39m abc_test_set\u001b[38;5;241m.\u001b[39mshuffle(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m----> 9\u001b[0m en_train_messages \u001b[38;5;241m=\u001b[39m {get_translate_prompt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCantonese\u001b[39m\u001b[38;5;124m'\u001b[39m, sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m train_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m     10\u001b[0m en_test_messages \u001b[38;5;241m=\u001b[39m {get_translate_prompt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCantonese\u001b[39m\u001b[38;5;124m'\u001b[39m, sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m test_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m     11\u001b[0m yue_train_messages \u001b[38;5;241m=\u001b[39m {get_translate_prompt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnglish\u001b[39m\u001b[38;5;124m'\u001b[39m, sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m train_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myue\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n",
      "Cell \u001b[0;32mIn[19], line 9\u001b[0m, in \u001b[0;36m<setcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m train_sample \u001b[38;5;241m=\u001b[39m abc_train_set\u001b[38;5;241m.\u001b[39mshuffle(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m      7\u001b[0m test_sample \u001b[38;5;241m=\u001b[39m abc_test_set\u001b[38;5;241m.\u001b[39mshuffle(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m----> 9\u001b[0m en_train_messages \u001b[38;5;241m=\u001b[39m {\u001b[43mget_translate_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCantonese\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m train_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m     10\u001b[0m en_test_messages \u001b[38;5;241m=\u001b[39m {get_translate_prompt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCantonese\u001b[39m\u001b[38;5;124m'\u001b[39m, sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m test_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m     11\u001b[0m yue_train_messages \u001b[38;5;241m=\u001b[39m {get_translate_prompt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnglish\u001b[39m\u001b[38;5;124m'\u001b[39m, sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m train_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myue\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n",
      "\u001b[0;31mTypeError\u001b[0m: get_translate_prompt() missing 1 required positional argument: 'source_text'"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Translate the following words into English:\\n乜嘢都係波士決定嘅，打工仔啲人淨係得個知字。\\n\"},\n",
    "]\n",
    "\n",
    "# get 5 random samples from train and test dataset\n",
    "train_sample = abc_train_set.shuffle(seed=42).select(range(5))\n",
    "test_sample = abc_test_set.shuffle(seed=42).select(range(5))\n",
    "\n",
    "en_train_messages = {get_translate_prompt('Cantonese', sentence) for sentence in train_sample['en']}\n",
    "en_test_messages = {get_translate_prompt('Cantonese', sentence) for sentence in test_sample['en']}\n",
    "yue_train_messages = {get_translate_prompt('English', sentence) for sentence in train_sample['yue']}\n",
    "yue_test_messages = {get_translate_prompt('English', sentence) for sentence in test_sample['yue']}\n",
    "\n",
    "for messages in [en_train_messages, en_test_messages, yue_train_messages, yue_test_messages]:\n",
    "    for message in messages:\n",
    "        print(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefinedError",
     "evalue": "'str object' has no attribute 'role'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUndefinedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(input_ids\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# response = base_tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True, max_length=100)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1745\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[0;34m(self, conversation, chat_template, add_generation_prompt, tokenize, padding, truncation, max_length, return_tensors, return_dict, **tokenizer_kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# Compilation function uses a cache to avoid recompiling the same template\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m compiled_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compile_jinja_template(chat_template)\n\u001b[0;32m-> 1745\u001b[0m rendered \u001b[38;5;241m=\u001b[39m \u001b[43mcompiled_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconversation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspecial_tokens_map\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m padding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1750\u001b[0m     padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# There's only one sequence here, so \"longest\" makes no sense\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/jinja2/environment.py:1301\u001b[0m, in \u001b[0;36mTemplate.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment\u001b[38;5;241m.\u001b[39mconcat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_render_func(ctx))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m-> 1301\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/trans/lib/python3.10/site-packages/jinja2/environment.py:936\u001b[0m, in \u001b[0;36mEnvironment.handle_exception\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Exception handling helper.  This is used internally to either raise\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;124;03mrewritten exceptions or return a rendered traceback for the template.\u001b[39;00m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rewrite_traceback_stack\n\u001b[0;32m--> 936\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m rewrite_traceback_stack(source\u001b[38;5;241m=\u001b[39msource)\n",
      "File \u001b[0;32m<template>:2\u001b[0m, in \u001b[0;36mtop-level template code\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUndefinedError\u001b[0m: 'str object' has no attribute 'role'"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\n",
    "output_ids = model.generate(input_ids.to('cuda'))\n",
    "# response = base_tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True, max_length=100)\n",
    "response = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True, max_length=100)\n",
    "\n",
    "# Model response: \"Hello! How can I assist you today?\"\n",
    "print(\"Tuned model:\", response)\n",
    "\n",
    "input_ids = base_tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\n",
    "output_ids = base_model.generate(input_ids.to('cuda'))\n",
    "# response = base_tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True, max_length=100)\n",
    "response = base_tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True, max_length=100)\n",
    "\n",
    "print(\"Base model:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
