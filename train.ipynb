{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import nltk.translate.bleu_score as bleu\n",
    "\n",
    "\n",
    "from modelscope import snapshot_download\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': \"It doesn't matter what other people say, as the first thing you should do is to ask yourself if what you did is right or not.\", 'yue': '姑勿論人哋點，諗吓你自己做得啱唔啱先。'}\n",
      "{'en': 'Why does he lack confidence in doing things? Does he now just want to muddle through?', 'yue': '佢點解冇信心做嘢？而家就想打因住波？'}\n",
      "{'en': 'At the beginning of the year I will start a new job.', 'yue': '年頭我會開始新嘅工作。'}\n",
      "{'en': 'You need at least two people to move this dining table to the dining room.', 'yue': '呢張餐枱起碼要兩個人先夠力搬去飯廳度。'}\n",
      "{'en': 'He is really an effeminate young man.', 'yue': '佢真係個女人形㗎。'}\n",
      "{'en': \"When I was in the sixth grade, I was bullied by a kid of mixed race, so in a neighboring class there was a big kid who suddenly ran out and helped me thoroughly bash and bloody the mongrel's head. After that he never bullied me.\", 'yue': '讀小六時我俾條雜種仔㗇，噉隔籬班有個好高大男仔突然走埋嚟幫我打到個雜種仔爆晒缸，以後佢都唔㗇我。'}\n"
     ]
    }
   ],
   "source": [
    "REPO_DIRECTORY = r'/root/'\n",
    "ABC_DICT_PATH = r'autodl-tmp/AIST4010-Cantonese-Translator-Data/ABC-Dict/abc_dict.csv'\n",
    "\n",
    "def load_abc_dataset():\n",
    "    abc_dict = pd.read_csv(REPO_DIRECTORY + ABC_DICT_PATH)\n",
    "    abc_dataset = Dataset.from_pandas(abc_dict)\n",
    "    return abc_dataset\n",
    "\n",
    "abc_set = load_abc_dataset()\n",
    "abc_shuffled_set = abc_set.shuffle(seed=42).train_test_split(test_size=0.1)\n",
    "abc_train_set = abc_shuffled_set['train']\n",
    "abc_test_set = abc_shuffled_set['test']\n",
    "for (i, example) in enumerate(abc_train_set):\n",
    "    print(example)\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[918672 191407]\n",
      "[70.52602487 14.69422693]\n"
     ]
    }
   ],
   "source": [
    "def count_dataset_tokens(dataset):\n",
    "    en_count = 0\n",
    "    yue_count = 0\n",
    "    for example in dataset:\n",
    "        en_count += len(example['en'])\n",
    "        yue_count += len(example['yue'])\n",
    "    return en_count, yue_count\n",
    "\n",
    "\n",
    "counts = np.array(count_dataset_tokens(abc_train_set))\n",
    "print(counts)\n",
    "print(counts/len(abc_train_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=r'/root/autodl-tmp/01ai/Yi-6B-Chat'\n",
    "\n",
    "# model = Model.from_pretrained('01ai/Yi-6B')\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     device_map=\"auto\",\n",
    "#     torch_dtype='auto'\n",
    "# ).eval()\n",
    "\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = snapshot_download('01ai/Yi-6B-Chat', cache_dir='/root/autodl-tmp', revision='master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  3.36it/s]\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "base_tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
    "\n",
    "\n",
    "\n",
    "# Since transformers 4.35.0, the GPT-Q/AWQ model can be loaded using AutoModelForCausalLM.\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype='auto',\n",
    ")\n",
    "\n",
    "\n",
    "# # Prompt content: \"hi\"\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": \"hi\"}\n",
    "# ]\n",
    "\n",
    "\n",
    "# input_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\n",
    "# output_ids = model.generate(input_ids.to('cuda'))\n",
    "# response = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "# # Model response: \"Hello! How can I assist you today?\"\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "作為一個人工智能，我沒有語言能力，因為我沒有感受和思維，只是一個由程式和數據組成的系統。但我可以幫助你解答問題或提供信息，如果你使用英文問話。\n"
     ]
    }
   ],
   "source": [
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": \"你識唔識講廣東話?\"},\n",
    "# ]\n",
    "\n",
    "# input_ids = base_tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\n",
    "# output_ids = base_model.generate(input_ids.to('cuda'))\n",
    "# response = base_tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "# # Model response: \"Hello! How can I assist you today?\"\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    6,  2942,   144, 59725, 62028,   534,   453,   453, 62028, 62364,\n",
      "         62098, 61518, 61845,   100,     7, 59568,   144,     6, 14135,   144]])\n",
      "tensor([[    6,  2942,   144, 59725, 62028,   534,   453,   453, 62028, 62364,\n",
      "         62098, 61518, 61845,   100,     7, 59568,   144,     6, 14135,   144,\n",
      "         26747, 12666, 13992,   101, 59646, 10870, 53202,  2604,   101, 12354,\n",
      "         59646, 10870,  6905, 59652, 60084, 62108,   101,  3331, 12666, 59903,\n",
      "         56241, 59652, 37228, 61728, 11984, 25786,   102, 16097,  1229, 36293,\n",
      "         59725, 21226, 14375, 59876,  2479,  2530,   101,  7953,  2253, 14877,\n",
      "         61420, 61845,   102,     7]], device='cuda:0')\n",
      "<|im_start|> user\n",
      "你識唔識講廣東話?<|im_end|> \n",
      "<|im_start|> assistant\n",
      "\n",
      "<|im_start|> user\n",
      "你識唔識講廣東話?<|im_end|> \n",
      "<|im_start|> assistant\n",
      "\n",
      "['<|im_start|>', 'user', '\\n', '你', '識', '�', '�', '�', '識', '講', '廣', '東', '話', '?', '<|im_end|>', '', '\\n', '<|im_start|>', 'assistant', '\\n', '作為', '一個', '人工智能', '，', '我', '沒有', '語言', '能力', '，', '因為', '我', '沒有', '感受', '和', '思', '維', '，', '只是', '一個', '由', '程式', '和', '數據', '組', '成的', '系統', '。', '但我', '可以', '幫助', '你', '解答', '問題', '或', '提供', '信息', '，', '如果你', '使用', '英文', '問', '話', '。', '<|im_end|>']\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)\n",
    "print(output_ids)\n",
    "print(base_tokenizer.decode(input_ids[0]))\n",
    "print(base_tokenizer.decode(input_ids[0]))\n",
    "\n",
    "#get text of list of tokens in output_ids stored in array\n",
    "print([base_tokenizer.decode([token]) for token in output_ids[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['en'])):\n",
    "        text1 = f\"\"\"\n",
    "        <|im_start|> user\n",
    "        Translate the following words into Cantonese: \n",
    "        {example['en'][i]}\n",
    "        <|im_start|>assistant\n",
    "        {example['yue'][i]}\n",
    "        \"\"\"\n",
    "        text2 = f\"\"\"\n",
    "        <|im_start|> user\n",
    "        Translate the following words into English:\n",
    "        {example['yue'][i]}\n",
    "        <|im_start|>assistant\n",
    "        {example['en'][i]}\n",
    "        \"\"\"\n",
    "        output_texts.append(text1)\n",
    "        output_texts.append(text2)\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into Cantonese: \n",
      "        Scoop up water\n",
      "        <|im_start|>assistant\n",
      "        㧾水\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into English:\n",
      "        㧾水\n",
      "        <|im_start|>assistant\n",
      "        Scoop up water\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into Cantonese: \n",
      "        Ladle out soup\n",
      "        <|im_start|>assistant\n",
      "        㧾湯\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into English:\n",
      "        㧾湯\n",
      "        <|im_start|>assistant\n",
      "        Ladle out soup\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into Cantonese: \n",
      "        Third son of a rich family\n",
      "        <|im_start|>assistant\n",
      "        三少\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into English:\n",
      "        三少\n",
      "        <|im_start|>assistant\n",
      "        Third son of a rich family\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into Cantonese: \n",
      "        Young pigeon or squab that has been roasted\n",
      "        <|im_start|>assistant\n",
      "        乳鴿\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into English:\n",
      "        乳鴿\n",
      "        <|im_start|>assistant\n",
      "        Young pigeon or squab that has been roasted\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into Cantonese: \n",
      "        Husband's second older brother\n",
      "        <|im_start|>assistant\n",
      "        二少\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into English:\n",
      "        二少\n",
      "        <|im_start|>assistant\n",
      "        Husband's second older brother\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into Cantonese: \n",
      "        This time\n",
      "        <|im_start|>assistant\n",
      "        今勻\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into English:\n",
      "        今勻\n",
      "        <|im_start|>assistant\n",
      "        This time\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into Cantonese: \n",
      "        One dollar and sixty cents\n",
      "        <|im_start|>assistant\n",
      "        個六\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into English:\n",
      "        個六\n",
      "        <|im_start|>assistant\n",
      "        One dollar and sixty cents\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into Cantonese: \n",
      "        Rabbit\n",
      "        <|im_start|>assistant\n",
      "        兔仔\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into English:\n",
      "        兔仔\n",
      "        <|im_start|>assistant\n",
      "        Rabbit\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into Cantonese: \n",
      "        Exit through turnstile\n",
      "        <|im_start|>assistant\n",
      "        出閘\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into English:\n",
      "        出閘\n",
      "        <|im_start|>assistant\n",
      "        Exit through turnstile\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into Cantonese: \n",
      "        Chop something into cubes\n",
      "        <|im_start|>assistant\n",
      "        切粒\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into English:\n",
      "        切粒\n",
      "        <|im_start|>assistant\n",
      "        Chop something into cubes\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "prompts = formatting_prompts_func(abc_set[:10])\n",
    "for prompt in prompts:\n",
    "    print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in base_model.named_parameters():\n",
    "#     print(f\"Parameter name: {name}\")\n",
    "#     print(param)\n",
    "#     print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"/root/autodl-tmp/01ai/Yi-6B-Chat\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 5000000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(base_model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 17,825,792 || all params: 6,078,861,312 || trainable%: 0.293242288071467\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=32, # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules = [\"k_proj\", \"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "peft_model = get_peft_model(base_model, \n",
    "                            lora_config)\n",
    "\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('tokenizer/tokenizer_config.json',\n",
       " 'tokenizer/special_tokens_map.json',\n",
       " 'tokenizer/tokenizer.model',\n",
       " 'tokenizer/added_tokens.json',\n",
       " 'tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_training_corpus(dataset):\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        sample_en = samples[\"en\"]\n",
    "        sample_yue = samples[\"yue\"]\n",
    "        for i in range(len(sample_en)):\n",
    "            yield sample_en[i]\n",
    "            yield sample_yue[i]\n",
    "\n",
    "training_corpus = get_training_corpus(abc_train_set)\n",
    "\n",
    "tokenizer = base_tokenizer.train_new_from_iterator(training_corpus, vocab_size=40000)\n",
    "tokenizer.save_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [89, 633, 504, 1928, 3992, 668], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [59568, 534, 456, 445, 534, 450, 436, 536, 454, 433, 534, 454, 483, 534, 457, 467, 534, 458, 436], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [25778, 5554], 'attention_mask': [1, 1]}\n",
      "{'input_ids': [6076, 4040], 'attention_mask': [1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(\"嗌呃畀啲嘢噃\"))\n",
    "print(base_tokenizer(\"嗌呃畀啲嘢噃\"))\n",
    "print(tokenizer(\"Good morning\"))\n",
    "print(base_tokenizer(\"Good morning\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bleu = evaluate.load('bleu')\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    print(predictions.shape, labels.shape)\n",
    "    return {\"bleu\": bleu(predictions, labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() missing 1 required positional argument: 'output_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Higher learning rate than full fine-tuning.\u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[1;32m      8\u001b[0m     peft_model,\n\u001b[1;32m      9\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[0;31mTypeError\u001b[0m: TrainingArguments.__init__() missing 1 required positional argument: 'output_dir'"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=100,\n",
    "    output_dir=\"root/peft_model\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset= abc_train_set,\n",
    "    eval_dataset= abc_test_set,\n",
    "    tokenizer=tokenizer,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(\"root/peft_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "translate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
