{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import nltk.translate.bleu_score as bleu\n",
    "\n",
    "\n",
    "from modelscope import snapshot_download\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_DIRECTORY = r'/root/'\n",
    "ABC_DICT_PATH = r'autodl-tmp/AIST4010-Cantonese-Translator-Data/ABC-Dict/abc_dict.csv'\n",
    "\n",
    "def load_abc_dataset():\n",
    "    abc_dict = pd.read_csv(REPO_DIRECTORY + ABC_DICT_PATH)\n",
    "    abc_dataset = Dataset.from_pandas(abc_dict)\n",
    "    return abc_dataset\n",
    "\n",
    "abc_set = load_abc_dataset()\n",
    "abc_shuffled_set = abc_set.shuffle(seed=42).train_test_split(test_size=0.1)\n",
    "abc_train_set = abc_shuffled_set['train']\n",
    "abc_test_set = abc_shuffled_set['test']\n",
    "for (i, example) in enumerate(abc_train_set):\n",
    "    print(example)\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_dataset_tokens(dataset):\n",
    "    en_count = 0\n",
    "    yue_count = 0\n",
    "    for example in dataset:\n",
    "        en_count += len(example['en'])\n",
    "        yue_count += len(example['yue'])\n",
    "    return en_count, yue_count\n",
    "\n",
    "\n",
    "counts = np.array(count_dataset_tokens(abc_train_set))\n",
    "print(counts)\n",
    "print(counts/len(abc_train_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=r'/root/autodl-tmp/01ai/Yi-6B-Chat'\n",
    "\n",
    "# model = Model.from_pretrained('01ai/Yi-6B')\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     device_map=\"auto\",\n",
    "#     torch_dtype='auto'\n",
    "# ).eval()\n",
    "\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = snapshot_download('01ai/Yi-6B-Chat', cache_dir='/root/autodl-tmp', revision='master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True, padding_side='left', max_length=512, return_tensors='pt')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "\n",
    "# Since transformers 4.35.0, the GPT-Q/AWQ model can be loaded using AutoModelForCausalLM.\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=device,\n",
    "    torch_dtype='auto',\n",
    ")\n",
    "\n",
    "\n",
    "# # Prompt content: \"hi\"\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": \"hi\"}\n",
    "# ]\n",
    "\n",
    "\n",
    "# input_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\n",
    "# output_ids = model.generate(input_ids.to('cuda'))\n",
    "# response = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "# # Model response: \"Hello! How can I assist you today?\"\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"hi\"},\n",
    "]\n",
    "\n",
    "input_ids = base_tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\n",
    "output_ids = base_model.generate(input_ids.to('cuda'))\n",
    "response = base_tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True, max_length=100)\n",
    "\n",
    "# Model response: \"Hello! How can I assist you today?\"\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(input_ids)\n",
    "# print(output_ids)\n",
    "# print(base_tokenizer.decode(input_ids[0]))\n",
    "# print(base_tokenizer.decode(input_ids[0]))\n",
    "\n",
    "# #get text of list of tokens in output_ids stored in array\n",
    "# print([base_tokenizer.decode([token]) for token in output_ids[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['en'])):\n",
    "        text1 = f\"\"\"\n",
    "        <|im_start|> user\n",
    "        Translate the following words into Cantonese: \n",
    "        {example['en'][i]}\n",
    "        <|im_start|>assistant\n",
    "        {example['yue'][i]}\n",
    "        \"\"\"\n",
    "        text2 = f\"\"\"\n",
    "        <|im_start|> user\n",
    "        Translate the following words into English:\n",
    "        {example['yue'][i]}\n",
    "        <|im_start|>assistant\n",
    "        {example['en'][i]}\n",
    "        \"\"\"\n",
    "        output_texts.append(text1)\n",
    "        output_texts.append(text2)\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = formatting_prompts_func(abc_set[:10])\n",
    "for prompt in prompts:\n",
    "    print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in base_model.named_parameters():\n",
    "#     print(f\"Parameter name: {name}\")\n",
    "#     print(param)\n",
    "#     print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(base_model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=32, # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules = [\"k_proj\", \"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "peft_model = get_peft_model(base_model, \n",
    "                            lora_config)\n",
    "\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus(dataset):\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        sample_en = samples[\"en\"]\n",
    "        sample_yue = samples[\"yue\"]\n",
    "        for i in range(len(sample_en)):\n",
    "            yield sample_en[i]\n",
    "            yield sample_yue[i]\n",
    "\n",
    "training_corpus = get_training_corpus(abc_train_set)\n",
    "\n",
    "tokenizer = base_tokenizer.train_new_from_iterator(training_corpus, vocab_size=40000)\n",
    "tokenizer.save_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer(\"嗌呃畀啲嘢噃\"))\n",
    "print(base_tokenizer(\"嗌呃畀啲嘢噃\"))\n",
    "print(tokenizer(\"Good morning\"))\n",
    "print(base_tokenizer(\"Good morning\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bleu = evaluate.load('bleu')\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    print(predictions.shape, labels.shape)\n",
    "    return {\"bleu\": bleu(predictions, labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TrainingArguments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m(\n\u001b[1;32m      2\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, \u001b[38;5;66;03m# Higher learning rate than full fine-tuning.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      4\u001b[0m     logging_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m      5\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot/peft_model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[1;32m      9\u001b[0m     peft_model,\n\u001b[1;32m     10\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TrainingArguments' is not defined"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=100,\n",
    "    output_dir=\"root/peft_model\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset= abc_train_set,\n",
    "    eval_dataset= abc_test_set,\n",
    "    tokenizer=tokenizer,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(\"/root/peft_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get random data from test dataset\n",
    "for i in range(5):\n",
    "    example = abc_test_set[i]\n",
    "    print(example)\n",
    "    text1 = f\"\"\"Translate the following words into Cantonese: \n",
    "        {example['en']}\n",
    "        \"\"\"\n",
    "    text2 = f\"\"\"Translate the following words into English:\n",
    "        {example['yue']}\n",
    "        \"\"\"\n",
    "    texts = [text1, text2]\n",
    "    for text in texts:\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ]\n",
    "        print(messages)\n",
    "        #print model outputs for base_model and peft_model\n",
    "        base_input_ids = base_tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\n",
    "        peft_input_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\n",
    "        print(\"Base ID:\", base_input_ids)\n",
    "        print(\"Base Input:\", base_tokenizer.decode(base_input_ids[base_input_ids.shape[1]:], skip_special_tokens=True))\n",
    "        print(\"PEFT ID:\", peft_input_ids)\n",
    "        print(\"PEFT Input:\", tokenizer.decode(peft_input_ids[peft_input_ids.shape[1]:], skip_special_tokens=True))\n",
    "        print(peft_input_ids)\n",
    "        base_output_ids = base_model.generate(base_input_ids.to('cuda'), max_new_tokens=100)\n",
    "        peft_output_ids = peft_model.generate(peft_input_ids.to('cuda'), max_new_tokens=100)\n",
    "        print(base_output_ids.shape, peft_output_ids.shape)\n",
    "        print(\"Base model: \", base_tokenizer.decode(base_output_ids[0][base_input_ids.shape[1]:], skip_special_tokens=True))\n",
    "        print(\"Fine-tuned: \", tokenizer.decode(peft_output_ids[0][peft_input_ids.shape[1]:], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
