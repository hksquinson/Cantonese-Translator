{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-13 22:47:03,349 - modelscope - INFO - PyTorch version 2.0.1 Found.\n",
      "2024-03-13 22:47:03,351 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
      "2024-03-13 22:47:03,512 - modelscope - INFO - Loading done! Current index file version is 1.13.1, with md5 ce2a1413d67bf5615c2e26752330cf67 and a total number of 972 components indexed\n",
      "/root/miniconda3/envs/translate/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-13 22:47:05,047] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "from modelscope import snapshot_download\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import SFTTrainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': ['Scoop up water', 'Ladle out soup', 'Third son of a rich family', 'Young pigeon or squab that has been roasted', \"Husband's second older brother\", 'This time', 'One dollar and sixty cents', 'Rabbit', 'Exit through turnstile', 'Chop something into cubes'], 'yue': ['㧾水', '㧾湯', '三少', '乳鴿', '二少', '今勻', '個六', '兔仔', '出閘', '切粒']}\n"
     ]
    }
   ],
   "source": [
    "REPO_DIRECTORY = r'/root/'\n",
    "ABC_DICT_PATH = r'/autodl-tmp/AIST4010-Cantonese-Translator-Data/ABC-Dict/abc_dict.csv'\n",
    "\n",
    "def load_abc_dataset():\n",
    "    abc_dict = pd.read_csv(REPO_DIRECTORY + ABC_DICT_PATH)\n",
    "    abc_dataset = Dataset.from_pandas(abc_dict)\n",
    "    return abc_dataset\n",
    "\n",
    "abc_dict_dataset = load_abc_dataset()\n",
    "print(abc_dict_dataset[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=r'/root/autodl-tmp/01ai/Yi-6B-Chat-4bits'\n",
    "\n",
    "# model = Model.from_pretrained('01ai/Yi-6B')\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     device_map=\"auto\",\n",
    "#     torch_dtype='auto'\n",
    "# ).eval()\n",
    "\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from modelscope import snapshot_download, AutoModel, AutoTokenizer\n",
    "import os\n",
    "model_dir = snapshot_download('01ai/Yi-6B-Chat-4bits', cache_dir='/root/autodl-tmp', revision='master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We suggest you to set `torch_dtype=torch.float16` for better efficiency with AWQ.\n"
     ]
    }
   ],
   "source": [
    "base_tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "\n",
    "\n",
    "\n",
    "# Since transformers 4.35.0, the GPT-Q/AWQ model can be loaded using AutoModelForCausalLM.\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype='auto',\n",
    ").eval()\n",
    "\n",
    "# # Prompt content: \"hi\"\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": \"hi\"}\n",
    "# ]\n",
    "\n",
    "\n",
    "# input_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\n",
    "# output_ids = model.generate(input_ids.to('cuda'))\n",
    "# response = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "# # Model response: \"Hello! How can I assist you today?\"\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"hi\"}\n",
    "]\n",
    "\n",
    "input_ids = base_tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\n",
    "output_ids = base_model.generate(input_ids.to('cuda'))\n",
    "response = base_tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "# Model response: \"Hello! How can I assist you today?\"\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    6,  3903,   144,  7637,     7,   144,     6,   765, 13611,   144]])\n",
      "tensor([[    6,  3903,   144,  7637,     7,   144,     6,   765, 13611,   144,\n",
      "         25102,    99,  1742,   748,   616,  4366,   641,  2272,   100,     7]],\n",
      "       device='cuda:0')\n",
      "<|im_start|> user\n",
      "hi<|im_end|> \n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)\n",
    "print(output_ids)\n",
    "print(base_tokenizer.decode(input_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['en'])):\n",
    "        text1 = f\"\"\"\n",
    "        <|im_start|> user\n",
    "        Translate the following words into Cantonese: \n",
    "        {example['en'][i]}\n",
    "        <|im_start|>assistant\n",
    "        {example['yue'][i]}\n",
    "        \"\"\"\n",
    "        text2 = f\"\"\"\n",
    "        <|im_start|> user\n",
    "        Translate the following words into English:\n",
    "        {example['yue'][i]}\n",
    "        <|im_start|>assistant\n",
    "        {example['en'][i]}\n",
    "        \"\"\"\n",
    "        output_texts.append(text1)\n",
    "        output_texts.append(text2)\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into Cantonese: \n",
      "        Scoop up water\n",
      "        <|im_start|>assistant\n",
      "        㧾水\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into English:\n",
      "        㧾水\n",
      "        <|im_start|>assistant\n",
      "        Scoop up water\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into Cantonese: \n",
      "        Ladle out soup\n",
      "        <|im_start|>assistant\n",
      "        㧾湯\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into English:\n",
      "        㧾湯\n",
      "        <|im_start|>assistant\n",
      "        Ladle out soup\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into Cantonese: \n",
      "        Third son of a rich family\n",
      "        <|im_start|>assistant\n",
      "        三少\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into English:\n",
      "        三少\n",
      "        <|im_start|>assistant\n",
      "        Third son of a rich family\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into Cantonese: \n",
      "        Young pigeon or squab that has been roasted\n",
      "        <|im_start|>assistant\n",
      "        乳鴿\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into English:\n",
      "        乳鴿\n",
      "        <|im_start|>assistant\n",
      "        Young pigeon or squab that has been roasted\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into Cantonese: \n",
      "        Husband's second older brother\n",
      "        <|im_start|>assistant\n",
      "        二少\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into English:\n",
      "        二少\n",
      "        <|im_start|>assistant\n",
      "        Husband's second older brother\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into Cantonese: \n",
      "        This time\n",
      "        <|im_start|>assistant\n",
      "        今勻\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into English:\n",
      "        今勻\n",
      "        <|im_start|>assistant\n",
      "        This time\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into Cantonese: \n",
      "        One dollar and sixty cents\n",
      "        <|im_start|>assistant\n",
      "        個六\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into English:\n",
      "        個六\n",
      "        <|im_start|>assistant\n",
      "        One dollar and sixty cents\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into Cantonese: \n",
      "        Rabbit\n",
      "        <|im_start|>assistant\n",
      "        兔仔\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into English:\n",
      "        兔仔\n",
      "        <|im_start|>assistant\n",
      "        Rabbit\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into Cantonese: \n",
      "        Exit through turnstile\n",
      "        <|im_start|>assistant\n",
      "        出閘\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into English:\n",
      "        出閘\n",
      "        <|im_start|>assistant\n",
      "        Exit through turnstile\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into Cantonese: \n",
      "        Chop something into cubes\n",
      "        <|im_start|>assistant\n",
      "        切粒\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into English:\n",
      "        切粒\n",
      "        <|im_start|>assistant\n",
      "        Chop something into cubes\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "prompts = formatting_prompts_func(abc_dict_dataset[:10])\n",
    "for prompt in prompts:\n",
    "    print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: model.embed_tokens.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0000e+00, -5.9605e-08,  0.0000e+00,  ..., -5.9605e-08,\n",
      "          5.9605e-08, -5.9605e-08],\n",
      "        [-0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "         -5.9605e-08, -5.9605e-08],\n",
      "        [ 4.3945e-03,  3.1853e-04,  4.3030e-03,  ...,  3.3875e-03,\n",
      "          5.4550e-04, -1.2451e-02],\n",
      "        ...,\n",
      "        [ 2.7100e-02,  1.6724e-02, -3.3447e-02,  ...,  2.8687e-03,\n",
      "          1.2756e-02,  1.6602e-02],\n",
      "        [-2.4048e-02, -2.3560e-02,  1.3977e-02,  ..., -3.7689e-03,\n",
      "          2.5635e-02,  5.3406e-03],\n",
      "        [ 1.5869e-02,  1.3550e-02,  3.9062e-02,  ...,  3.1006e-02,\n",
      "         -7.5378e-03, -5.8899e-03]], device='cuda:0', dtype=torch.float16,\n",
      "       requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.0.input_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([ 0.0046,  0.0053, -0.0004,  ...,  0.0048,  0.0043,  0.0042],\n",
      "       device='cuda:0', dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.0.post_attention_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([0.1239, 0.1270, 0.1293,  ..., 0.1227, 0.1274, 0.1218], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.1.input_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([0.0120, 0.0112, 0.0143,  ..., 0.0033, 0.0092, 0.0144], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.1.post_attention_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([0.1874, 0.1877, 0.1754,  ..., 0.1750, 0.1873, 0.1777], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.2.input_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([0.1483, 0.1622, 0.2991,  ..., 0.1445, 0.1383, 0.2272], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.2.post_attention_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([0.2394, 0.2362, 0.2118,  ..., 0.2365, 0.2440, 0.2025], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.3.input_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([0.2206, 0.1899, 0.2539,  ..., 0.1876, 0.1725, 0.2646], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.3.post_attention_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([0.2137, 0.2178, 0.1959,  ..., 0.2087, 0.2117, 0.1884], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.4.input_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([0.7954, 0.8032, 0.9434,  ..., 0.7305, 0.7368, 0.9287], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.4.post_attention_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([0.3491, 0.3569, 0.3323,  ..., 0.3540, 0.3521, 0.3235], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.5.input_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([0.7944, 0.8027, 0.8779,  ..., 0.8291, 0.7905, 0.8940], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.5.post_attention_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([0.5010, 0.4885, 0.4602,  ..., 0.4880, 0.4814, 0.4712], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.6.input_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([0.8813, 0.9502, 1.0088,  ..., 0.9155, 0.9058, 1.0537], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.6.post_attention_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([0.5137, 0.5161, 0.4885,  ..., 0.5059, 0.5073, 0.4827], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.7.input_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([0.8188, 0.9048, 0.8159,  ..., 0.8047, 0.8120, 0.8384], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.7.post_attention_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([0.5459, 0.5532, 0.5005,  ..., 0.5322, 0.5483, 0.5122], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.8.input_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([0.8491, 0.8857, 0.8496,  ..., 0.8828, 0.8481, 0.8594], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.8.post_attention_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([0.6406, 0.6484, 0.5859,  ..., 0.6055, 0.6411, 0.5991], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.9.input_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([0.9932, 1.0498, 0.9565,  ..., 0.9849, 1.0303, 0.9951], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.9.post_attention_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([0.6455, 0.6489, 0.6011,  ..., 0.6113, 0.6514, 0.6099], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.10.input_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([1.0488, 1.0713, 1.0469,  ..., 1.0996, 1.1113, 1.1064], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.10.post_attention_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([0.7476, 0.7563, 0.6875,  ..., 0.6948, 0.7378, 0.7036], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.11.input_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([1.4707, 1.4258, 1.3242,  ..., 1.3945, 1.5137, 1.4668], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.11.post_attention_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([0.6431, 0.6499, 0.5933,  ..., 0.5962, 0.6406, 0.6011], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.12.input_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([1.2617, 1.2920, 1.1709,  ..., 1.2646, 1.3174, 1.2910], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.12.post_attention_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([0.8169, 0.8403, 0.7466,  ..., 0.7666, 0.8345, 0.7852], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.13.input_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([1.6436, 1.6230, 1.4541,  ..., 1.5137, 1.7656, 1.6230], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.13.post_attention_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([0.8477, 0.8604, 0.7520,  ..., 0.7891, 0.8452, 0.8096], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.14.input_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([1.6543, 1.7490, 1.4570,  ..., 1.6270, 1.8574, 1.6680], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.14.post_attention_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([0.8579, 0.8823, 0.7563,  ..., 0.8101, 0.8638, 0.8379], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.15.input_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([1.4658, 1.5020, 1.2402,  ..., 1.3242, 1.5654, 1.4277], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.15.post_attention_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([0.9009, 0.9272, 0.7974,  ..., 0.8467, 0.9111, 0.8726], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.16.input_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([1.4863, 1.5146, 1.2939,  ..., 1.4053, 1.5742, 1.4766], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.16.post_attention_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([0.9160, 0.9448, 0.8306,  ..., 0.8760, 0.9463, 0.9155], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.17.input_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([1.7227, 1.7959, 1.4854,  ..., 1.7041, 1.8096, 1.7061], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.17.post_attention_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([0.9307, 0.9448, 0.8306,  ..., 0.8774, 0.9434, 0.9180], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.18.input_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([1.6357, 1.6934, 1.4658,  ..., 1.5967, 1.7227, 1.6221], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.18.post_attention_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([0.9517, 0.9678, 0.8433,  ..., 0.8682, 0.9438, 0.9282], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.19.input_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([1.5342, 1.6729, 1.4463,  ..., 1.5264, 1.6240, 1.5693], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.19.post_attention_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([0.9424, 0.9795, 0.8667,  ..., 0.8857, 0.9458, 0.9473], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.20.input_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([1.6064, 1.7402, 1.5322,  ..., 1.6807, 1.7080, 1.7139], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.20.post_attention_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([1.0020, 1.0186, 0.9214,  ..., 0.9312, 0.9829, 1.0146], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.21.input_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([1.5830, 1.6621, 1.5498,  ..., 1.5957, 1.6504, 1.5986], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.21.post_attention_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([1.1035, 1.1104, 1.0586,  ..., 1.0400, 1.0859, 1.1182], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.22.input_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([1.5293, 1.5830, 1.5830,  ..., 1.5518, 1.6182, 1.5498], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.22.post_attention_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([1.1738, 1.2031, 1.1514,  ..., 1.1074, 1.1748, 1.1797], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.23.input_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([1.6973, 1.7510, 1.7734,  ..., 1.6963, 1.8066, 1.6973], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.23.post_attention_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([1.2441, 1.2754, 1.2344,  ..., 1.1973, 1.2715, 1.2695], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.24.input_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([1.5146, 1.5381, 1.6230,  ..., 1.6133, 1.6748, 1.5088], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.24.post_attention_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([1.3096, 1.3359, 1.2842,  ..., 1.2607, 1.3301, 1.3223], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.25.input_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([1.9141, 1.9814, 1.9883,  ..., 1.9121, 2.0391, 1.8535], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.25.post_attention_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([1.2480, 1.2666, 1.2129,  ..., 1.2100, 1.2490, 1.2490], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.26.input_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([1.7334, 1.8613, 1.8389,  ..., 1.8447, 1.8242, 1.7051], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.26.post_attention_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([1.3350, 1.3516, 1.3203,  ..., 1.2900, 1.3408, 1.3447], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.27.input_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([1.8535, 1.8711, 1.9375,  ..., 1.9043, 1.9551, 1.7529], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.27.post_attention_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([1.3047, 1.2930, 1.2695,  ..., 1.2480, 1.2910, 1.2852], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.28.input_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([2.1777, 2.2852, 2.3340,  ..., 2.2695, 2.2832, 2.1719], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.28.post_attention_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([1.4160, 1.4316, 1.4229,  ..., 1.3984, 1.4268, 1.4248], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.29.input_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([1.4922, 1.4834, 1.5840,  ..., 1.5703, 1.5957, 1.4570], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.29.post_attention_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([1.8096, 1.8154, 1.8047,  ..., 1.7881, 1.8418, 1.8066], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.30.input_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([1.8496, 1.9053, 1.8740,  ..., 1.9893, 1.9424, 1.8887], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.30.post_attention_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([2.2012, 2.2168, 2.1660,  ..., 2.1738, 2.1836, 2.1953], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.31.input_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([2.4004, 2.3496, 2.4707,  ..., 2.4453, 2.4023, 2.3418], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.layers.31.post_attention_layernorm.weight\n",
      "Parameter containing:\n",
      "tensor([5.6914, 5.7188, 5.4258,  ..., 5.1680, 5.6992, 5.4023], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: model.norm.weight\n",
      "Parameter containing:\n",
      "tensor([3.3594, 3.3438, 3.2969,  ..., 3.4375, 3.2656, 3.4219], device='cuda:0',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Parameter name: lm_head.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0052, -0.0020,  0.0099,  ..., -0.0083, -0.0018, -0.0034],\n",
      "        [ 0.0052, -0.0020,  0.0099,  ..., -0.0083, -0.0018, -0.0034],\n",
      "        [ 0.0123, -0.0173, -0.0032,  ...,  0.0142,  0.0195, -0.0041],\n",
      "        ...,\n",
      "        [ 0.0026,  0.0002, -0.0107,  ...,  0.0101, -0.0091,  0.0077],\n",
      "        [-0.0073, -0.0123,  0.0148,  ..., -0.0292, -0.0294, -0.0123],\n",
      "        [-0.0143, -0.0117,  0.0237,  ...,  0.0175, -0.0164,  0.0179]],\n",
      "       device='cuda:0', dtype=torch.float16, requires_grad=True)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for name, param in base_model.named_parameters():\n",
    "    print(f\"Parameter name: {name}\")\n",
    "    print(param)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"/root/autodl-tmp/01ai/Yi-6B-Chat-4bits\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"backend\": \"autoawq\",\n",
      "    \"bits\": 4,\n",
      "    \"do_fuse\": false,\n",
      "    \"fuse_max_seq_len\": null,\n",
      "    \"group_size\": 128,\n",
      "    \"modules_to_fuse\": null,\n",
      "    \"modules_to_not_convert\": null,\n",
      "    \"quant_method\": \"awq\",\n",
      "    \"version\": \"gemm\",\n",
      "    \"zero_point\": true\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 5000000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "adapter model file not found in /root/autodl-tmp/01ai/Yi-6B-Chat-4bits. Make sure you are passing the correct path to the adapter model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(base_model\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/translate/lib/python3.10/site-packages/transformers/integrations/peft.py:175\u001b[0m, in \u001b[0;36mPeftAdapterMixin.load_adapter\u001b[0;34m(self, peft_model_id, adapter_name, revision, token, device_map, max_memory, offload_folder, offload_index, peft_config, adapter_state_dict, adapter_kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     adapter_config_file \u001b[38;5;241m=\u001b[39m find_adapter_config_file(\n\u001b[1;32m    169\u001b[0m         peft_model_id,\n\u001b[1;32m    170\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39madapter_kwargs,\n\u001b[1;32m    172\u001b[0m     )\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m adapter_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    176\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madapter model file not found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeft_model_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Make sure you are passing the correct path to the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    177\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madapter model.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    178\u001b[0m         )\n\u001b[1;32m    180\u001b[0m     peft_config \u001b[38;5;241m=\u001b[39m PeftConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    181\u001b[0m         peft_model_id,\n\u001b[1;32m    182\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39madapter_kwargs,\n\u001b[1;32m    184\u001b[0m     )\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# Create and add fresh new adapters into the model.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: adapter model file not found in /root/autodl-tmp/01ai/Yi-6B-Chat-4bits. Make sure you are passing the correct path to the adapter model."
     ]
    }
   ],
   "source": [
    "print(base_model.config)\n",
    "base_model.load_adapter(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target module WQLinear_GEMM(in_features=4096, out_features=4096, bias=False, w_bit=4, group_size=128) is not supported. Currently, only `torch.nn.Linear` and `Conv1D` are supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m lora_config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[1;32m      2\u001b[0m     r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, \u001b[38;5;66;03m# Rank\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     task_type\u001b[38;5;241m=\u001b[39mTaskType\u001b[38;5;241m.\u001b[39mSEQ_2_SEQ_LM \u001b[38;5;66;03m# FLAN-T5\u001b[39;00m\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m peft_model \u001b[38;5;241m=\u001b[39m \u001b[43mget_peft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/translate/lib/python3.10/site-packages/peft/mapping.py:98\u001b[0m, in \u001b[0;36mget_peft_model\u001b[0;34m(model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(peft_config, PromptLearningConfig):\n\u001b[1;32m     97\u001b[0m     peft_config \u001b[38;5;241m=\u001b[39m _prepare_prompt_learning_config(peft_config, model_config)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/translate/lib/python3.10/site-packages/peft/peft_model.py:1058\u001b[0m, in \u001b[0;36mPeftModelForSeq2SeqLM.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, peft_config: PeftConfig, adapter_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1058\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1059\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_inputs_for_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation\n\u001b[1;32m   1060\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_encoder_decoder_kwargs_for_generation \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1061\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39m_prepare_encoder_decoder_kwargs_for_generation\n\u001b[1;32m   1062\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/translate/lib/python3.10/site-packages/peft/peft_model.py:112\u001b[0m, in \u001b[0;36mPeftModel.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(peft_config, PromptLearningConfig):\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name] \u001b[38;5;241m=\u001b[39m peft_config\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[43mPEFT_TYPE_TO_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpeft_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_additional_trainable_modules(peft_config, adapter_name)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/translate/lib/python3.10/site-packages/peft/tuners/lora.py:180\u001b[0m, in \u001b[0;36mLoraModel.__init__\u001b[0;34m(self, model, config, adapter_name)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpeft_config\u001b[49m\u001b[43m[\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# transformers models have a .config attribute, whose presence is assumed later on\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/translate/lib/python3.10/site-packages/peft/tuners/lora.py:194\u001b[0m, in \u001b[0;36mLoraModel.add_adapter\u001b[0;34m(self, adapter_name, config)\u001b[0m\n\u001b[1;32m    192\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_lora_config(config, model_config)\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name] \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m--> 194\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_and_replace\u001b[49m\u001b[43m(\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name]\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoraModel supports only 1 adapter with bias. When using multiple adapters, set bias to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for all adapters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/translate/lib/python3.10/site-packages/peft/tuners/lora.py:352\u001b[0m, in \u001b[0;36mLoraModel._find_and_replace\u001b[0;34m(self, adapter_name)\u001b[0m\n\u001b[1;32m    344\u001b[0m         target\u001b[38;5;241m.\u001b[39mupdate_layer(\n\u001b[1;32m    345\u001b[0m             adapter_name,\n\u001b[1;32m    346\u001b[0m             lora_config\u001b[38;5;241m.\u001b[39mr,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    349\u001b[0m             lora_config\u001b[38;5;241m.\u001b[39minit_lora_weights,\n\u001b[1;32m    350\u001b[0m         )\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 352\u001b[0m         new_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replace_module(parent, target_name, new_module, target)\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_target_modules_in_base_model:\n",
      "File \u001b[0;32m~/miniconda3/envs/translate/lib/python3.10/site-packages/peft/tuners/lora.py:305\u001b[0m, in \u001b[0;36mLoraModel._create_new_module\u001b[0;34m(self, lora_config, adapter_name, target)\u001b[0m\n\u001b[1;32m    303\u001b[0m             kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfan_in_fan_out\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m lora_config\u001b[38;5;241m.\u001b[39mfan_in_fan_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 305\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    306\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget module \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently, only `torch.nn.Linear` and `Conv1D` are supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m         )\n\u001b[1;32m    309\u001b[0m     new_module \u001b[38;5;241m=\u001b[39m Linear(adapter_name, in_features, out_features, bias\u001b[38;5;241m=\u001b[39mbias, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_module\n",
      "\u001b[0;31mValueError\u001b[0m: Target module WQLinear_GEMM(in_features=4096, out_features=4096, bias=False, w_bit=4, group_size=128) is not supported. Currently, only `torch.nn.Linear` and `Conv1D` are supported."
     ]
    }
   ],
   "source": [
    "# lora_config = LoraConfig(\n",
    "#     r=32, # Rank\n",
    "#     lora_alpha=32,\n",
    "#     target_modules = [\"k_proj\", \"q_proj\", \"v_proj\"],\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\",\n",
    "#     task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
    "# )\n",
    "# peft_model = get_peft_model(base_model, \n",
    "#                             lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/translate/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:225: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/translate/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You cannot perform fine-tuning on purely quantized models. Please attach trainable adapters on top of the quantized model to correctly perform fine-tuning. Please see: https://huggingface.co/docs/transformers/peft for more details",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mabc_dict_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mformatting_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatting_prompts_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/translate/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:299\u001b[0m, in \u001b[0;36mSFTTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, dataset_text_field, packing, formatting_func, max_seq_length, infinite, num_of_sequences, chars_per_token, dataset_num_proc, dataset_batch_size, neftune_noise_alpha, model_init_kwargs, dataset_kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    294\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    295\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` to your code.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 299\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# Add tags for models that have been loaded with the correct transformers version\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd_model_tags\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/translate/lib/python3.10/site-packages/transformers/trainer.py:440\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;66;03m# At this stage the model is already loaded\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_quantized_and_base_model \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_peft_model(model):\n\u001b[0;32m--> 440\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    441\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot perform fine-tuning on purely quantized models. Please attach trainable adapters on top of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m the quantized model to correctly perform fine-tuning. Please see: https://huggingface.co/docs/transformers/peft\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for more details\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    444\u001b[0m     )\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _is_quantized_and_base_model \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _quantization_method_supports_training:\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model you are trying to fine-tune is quantized with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mhf_quantizer\u001b[38;5;241m.\u001b[39mquantization_config\u001b[38;5;241m.\u001b[39mquant_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but that quantization method do not support training. Please open an issue on GitHub: https://github.com/huggingface/transformers\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to request the support for training support for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mhf_quantizer\u001b[38;5;241m.\u001b[39mquantization_config\u001b[38;5;241m.\u001b[39mquant_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    450\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: You cannot perform fine-tuning on purely quantized models. Please attach trainable adapters on top of the quantized model to correctly perform fine-tuning. Please see: https://huggingface.co/docs/transformers/peft for more details"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    base_model,\n",
    "    train_dataset= abc_dict_dataset,\n",
    "    formatting_func=formatting_prompts_func,\n",
    ")\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "translate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
