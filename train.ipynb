{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-14 10:23:16,056 - modelscope - INFO - PyTorch version 2.2.1 Found.\n",
      "2024-03-14 10:23:16,058 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
      "2024-03-14 10:23:16,157 - modelscope - INFO - Loading done! Current index file version is 1.13.1, with md5 b5a2c5fe01f7460b3e700a8ce7e6fc94 and a total number of 972 components indexed\n",
      "/root/miniconda3/envs/trans/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import nltk.translate.bleu_score as bleu\n",
    "\n",
    "\n",
    "from modelscope import snapshot_download\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': 'You know what you have done, so tomorrow submit your resignation letter to me.', 'yue': '你自己知自己事，聽日遞封辭職信畀我。'}\n",
      "{'en': \"That one that I patronized was a young prostitute. Her skills were ample. She didn't loaf around one bit.\", 'yue': '我幫襯咗嗰個係後生囡囡，功夫做到足，一啲都冇偷懶。'}\n",
      "{'en': \"Everybody be more cooperative. If each person just thinks of himself and you disregard other people's interests, then we can't get anything done.\", 'yue': '大家合作啲，如果各人都諗縮數就乜都唔使做。'}\n",
      "{'en': 'The kid sounds quite all right when he speaks.', 'yue': '個細路講起嘢嚟都有紋有路喎。'}\n",
      "{'en': \"Hey, they're so pretty! Take a look, there are a pair of rainbows!\", 'yue': '嘩，咁靚啊，你睇，彩虹係打孖嚟㗎！'}\n",
      "{'en': \"There are so damned many people! I can't move!\", 'yue': '人鬼咁多！我唔喐得嘅！'}\n"
     ]
    }
   ],
   "source": [
    "REPO_DIRECTORY = r'/root/'\n",
    "ABC_DICT_PATH = r'autodl-tmp/AIST4010-Cantonese-Translator-Data/ABC-Dict/abc_dict.csv'\n",
    "\n",
    "def load_abc_dataset():\n",
    "    abc_dict = pd.read_csv(REPO_DIRECTORY + ABC_DICT_PATH)\n",
    "    abc_dataset = Dataset.from_pandas(abc_dict)\n",
    "    return abc_dataset\n",
    "\n",
    "abc_set = load_abc_dataset()\n",
    "abc_shuffled_set = abc_set.shuffle(seed=42).train_test_split(test_size=0.1)\n",
    "abc_train_set = abc_shuffled_set['train']\n",
    "abc_test_set = abc_shuffled_set['test']\n",
    "for (i, example) in enumerate(abc_train_set):\n",
    "    print(example)\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[920030 191513]\n",
      "[70.63027791 14.7023645 ]\n"
     ]
    }
   ],
   "source": [
    "def count_dataset_tokens(dataset):\n",
    "    en_count = 0\n",
    "    yue_count = 0\n",
    "    for example in dataset:\n",
    "        en_count += len(example['en'])\n",
    "        yue_count += len(example['yue'])\n",
    "    return en_count, yue_count\n",
    "\n",
    "\n",
    "counts = np.array(count_dataset_tokens(abc_train_set))\n",
    "print(counts)\n",
    "print(counts/len(abc_train_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=r'/root/autodl-tmp/01ai/Yi-6B-Chat'\n",
    "\n",
    "# model = Model.from_pretrained('01ai/Yi-6B')\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     device_map=\"auto\",\n",
    "#     torch_dtype='auto'\n",
    "# ).eval()\n",
    "\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = snapshot_download('01ai/Yi-6B-Chat', cache_dir='/root/autodl-tmp', revision='master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.18it/s]\n"
     ]
    }
   ],
   "source": [
    "base_tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True, padding_side='left', max_length=512, return_tensors='pt')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "\n",
    "# Since transformers 4.35.0, the GPT-Q/AWQ model can be loaded using AutoModelForCausalLM.\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=device,\n",
    "    torch_dtype='auto',\n",
    ")\n",
    "\n",
    "\n",
    "# # Prompt content: \"hi\"\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": \"hi\"}\n",
    "# ]\n",
    "\n",
    "\n",
    "# input_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\n",
    "# output_ids = model.generate(input_ids.to('cuda'))\n",
    "# response = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "# # Model response: \"Hello! How can I assist you today?\"\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": \"你識唔識講廣東話?\"},\n",
    "# ]\n",
    "\n",
    "# input_ids = base_tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\n",
    "# output_ids = base_model.generate(input_ids.to('cuda'))\n",
    "# response = base_tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "# # Model response: \"Hello! How can I assist you today?\"\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(input_ids)\n",
    "# print(output_ids)\n",
    "# print(base_tokenizer.decode(input_ids[0]))\n",
    "# print(base_tokenizer.decode(input_ids[0]))\n",
    "\n",
    "# #get text of list of tokens in output_ids stored in array\n",
    "# print([base_tokenizer.decode([token]) for token in output_ids[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['en'])):\n",
    "        text1 = f\"\"\"\n",
    "        <|im_start|> user\n",
    "        Translate the following words into Cantonese: \n",
    "        {example['en'][i]}\n",
    "        <|im_start|>assistant\n",
    "        {example['yue'][i]}\n",
    "        \"\"\"\n",
    "        text2 = f\"\"\"\n",
    "        <|im_start|> user\n",
    "        Translate the following words into English:\n",
    "        {example['yue'][i]}\n",
    "        <|im_start|>assistant\n",
    "        {example['en'][i]}\n",
    "        \"\"\"\n",
    "        output_texts.append(text1)\n",
    "        output_texts.append(text2)\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into Cantonese: \n",
      "        Scoop up water\n",
      "        <|im_start|>assistant\n",
      "        㧾水\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into English:\n",
      "        㧾水\n",
      "        <|im_start|>assistant\n",
      "        Scoop up water\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into Cantonese: \n",
      "        Ladle out soup\n",
      "        <|im_start|>assistant\n",
      "        㧾湯\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into English:\n",
      "        㧾湯\n",
      "        <|im_start|>assistant\n",
      "        Ladle out soup\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into Cantonese: \n",
      "        Third son of a rich family\n",
      "        <|im_start|>assistant\n",
      "        三少\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into English:\n",
      "        三少\n",
      "        <|im_start|>assistant\n",
      "        Third son of a rich family\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into Cantonese: \n",
      "        Young pigeon or squab that has been roasted\n",
      "        <|im_start|>assistant\n",
      "        乳鴿\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into English:\n",
      "        乳鴿\n",
      "        <|im_start|>assistant\n",
      "        Young pigeon or squab that has been roasted\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into Cantonese: \n",
      "        Husband's second older brother\n",
      "        <|im_start|>assistant\n",
      "        二少\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into English:\n",
      "        二少\n",
      "        <|im_start|>assistant\n",
      "        Husband's second older brother\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into Cantonese: \n",
      "        This time\n",
      "        <|im_start|>assistant\n",
      "        今勻\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into English:\n",
      "        今勻\n",
      "        <|im_start|>assistant\n",
      "        This time\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into Cantonese: \n",
      "        One dollar and sixty cents\n",
      "        <|im_start|>assistant\n",
      "        個六\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into English:\n",
      "        個六\n",
      "        <|im_start|>assistant\n",
      "        One dollar and sixty cents\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into Cantonese: \n",
      "        Rabbit\n",
      "        <|im_start|>assistant\n",
      "        兔仔\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into English:\n",
      "        兔仔\n",
      "        <|im_start|>assistant\n",
      "        Rabbit\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into Cantonese: \n",
      "        Exit through turnstile\n",
      "        <|im_start|>assistant\n",
      "        出閘\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into English:\n",
      "        出閘\n",
      "        <|im_start|>assistant\n",
      "        Exit through turnstile\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into Cantonese: \n",
      "        Chop something into cubes\n",
      "        <|im_start|>assistant\n",
      "        切粒\n",
      "        \n",
      "\n",
      "        <|im_start|> user\n",
      "        Translate the following words into English:\n",
      "        切粒\n",
      "        <|im_start|>assistant\n",
      "        Chop something into cubes\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "prompts = formatting_prompts_func(abc_set[:10])\n",
    "for prompt in prompts:\n",
    "    print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in base_model.named_parameters():\n",
    "#     print(f\"Parameter name: {name}\")\n",
    "#     print(param)\n",
    "#     print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"/root/autodl-tmp/01ai/Yi-6B-Chat\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 5000000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(base_model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 17,825,792 || all params: 6,078,861,312 || trainable%: 0.293242288071467\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=32, # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules = [\"k_proj\", \"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "peft_model = get_peft_model(base_model, \n",
    "                            lora_config)\n",
    "\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwargs option padding_side\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('tokenizer/tokenizer_config.json',\n",
       " 'tokenizer/special_tokens_map.json',\n",
       " 'tokenizer/tokenizer.model',\n",
       " 'tokenizer/added_tokens.json',\n",
       " 'tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_training_corpus(dataset):\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        sample_en = samples[\"en\"]\n",
    "        sample_yue = samples[\"yue\"]\n",
    "        for i in range(len(sample_en)):\n",
    "            yield sample_en[i]\n",
    "            yield sample_yue[i]\n",
    "\n",
    "training_corpus = get_training_corpus(abc_train_set)\n",
    "\n",
    "tokenizer = base_tokenizer.train_new_from_iterator(training_corpus, vocab_size=40000)\n",
    "tokenizer.save_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [89, 636, 507, 1929, 4015, 670], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [59568, 534, 456, 445, 534, 450, 436, 536, 454, 433, 534, 454, 483, 534, 457, 467, 534, 458, 436], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [25911, 4862], 'attention_mask': [1, 1]}\n",
      "{'input_ids': [6076, 4040], 'attention_mask': [1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(\"嗌呃畀啲嘢噃\"))\n",
    "print(base_tokenizer(\"嗌呃畀啲嘢噃\"))\n",
    "print(tokenizer(\"Good morning\"))\n",
    "print(base_tokenizer(\"Good morning\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bleu = evaluate.load('bleu')\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    print(predictions.shape, labels.shape)\n",
    "    return {\"bleu\": bleu(predictions, labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/trans/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:225: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/13026 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 13026/13026 [00:01<00:00, 8612.01 examples/s]\n",
      "Map: 100%|██████████| 1448/1448 [00:00<00:00, 8712.10 examples/s]\n",
      "/root/miniconda3/envs/trans/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3257' max='3257' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3257/3257 12:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.910600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.540900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.557800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.505000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.434200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.529000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.622600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.590600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.541600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.481000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.520600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.483800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.544800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.561700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.612500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.579400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.697200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.712200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.473600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.481500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.541300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.451100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>2.456200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.391500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.406200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.422600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>2.424400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.433600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>2.445700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.423800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>2.426200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.409300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/trans/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/01ai/Yi-6B-Chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/trans/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/01ai/Yi-6B-Chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/trans/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/01ai/Yi-6B-Chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/trans/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/01ai/Yi-6B-Chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/trans/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/01ai/Yi-6B-Chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/trans/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/01ai/Yi-6B-Chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3257, training_loss=2.517511322705483, metrics={'train_runtime': 725.9868, 'train_samples_per_second': 35.885, 'train_steps_per_second': 4.486, 'total_flos': 9.45612565314601e+16, 'train_loss': 2.517511322705483, 'epoch': 1.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=100,\n",
    "    output_dir=\"root/peft_model\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset= abc_train_set,\n",
    "    eval_dataset= abc_test_set,\n",
    "    tokenizer=tokenizer,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/trans/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/01ai/Yi-6B-Chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.model.save_pretrained(\"/root/peft_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': 'I much needed to shit, so as soon as I saw there was a place in the stairway, I immediately squatted down and took a shit, and after I had taken a shit, my life was smooth and easy, and the unusual situation was quickly relieved.', 'yue': '我好急，所以一見到樓梯有個地方，就馬上踎低擺堆，擺完堆之後，生命流暢，異常快慰。'}\n",
      "Base ID: tensor([[    6,  2942,   144,   144,   135,  7759, 14429,   567,  1926,  3151,\n",
      "          1029, 26212,  2823, 59569, 59601, 59568,   144,   135, 59597,   144,\n",
      "           135,     7, 59568,   144,     6, 14135,   144]])\n",
      "PEFT ID: tensor([[    6,  2942,   144,   144,   135,  7759, 14429,   567,  1926,  3151,\n",
      "          1029, 26212,  2823, 59569, 59601, 59568,   144,   135, 59597,   144,\n",
      "           135,     7, 59568,   144,     6, 14135,   144]])\n",
      "tensor([[    3,  4785,  3336,     0,    89,    89,    89,    89,    89,    89,\n",
      "            89, 35380,    77,  4131,  3540,  7360, 26322, 10679,  7722, 17559,\n",
      "            89,     0,    89,    89,    89,    89,    89,    89,    89,  3359,\n",
      "             0,    89,    89,    89,    89,    89,    89,    89,    89,     4,\n",
      "            89,     0,     3, 39137,  3462,     0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 127]) torch.Size([1, 146])\n",
      "Base model:  ２２２２２２２２<|Human|><|Human|> Min box２２２２２２２２２２２<|Human|> stamped box box box<filename> Cal４２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２\n",
      "Fine-tuned:           !ant!!Cantones！，                                                                                  \n",
      "Base ID: tensor([[    6,  2942,   144,   144,   135,  7759, 14429,   567,  1926,  3151,\n",
      "          1029,  4750, 59601,   144,   135, 59646,   144,   135,     7, 59568,\n",
      "           144,     6, 14135,   144]])\n",
      "PEFT ID: tensor([[    6,  2942,   144,   144,   135,  7759, 14429,   567,  1926,  3151,\n",
      "          1029,  4750, 59601,   144,   135, 59646,   144,   135,     7, 59568,\n",
      "           144,     6, 14135,   144]])\n",
      "tensor([[    3,  4785,  3336,     0,    89,    89,    89,    89,    89,    89,\n",
      "            89, 35380,    77,  4131,  3540,  7360, 26322, 10679,  5242,    29,\n",
      "             0,    89,    89,    89,    89,    89,    89,    89,  3397,     0,\n",
      "            89,    89,    89,    89,    89,    89,    89,    89,     4,    89,\n",
      "             0,     3, 39137,  3462,     0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 124]) torch.Size([1, 145])\n",
      "Base model:  ２２２２２２２２<|Human|> stamped box box Min systems systems４２２２２２２２２<|Human|> stamped box box box２２２２２２２２２EC Min２２２２２２２２２<|Human|>２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２\n",
      "Fine-tuned:            assist!!!!。                                                                                   \n",
      "{'en': \"There's no need to be so hurried, as we still have almost one hour till we need to board the train.\", 'yue': '唔使咁急，我哋仲有差唔多一個鐘頭先至要上車。'}\n",
      "Base ID: tensor([[    6,  2942,   144,   144,   135,  7759, 14429,   567,  1926,  3151,\n",
      "          1029, 26212,  2823, 59569, 59601, 59568,   144,   135, 59578,   144,\n",
      "           135,     7, 59568,   144,     6, 14135,   144]])\n",
      "PEFT ID: tensor([[    6,  2942,   144,   144,   135,  7759, 14429,   567,  1926,  3151,\n",
      "          1029, 26212,  2823, 59569, 59601, 59568,   144,   135, 59578,   144,\n",
      "           135,     7, 59568,   144,     6, 14135,   144]])\n",
      "tensor([[    3,  4785,  3336,     0,    89,    89,    89,    89,    89,    89,\n",
      "            89, 35380,    77,  4131,  3540,  7360, 26322, 10679,  7722, 17559,\n",
      "            89,     0,    89,    89,    89,    89,    89,    89,    89,  3398,\n",
      "             0,    89,    89,    89,    89,    89,    89,    89,    89,     4,\n",
      "            89,     0,     3, 39137,  3462,     0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 127]) torch.Size([1, 146])\n",
      "Base model:  ２２２２２２２２<|Human|> stamped box box box４２２２２２２２２２<|Human|> stamped box box box<filename>４２２２２２２２２２２２２２２２２２２２２２２nWhennWhen$(４２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２\n",
      "Fine-tuned:                                                                                                     \n",
      "Base ID: tensor([[    6,  2942,   144,   144,   135,  7759, 14429,   567,  1926,  3151,\n",
      "          1029,  4750, 59601,   144,   135, 59897,   144,   135,     7, 59568,\n",
      "           144,     6, 14135,   144]])\n",
      "PEFT ID: tensor([[    6,  2942,   144,   144,   135,  7759, 14429,   567,  1926,  3151,\n",
      "          1029,  4750, 59601,   144,   135, 59897,   144,   135,     7, 59568,\n",
      "           144,     6, 14135,   144]])\n",
      "tensor([[    3,  4785,  3336,     0,    89,    89,    89,    89,    89,    89,\n",
      "            89, 35380,    77,  4131,  3540,  7360, 26322, 10679,  5242,    29,\n",
      "             0,    89,    89,    89,    89,    89,    89,    89, 29873,     0,\n",
      "            89,    89,    89,    89,    89,    89,    89,    89,     4,    89,\n",
      "             0,     3, 39137,  3462,     0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 124]) torch.Size([1, 145])\n",
      "Base model:  ２２２２２２２２<|Human|> stamped box box活动中４２２２２２２２２２２<|Human|><|Human|>ava box box box box２２２２２２２２２ feltoll Min２２２２２２２２２<|Human|> stamped２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２\n",
      "Fine-tuned:           assistantant!中!檸。                  我 佢!ing 。                   ， the ?!！!           assistantant!ant懶。             \n",
      "{'en': 'They squat down and wait for the judge to give the order for the race to start.', 'yue': '佢哋踎低等裁判員發令起跑。'}\n",
      "Base ID: tensor([[    6,  2942,   144,   144,   135,  7759, 14429,   567,  1926,  3151,\n",
      "          1029, 26212,  2823, 59569, 59601, 59568,   144,   135, 59569,   144,\n",
      "           135,     7, 59568,   144,     6, 14135,   144]])\n",
      "PEFT ID: tensor([[    6,  2942,   144,   144,   135,  7759, 14429,   567,  1926,  3151,\n",
      "          1029, 26212,  2823, 59569, 59601, 59568,   144,   135, 59569,   144,\n",
      "           135,     7, 59568,   144,     6, 14135,   144]])\n",
      "tensor([[    3,  4785,  3336,     0,    89,    89,    89,    89,    89,    89,\n",
      "            89, 35380,    77,  4131,  3540,  7360, 26322, 10679,  7722, 17559,\n",
      "            89,     0,    89,    89,    89,    89,    89,    89,    89,    89,\n",
      "            63,     0,    89,    89,    89,    89,    89,    89,    89,    89,\n",
      "             4,    89,     0,     3, 39137,  3462,     0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 127]) torch.Size([1, 147])\n",
      "Base model:  ２２２２２２２２<|Human|><|Human|><|Human|>科技２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２\n",
      "Fine-tuned:           antantant?!!。                    ing !!s.!啲 你要 佢!㧾.咪。         assistantantant檸is 咪.啲 佢!          assistantantant啲ant就 and !s. 我！.ing，    \n",
      "Base ID: tensor([[    6,  2942,   144,   144,   135,  7759, 14429,   567,  1926,  3151,\n",
      "          1029,  4750, 59601,   144,   135,   537,   489,   447,   144,   135,\n",
      "             7, 59568,   144,     6, 14135,   144]])\n",
      "PEFT ID: tensor([[    6,  2942,   144,   144,   135,  7759, 14429,   567,  1926,  3151,\n",
      "          1029,  4750, 59601,   144,   135,   537,   489,   447,   144,   135,\n",
      "             7, 59568,   144,     6, 14135,   144]])\n",
      "tensor([[    3,  4785,  3336,     0,    89,    89,    89,    89,    89,    89,\n",
      "            89, 35380,    77,  4131,  3540,  7360, 26322, 10679,  5242,    29,\n",
      "             0,    89,    89,    89,    89,    89,    89,    89,    89,  2743,\n",
      "             0,    89,    89,    89,    89,    89,    89,    89,    89,     4,\n",
      "            89,     0,     3, 39137,  3462,     0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 126]) torch.Size([1, 146])\n",
      "Base model:  ２２２２２２２２<|Human|> stamped box�２２２２２２２２基本se基本 images<filename>４２２２２２２２２２２<|Human|> box２２２２２２２２２<|Human|> York４２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２\n",
      "Fine-tuned:           佢。                                                                                         \n",
      "{'en': \"I'm not clear about what trouble the guy has gotten into, but I know the police are looking for him.\", 'yue': '條友𦧺啲乜嘢屎𦡆，我唔清楚，但係知道警察搵緊佢。'}\n",
      "Base ID: tensor([[    6,  2942,   144,   144,   135,  7759, 14429,   567,  1926,  3151,\n",
      "          1029, 26212,  2823, 59569, 59601, 59568,   144,   134,   144,   135,\n",
      "             7, 59568,   144,     6, 14135,   144]])\n",
      "PEFT ID: tensor([[    6,  2942,   144,   144,   135,  7759, 14429,   567,  1926,  3151,\n",
      "          1029, 26212,  2823, 59569, 59601, 59568,   144,   134,   144,   135,\n",
      "             7, 59568,   144,     6, 14135,   144]])\n",
      "tensor([[    3,  4785,  3336,     0,    89,    89,    89,    89,    89,    89,\n",
      "            89, 35380,    77,  4131,  3540,  7360, 26322, 10679,  7722, 17559,\n",
      "            89,     0,    89,    89,    89,    89,    89,    89,    89,    89,\n",
      "            89,     0,    89,    89,    89,    89,    89,    89,    89,    89,\n",
      "             4,    89,     0,     3, 39137,  3462,     0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 126]) torch.Size([1, 147])\n",
      "Base model:  ２２２２２２２２<|Human|> stamped box boxava４２２２２２２２２２<|Human|> stamped box box box４２２２２２２２２EC per Min２２２２２２２２２２<|Human|> stamped box２２２２２２２２２２ECava４２２２２２２２２２<|Human|>EC box４２２２２２２２２se Min<filename>４２２２２２２\n",
      "Fine-tuned:                                                                                                     \n",
      "Base ID: tensor([[    6,  2942,   144,   144,   135,  7759, 14429,   567,  1926,  3151,\n",
      "          1029,  4750, 59601,   144,   135,   534,   454,   483,   144,   135,\n",
      "             7, 59568,   144,     6, 14135,   144]])\n",
      "PEFT ID: tensor([[    6,  2942,   144,   144,   135,  7759, 14429,   567,  1926,  3151,\n",
      "          1029,  4750, 59601,   144,   135,   534,   454,   483,   144,   135,\n",
      "             7, 59568,   144,     6, 14135,   144]])\n",
      "tensor([[    3,  4785,  3336,     0,    89,    89,    89,    89,    89,    89,\n",
      "            89, 35380,    77,  4131,  3540,  7360, 26322, 10679,  5242,    29,\n",
      "             0,    89,    89,    89,    89,    89,    89,    89,  3832,     0,\n",
      "            89,    89,    89,    89,    89,    89,    89,    89,     4,    89,\n",
      "             0,     3, 39137,  3462,     0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 126]) torch.Size([1, 145])\n",
      "Base model:  ２２２２２２２２<|Human|> stamped box�４２２２２２２２２２<|Human|> stamped box box box box Min４２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２\n",
      "Fine-tuned:            assistant!!請!。                      我ing !is ing                                                        \n",
      "{'en': 'Yee Nga Court is located on On Po Road in Tai Po, New Territories.', 'yue': '怡雅苑係位於新界、大埔、安埔路。'}\n",
      "Base ID: tensor([[    6,  2942,   144,   144,   135,  7759, 14429,   567,  1926,  3151,\n",
      "          1029, 26212,  2823, 59569, 59601, 59568,   144,   135, 59618,   144,\n",
      "           135,     7, 59568,   144,     6, 14135,   144]])\n",
      "PEFT ID: tensor([[    6,  2942,   144,   144,   135,  7759, 14429,   567,  1926,  3151,\n",
      "          1029, 26212,  2823, 59569, 59601, 59568,   144,   135, 59618,   144,\n",
      "           135,     7, 59568,   144,     6, 14135,   144]])\n",
      "tensor([[    3,  4785,  3336,     0,    89,    89,    89,    89,    89,    89,\n",
      "            89, 35380,    77,  4131,  3540,  7360, 26322, 10679,  7722, 17559,\n",
      "            89,     0,    89,    89,    89,    89,    89,    89,    89,  3753,\n",
      "             0,    89,    89,    89,    89,    89,    89,    89,    89,     4,\n",
      "            89,     0,     3, 39137,  3462,     0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 127]) torch.Size([1, 146])\n",
      "Base model:  ２２２２２２２２２<|Human|> stamped box２２２２２２２２２<|Human|> stamped box box box４２２２２２２２２２２<|Human|>wise<filename>２２２２２２２２２EC Minwise<commit_after>４２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２\n",
      "Fine-tuned:                                                                                                    \n",
      "Base ID: tensor([[    6,  2942,   144,   144,   135,  7759, 14429,   567,  1926,  3151,\n",
      "          1029,  4750, 59601,   144,   135, 59817,   144,   135,     7, 59568,\n",
      "           144,     6, 14135,   144]])\n",
      "PEFT ID: tensor([[    6,  2942,   144,   144,   135,  7759, 14429,   567,  1926,  3151,\n",
      "          1029,  4750, 59601,   144,   135, 59817,   144,   135,     7, 59568,\n",
      "           144,     6, 14135,   144]])\n",
      "tensor([[    3,  4785,  3336,     0,    89,    89,    89,    89,    89,    89,\n",
      "            89, 35380,    77,  4131,  3540,  7360, 26322, 10679,  5242,    29,\n",
      "             0,    89,    89,    89,    89,    89,    89,    89,    89,   221,\n",
      "             0,    89,    89,    89,    89,    89,    89,    89,    89,     4,\n",
      "            89,     0,     3, 39137,  3462,     0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 124]) torch.Size([1, 146])\n",
      "Base model:  ２２２２２２２２<|Human|> stamped box box２２２２２２２２２２２２２<|Human|><|Human|><|Human|>２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２２\n",
      "Fine-tuned:                                                                                                     \n"
     ]
    }
   ],
   "source": [
    "#get random data from test dataset\n",
    "for i in range(5):\n",
    "    example = abc_test_set[i]\n",
    "    print(example)\n",
    "    text1 = f\"\"\"\n",
    "        Translate the following words into Cantonese: \n",
    "        {example['en'][i]}\n",
    "        \"\"\"\n",
    "    text2 = f\"\"\"\n",
    "        Translate the following words into English:\n",
    "        {example['yue'][i]}\n",
    "        \"\"\"\n",
    "    texts = [text1, text2]\n",
    "    for text in texts:\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ]\n",
    "        #print model outputs for base_model and peft_model\n",
    "        base_input_ids = base_tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\n",
    "        peft_input_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\n",
    "        print(\"Base ID:\", base_input_ids)\n",
    "        print(\"PEFT ID:\", base_input_ids)\n",
    "        print(peft_input_ids)\n",
    "        base_output_ids = base_model.generate(base_input_ids.to('cuda'), max_new_tokens=100)\n",
    "        peft_output_ids = peft_model.generate(peft_input_ids.to('cuda'), max_new_tokens=100)\n",
    "        print(base_output_ids.shape, peft_output_ids.shape)\n",
    "        print(\"Base model: \", base_tokenizer.decode(base_output_ids[0][base_input_ids.shape[1]:], skip_special_tokens=True))\n",
    "        print(\"Fine-tuned: \", tokenizer.decode(peft_output_ids[0][peft_input_ids.shape[1]:], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "translate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
